\section{سوال چهارم - تئوری}

یکی از مشکلات ترنسفورمرها مرتبه هزینه محاسباتی و هزینه ذخیره‌سازی عملیات \lr{self-attention} است که از مرتبه $O(N^2)$ می‌باشد. تلاش‌هایی برای کاهش این مشکل انجام شد. مقالاتی نشان دادند که عملکرد \lr{softmax} باعث می‌شود تا نتوانیم این پیچیدگی را کاهش دهیم. توضیح دهید چرا عملکرد \lr{softmax} باعث وجود این مسئله می‌شود. همچنین یکی از پیشنهادها برای حل این مشکل استفاده از مکانیزم‌های توجه کرنلی است. در مورد این مکانیزم تحقیق کنید و نشان دهید چطور این روش منجر به کاهش پیچیدگی می‌شود. یک کرنل به دلخواه انتخاب کنید و عبارت «۱» را بازنویسی کنید و مرتبه زمانی و حافظه مورد نیاز برای عملکرد \lr{self-attention} را محاسبه کنید. لطفا به مقاله که برای انتخاب کرنل مراجعه کردید، ارجاع دهید.

\begin{equation}
	Attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}




\begin{qsolve}
بر اساس رابطه ۱ مکانیزم \lr{Self-attention}  شامل محاسبه ضرب داخلی $Q$و $K$ می‌شود که منجر به تولید ماتریسی با ابعاد $N\times N$ می‌شود. تابع \lr{softmax} به هر سطر این ماتریس اعمال می‌شود که مقادیر توجه را نرمالیزه می‌کند. پیچیدگی درجه دوم از اینجا ناشی می‌شود که:

\begin{enumerate}
	\item \textbf{ضرب ماتریسی}: محاسبه \( QK^T \) شامل عملیات \( O(N^2d_k) \) است.
	\item \textbf{محاسبه \lr{softmax}}: اگرچه softmax برای هر سطر \( O(N) \) است، به همه \( N \) سطر اعمال می‌شود که منجر به \( O(N^2) \) در کل می‌شود.
\end{enumerate}

برای حل مشکل پیچیدگی درجه دوم، مکانیزم‌های توجه مبتنی بر هسته پیشنهاد شده‌اند. این روش‌ها با استفاده از توابع هسته، تابع \lr{softmax} را تقریب می‌زنند که می‌تواند پیچیدگی محاسبات توجه را کاهش دهد.

یکی از این روش‌ها استفاده از نقشه ویژگی تصادفی برای تقریب تابع هسته \lr{softmax} است. ایده اصلی این است که ورودی را با استفاده از نقشه ویژگی به فضایی تبدیل کنیم که در آن ضرب داخلی، تقریب تابع هسته اصلی را ارائه دهد. حال سوال پیش می‌آید که از چه هسته هایی می‌توان استفاده نمود؟

\begin{enumerate}
	\item \textbf{هسته \lr{RBF}}\\
هسته \lr{RBF} یکی از انتخاب‌های محبوب برای چنین تقریب‌هایی است. هسته \lr{RBF} به صورت زیر تعریف می‌شود:
	
	\[ k(x, y) = e^{-\frac{\|x - y\|^2}{2\sigma^2}} \]
	
اما برای کارایی محاسباتی، می‌توانیم از تقریب های سری فوریه تصادفی استفاده کنیم.


	\item \textbf{تقریب \lr{softmax} با هسته \lr{RBF}}\\
	هسته \lr{RBF} می‌تواند با استفاده از ویژگی‌های سری فوریه تصادفی به صورت زیر تقریب زده شود:
	
	\[ k(x, y) \approx \phi(x)^T \phi(y) \]
	
	که در آن \( \phi(x) \) یک نقشه ویژگی تصادفی از \( x \) است.
\end{enumerate}
\end{qsolve}






\begin{qsolve}
	\begin{enumerate}
		
		
			\item \textbf{بازنویسی توجه مبتنی بر هسته}\\
			با استفاده از این تقریب، مکانیزم توجه می‌تواند به صورت زیر بازنویسی شود:
			
			\[ Attention(Q, K, V) \approx \left(\phi(Q) \phi(K)^T \right) V \]
		
		
		
		
			\item \textbf{کاهش پیچیدگی}\\
			\begin{itemize}
					\item \textbf{تبدیلات}: نقشه ویژگی تصادفی \( \phi \) به طور معمول ابعاد کمتری \( r \) دارد. تبدیل \( Q \) و \( K \) به \( \phi(Q) \) و \( \phi(K) \) به ترتیب شامل عملیات \( O(Nd_kr) \) است.
					\item \textbf{ضرب داخلی}: ضرب داخلی \( \phi(Q) \phi(K)^T \) شامل عملیات \( O(N^2 r) \) است.
					\item \textbf{ضرب نهایی با \( V \)}: ضرب نتیجه با \( V \) شامل عملیات \( O(Nrd_v) \) است.
				\end{itemize}
			با انتخاب \( r \ll N \)، پیچیدگی به طور قابل توجهی کاهش می‌یابد.
		
		
		
		
		
		
		\item \textbf{پیچیدگی زمانی و حافظه}\\
		\begin{itemize}
			\item \textbf{پیچیدگی زمانی}:
			\begin{itemize}
				\item نقشه ویژگی: \( O(Nd_kr) \)
				\item ضرب داخلی: \( O(N^2r) \)
				\item محصول نهایی: \( O(Nrd_v) \)
			\end{itemize}
			
			با ترکیب اینها، پیچیدگی زمانی کلی:
			
			\[ O(Nd_kr + N^2r + Nrd_v) \]
			
			با \( r \ll N \)، عبارت غالب \( O(N^2r) \) است.
			
			\item \textbf{پیچیدگی حافظه}:
			\begin{itemize}
				\item ذخیره \( \phi(Q) \) و \( \phi(K) \): \( O(Nr) \)
			\end{itemize}
		\end{itemize}
		
		
		
	\end{enumerate}
\end{qsolve}


\begin{latin}
	\begin{thebibliography}{9}
		\bibitem{ref1}
		"Rethinking Attention with Performers" by Choromanski et al. (2021), which introduces the Performer model using kernel-based approximations to reduce the complexity of self-attention.
	\end{thebibliography} 
\end{latin}