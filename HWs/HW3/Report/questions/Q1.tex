\section{سوال اول - عملی نظری}

برای آموزش مدل‌های زبانی بزرگ (\lr{Large Language Model}) که حاوی میلیون‌ها و میلیارد‌ها پارامتر هستند، از حجم قابل توجهی داده استفاده می‌شود. اما در تمامی این مدل‌ها یک تاریخ قطع آموزش وجود دارد که مدل زبانی هیچ اطلاعاتی در خصوص داده‌های تولید شده‌ی پس از این زمان ندارد. به‌عنوان مثال، تاریخ قطع آموزش مدل \lr{GPT-3.5-turbu-instruction} سپتامبر ۲۰۲۱ است و از همین رو این مدل ممکن است به سوالات مربوط به رویداد‌های سال ۲۰۲۲، ۲۰۲۳ و ۲۰۲۴ پاسخ صحیح ندهد. چنین داده‌هایی که بعد از تاریخ قطع آموزش تولید شده‌اند و یا بخشی از داده‌ی آموزشی اولیه‌ی مدل زبانی نیستند را داده‌ی خارجی می‌گوییم. تکنیک تولید تقویت شده با بازیابی (\lr{RAG}) رویکردی است که با استخراج داده‌ی خارجی متناسب با فرمان، دریافت شده و افزودن آن به‌عنوان ورودی به مدل زبانی تلاش می‌کند که فرمان ورودی را تقویت کرده و به مدل زبانی کمک می‌کند تا جواب مرتبط و متناسبی بسازد. به‌عنوان مثال در پاسخ به یک فرمان متنی مانند «چه‌کسی شرکت توییتر را درسال ۲۰۲۲ خرید؟» تمامی داده‌های خارجی متناسب با این فرمان را استخراج می‌کند و آن‌ها را به‌عنوان ورودی به مدل زبانی \lr{GPT-3.5-turbo-instruct}
می‌دهد تا مدل زبانی بتواند با دانش دریافت شده پاسخ متناسبی تولید کند. این رویکرد نیاز به آموزش مجدد و با باز‌تنظیم (\lr{Fine tune})
مدل زبانی را برطرف می‌سازد. در این پروژه می‌خواهیم با استفاده از شبکه‌های خودسازمان‌ده این تکنیک را پیاده‌سازی کنیم.

\begin{center}
	\includegraphics*[width=0.8\linewidth]{pics/img1.png}
	\captionof{figure}{فرآیند کلی RAG در یک مدل زبانی بزرگ}
	\label{داده‌های تولید شده برای مسئله}
\end{center}


وظیفه اصلی RAG جست‌و‌جو معنایی (\lr{Semantic search}) در پایگاه داه‌های اطلاعاتی و بازیابی اطلاعات خارجی دارای تناسب محتوایی با فرمان داده‌شده به یک مدل زبانی است.برای تسهیل جست‌و‌جوی معنایی، ابتدا داده‌های خارجی استخراج شده به بازنمایی‌های عددی یا برداری تبدیل می‌شوند که به این بازنمایی، تعبیه‌ی متن (\lr{Text embedding}) می‌گوییم. در زمان بازیابی نیز ابتدا فرمان متنی به بازنمایی برداری تبدیل می‌شود و سپس نزدیک‌ترین بردار‌های داده‌ی خارجی متناسب با آن استخراج می‌شود. شکل «\textcolor{blue}{\ref{داده‌های تولید شده برای مسئله}}»
دیاگرام کلی این فرآیند را نشان می‌دهد. چالش اصلی این رویکرد این است که جست‌و‌جوی معنایی ذکر شده به دلیل نیازمندی به محاسبه‌ی فاصله‌ی بردار فرمان با حجم عظیمی از بردار‌های داده‌ی خارجی، به منابع پردازشی و محاسباتی زیاد و زمان قابل توجهی نیاز دارد. بنابر این پیدا کردن رویکردی که جست‌و‌جوی معنایی را به‌صورت کارا انجام دهد بسیار حائز اهمیت است.

برای افزایش کارایی جست‌و‌جو معنایی، یک رویکرد رایج این است که بردار‌های داده‌های خارجی را خوشه‌بندی کنیم و در زمان جست‌و‌جو نیز ایتدا خوشه مشابه با بردار فرمان ورودی را پیدا می‌کنیم و سپس شباهت بردار‌های داده‌های خارجی متعلق به آن خوشه با بردار فرمان را محاسبه می‌کنیم و اگر شباهت بردار‌ها از یک آستانه بیشتر باشد، آنها را به‌عنوان اطلاعات مرتبط درنظر می‌گیریم.

\begin{enumerate}
	\item در این پروژه قصد داریم برای خوشه‌بندی داده‌های خارجی از شبکه خود‌سازمان‌ده استفاده کنیم. بررسی کنید که در این شبکه‌ها نسبت به سایر روش‌های خوشه‌بندی که در یادگیری ماشین به‌کار گرفته می‌شود، چه مزایا و معایبی دارد؟ به نظر شما، چرا استفاده از شبکه خودسازمان‌ده به صورت با نظارت صورت نمی‌گیرد؟ فرآیند یادگیری این مدل‌ها را توضیح دهید.
	
	\begin{qsolve}
قبل از بررسی مزایا و معایب شبکه \lr{SOM} نیاز است که یک‌سری پیش‌نیاز ها را توضیح دهیم. پیش از هر چیزی ابتدا می‌بایست انواع الگوریتم های یادگیری ماشین و دلیل استفاده از آنها‌را توضیح دهیم. الگوریتم های یادگیری ماشین به ۳ دسته مختلف تقسیم می‌شوند:

	\begin{enumerate}
		\item یادگیری با نظارت (\lr{Supervised Learning})
		\item یادگیری نیمه نظارتی (\lr{Semi-supervised Learning})
		\item  یادگیری بدون نظارت (\lr{Unsupervised Learning})
	\end{enumerate}
در یادگیری بانظارت، داده و لیبل‌های متناظر با آنها را داریم. در یادگیری نیمه نظارتی، صرفا بخشی از داده‌ها لیبل دارند و لیبل بقیه داده‌ها مشخص نیست. دسته آخر که مورد بحث ماست، یادگیری بدون نظارت است که داده‌های موجود، لیبل ندارند و به ازای داده‌های مختلف، خروجی مناسب را نمی‌دانیم و از الگو‌های پنهان در داده‌ها اطلاعی نداریم. در این صورت است که به سمت الگوریتم‌های بدون نظارت می‌آییم تا به الگوریتم این اجازه را بدهیم که هرچه را می‌تواند یاد بگیرد و اطلاعات پنهان در داده‌ها را مشخص کند. الگوریتم‌های خوشه‌بندی در این دسته قرار می‌گیرند و دلیل قرارگیری در این دسته آن است که ما هیچ اطلاعاتی درمورد داده‌های ورودی نداریم و به دنبال ایجاد وابستگی میان آنها هستیم. الگوریتم‌های خوشه‌بندی این امکان را برای ما فراهم می‌سازد تا داده‌های شبیه به هم را در یک دسته قرار دهد. در این باره در صفحه ۱۴۱ \cite{ref1} گفته شده است:

« \textbf{تکنیک‌های خوشه‌بندی زمانی اعمال می‌شوند که کلاسی برای پیش‌بینی وجود نداشته باشد، بلکه زمانی که نمونه‌ها باید به گروه‌های طبیعی تقسیم شوند، اعمال می‌شوند.} »
	
	
پس اگر با داده‌هایی مواجه بودیم که اطلاعاتی در مورد آنها نمی‌دانیم،‌ خوشه‌یابی بهترین روش برای درک وابستگی‌ها میان داده‌هاست. الگوریتم‌های خوشه‌یابی را می‌توان به‌صورت زیر دسته‌بندی کرد:

	\begin{latin}
		\begin{enumerate}
			\item Density-based
			\item Distribution-based
			\item Centroid-based
			\item Hierarchical-based
		\end{enumerate}
	\end{latin}
	
	در الگوریتم‌های خوشه‌یابی مبتنی بر چگالی، داده‌ها بر اساس تراکم و غلظت داده‌ها در نقاط مختلف تقسیم‌بندی می‌شود. 
	\end{qsolve}
	
	
	
	\begin{qsolve}
در خوشه‌یابی توزیع شده، اساس خوشه یابی به‌صورت احتمالی است. یعنی برای تمام نقاط یک احتمال تعلق به یک خوشه خاص درنظر گرفته می‌شود که با دور شدن داده از مرکز آن خوشه، احتمال تعلق داده به خوشه مربوطه کاهش پیدا می‌کند.

پرکاربرد ترین و سریع‌ترین نوع خوشه‌یابی، خوشه‌یابی \lr{Centroid} است. این الگوریتم نقطه‌ها را بر اساس چندین مرکز در داده‌ها جدا می‌کند و هر نقطه بر اساس مجذور فاصله‌اش تا مرکز داده به یک خوشه اختصاص می‌یابد.

استفاده از خوشه‌بندی سلسله‌مراتبی محدود تر از سایر روش هاست. بدین صورت است که برای داده‌هایی که ذاتا به صورت سلسله‌مراتبی هستند استفاده می‌شود. مانند داده‌های مربوط به یک پایگاه داده.

الگوریتم های مختلفی برای خوشه‌یابی وجود دارد که می‌توان چندتا از آنها را به‌صورت زیر نام برد:

	\begin{latin}
		\begin{enumerate}
			\item SOM
			\item K-means
			\item DBSCAN
			\item Gaussian Mixture
			\item BIRCH
			\item Affinity Propagation
			\item Mean-Shift
			\item OPTICS
		\end{enumerate}
	\end{latin}
	
	در این سوال به بررسی دو مورد از مهم‌ترین الگوریتم ها یعنی \lr{SOM} و \lr{K-means} می‌پردازیم.
	
	\begin{enumerate}
		\item \lr{\textbf{:K-Means}}
		الگوریتم \lr{K-Means} یک الگوریتم بدون نظارت، مبتنی بر مرکز و تکراری (\lr{iterative}) است که داده های ورودی را دریافت می‌کند و آنها را به \lr{K} دسته تقسیم می‌کند. مقدار \lr{K} می‌بایست از قبل مشخص باشد. هدف در الگوریتم \lr{K-Means} به حداقل رساندن مجموع فواصل بین دو نقطه داده شده و خوشه مربوط به آنهاست و تا زمانی که مینیمم فاصله را پیدا نکند، الگوریتم متوقف نمی‌شود.
		
		\textbf{ذکر این نکته الزامی است که در این الگوریتم، آموزشی‌ای صورت نمی‌گیرد و صرفا یک کار تکراری چندین بار تکرار می‌شود تا زمانی که بهینه‌ترین حالت پیدا شود.}
		
		شکل «\ref{ساختار الگوریتم K-Means}» نحوه عملکرد الگوریتم \lr{K-Means} را نشان می‌دهد.
	
	
	\begin{center}
		\includegraphics*[width=0.7\linewidth]{pics/img2.png}
		\captionof{figure}{ساختار الگوریتم \lr{K-Means}}
		\label{ساختار الگوریتم K-Means}
	\end{center}
	
	
	\end{enumerate}
	
\end{qsolve}
	
	
	
\begin{qsolve}
مراحل انجام الگوریتم \lr{K-Means} به‌صورت زیر است:

\begin{enumerate}
	\item \textbf{مرحله ۱: } انتخاب مقدار \lr{K} بر اساس تعداد خوشه‌ها. اگر تعداد خوشه‌ها را نمی‌دانیم، عددی بزرگ را انتخاب می‌کنیم.
	
	\item \textbf{مرحله ۲:} انتخاب \lr{K} نقطه به‌صورت رندم و تصادفی.
	
	\item \textbf{مرحله ۳:} قرار دادن هر نقطه در نزدیک‌ترین مرکز آن. (مرکز \lr{K} خوشه‌ای که از قبل تعیین شده است.)
	
	\item \textbf{مرحله ۴:} واریناس را حساب کرده و مرکز جدید را برحسب واریانس انتخاب کرده
	
	\item \textbf{مرحله ۵:} تکرار مرحله ۳. یعنی قرار دادن هر نقطه در مرکز جدید تعیین شده
	
	\item \textbf{مرحله ۶:} اگر هر تخصیص مجددی رخ داد به مرحله ۴ باید برویم درغیر این صورت به مرحله ۷
	
	\item \textbf{مرحله ۷:} پایان الگوریتم
\end{enumerate}

برای درک بهتر، در ادامه با رسم شکل مراحل بالا را توضیح خواهیم داد. فرض شود که داده‌های ورودی ما به‌صورت زیر باشد:

\begin{center}
	\includegraphics*[width=0.5\linewidth]{pics/img3.png}
	\captionof{figure}{داده‌های ورودی}
	\label{داده‌های ورودی}
\end{center}

در اینجا چون تعداد خوشه‌ها برای ما مشخص است، مقدار \lr{K} را ۲ فرض می‌کنیم. و دو نقطه به‌صورت رندم در صفحه به‌عنوان نقاط شروع الگوریتم انتخاب می‌کنیم. شکل «\ref{نقاط ابتدایی الگوریتم}»

\end{qsolve}
	
	
	
	
	
\begin{qsolve}
	\begin{center}
		\includegraphics*[width=0.5\linewidth]{pics/img4.png}
		\captionof{figure}{نقاط ابتدایی الگوریتم}
		\label{نقاط ابتدایی الگوریتم}
	\end{center}
	
	
	اکنون هر نقطه را به نزدیک‌ترین مرکز اختصاص می‌دهیم. این عملیات با محاسبه فاصله بین نقطه‌ها انجام می‌شود. سپس به مرحله آپدیت مرکز می‌رویم و برحسب واریانس محاسبه کرده، مرکز نقاط را آپدیت می‌کنیم. شکل «\ref{آپدیت مراکز}»
	
	
	 \begin{center}
	 	\includegraphics*[width=0.5\linewidth]{pics/img5.png}
	 	\captionof{figure}{آپدیت مراکز}
	 	\label{آپدیت مراکز}
	 \end{center}

	این فرآیند را آنقدر ادامه می‌دهیم تا مینیمم ترین فاصله نقاط از مراکز به‌دست آید و داده‌ها خوشه‌بندی شود. شکل «\ref{داده‌های خوشه‌بندی شده}»

\end{qsolve}
	
	
	
	
	
\begin{qsolve}
	\begin{center}
		\includegraphics*[width=0.5\linewidth]{pics/img6.png}
		\captionof{figure}{داده‌های خوشه‌بندی شده}
		\label{داده‌های خوشه‌بندی شده}
	\end{center}
	
	
	
	\begin{enumerate}
		\item \textbf{\lr{:SOM}}
		\lr{SOM} بر خلاف \lr{K-Means} یک شبکه عصبی است که بر اساس یادگیری بدون نظارت کار می‌کند. شبکه \lr{SOM} کاربرد‌های مختلفی دارد. کاربرد اصلی شبکه \lr{SOM} نگاشت داده‌های با بعد بالا به داده‌هایی با بعد پایین است. اما از این شبکه برای خوشه‌یابی نیز استفاده می‌شود
		شبکه \lr{SOM} تنها از دو لایه تشکیل می‌شود. (لایه ورودی + خروجی) که لایه خروجی می‌تواند به صورت پشت سر هم و یا در یک ساختار شبکه ای قرار گیرند. شکل «\ref{ساختار شبکه SOM}»
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img7.png}
			\captionof{figure}{ساختار شبکه \lr{SOM}}
			\label{ساختار شبکه SOM}
		\end{center}
	
	در فرآیند آموزش وزن‌های متصل از ورودی به خروجی آموزش داده می‌شوند و در نهایت، هر نورون نماینده یک دسته (خوشه) از داده‌های ورودی است. الگوریتم یادگیری در \lr{SOM}، رقابتی (\lr{Competitive}) است. یعنی نورون‌های خروجی باهم بر سر نماینده شدن برای داده‌های ورودی رقابت می‌کنند و نورون برنده وزنش به نسبت نورون بازنده بیشتر اصلاح می‌شود. در \lr{SOM} نیز همانند \lr{K-Means} اگر تعداد خوشه‌ها را از قبل نمی‌دانستیم می‌بایست عدد بزرگی را برای آن درنظر بگیریم. \cite{ref2}
	
	الگوریتم رقابتی در \lr{SOM} را می‌ةوان به دو صورت انجام داد.
	
	\begin{enumerate}
		\item ارسال سیگنال به نورون‌های دیگر
		\item محاسبه فاصله تا ورودی
	\end{enumerate}
	
	معمولا در تمامی شبکه‌های \lr{SOM} متداول است که از روش دوم استفاده شود اما در ادامه توضیح هر دو روش را خواهیم داد.	
	\end{enumerate}
\end{qsolve}




\begin{qsolve}
برای روش اول داریم:

ابتدا می‌بایست هر نرون خروجی‌اش را به صورت زیر تولید کند:

 $$ u_j=\sum_{i-1}^{n} w_{ij}x_i $$
 
 پس از محاسبه $u_j$ هر نورون خروجی $u_j$ خودش را با علامت معکوس به تمامی نورون‌های دیگر می‌فرستد. نورون‌ها پس از دریافت سایر $u_j$ ها، می‌بایست مقادیر $u_j$ های خودشان را با سایر $u_j$ های وارد شده جمع کنند. حالا اگر مقدار حاصل از یک آستانه کمتر شود، (مثلا صفر) نورون مربوطه از رقابت خارج می‌شود. این فرآیند تا زمانی ادامه پیدا خواهد کرد که فقط یک نورون باقی بماند و آن نورون به عنوان نورون برنده مشخص می‌شود.
 
 
 همانطور که گفته شد معمولا از این روش استفاده نمی‌شود و از روش دوم استفاده می‌شود. یعنی محاسبه فاصله تا ورودی. در این روش فاصله بردار ورودی طبق یکی از روابط تعیین فاصله (در اینجا فاصله اقلیدسی) برای تمامی وزن‌ها محاسبه می‌شود و نورونی که کمترین فاصله با بردار ورودی را داشته باشد به‌عنوان نورون برنده مشخص می‌شود.
 
 $$ d_j=x-w_j=\sqrt{\sum_{i=1}^{n} (x_i-w_{ij})^2} $$
 
 پس از محاسبه همه فاصله‌ها، مقدار وزن نورون برنده به‌صورت زیر آپدیت می‌شود:
 
 $$ \Delta w_j=\beta(x-w_j)=\beta d_j$$
 
 در نهایت، پس از همگراشدن شبکه، داده‌ها همگی در خوشه‌های مربوط به خودشان قرار می‌گیرند. شکل«»
 
 \begin{center}
 	\includegraphics*[width=0.8\linewidth]{pics/img8.png}
 	\captionof{figure}{داده‌های خوشه‌بندی شده شبکه \lr{SOM} پس از آموزش}
 	\label{داده‌های خوشه‌بندی شده شبکه SOM پس از آموزش}
 \end{center}
 
 
 از مزایا و معایب این دو الگوریتم می‌توان به موارد زیر اشاره کرد:

\end{qsolve}




\begin{qsolve}
	\begin{enumerate}
		\item \textbf{:K-Means}
		\begin{enumerate}
			\item مزایا: 
			\begin{itemize}
				\item الگوریتم \lr{K-Means} نسبت به شبکه \lr{SOM} از سرعت بالا تری برخوردار است. دلیل این افزایش سرعت، سادگی پیاده‌سازی آن به نسبت \lr{SOM} است.
				
				\item بر روی داده‌های بزرگ به‌خوبی کار می کند
			\end{itemize}
			
			
			\item معایب:
			\begin{itemize}
				\item کاملا وابسته و حساس به مقدار‌دهی اولیه برای نقاط مربوط به \lr{K} هاست.
				
				\item برای خوشه‌یابی داده‌هایی که ساختار غیر محدب دارند، نامناسب است.
				
				\item وابستگی شبکه به مشخص کردن تعداد خوشه‌ها پیش از اجرای الگوریتم
			\end{itemize}
		\end{enumerate}
		
		
		
		
		
		
		
		
		\item \textbf{:SOM}
		\begin{enumerate}
			\item مزایا:
			\begin{itemize}
				\item کاربردهای گسترده به جز خوشه‌یابی
				\item بدست آوردن روابط پیچیده میان داده‌ها
			\end{itemize}
			
			
			
			
			\item معایب:
			\begin{itemize}
				\item بار محاسباتی بیشتر و سرعت کمتر نسبت به الگوریتم \lr{K-Means} 
				
				\item وابسته بودن شبکه به پارامتر‌های مختلف ورودی مثل نرخ یادگیری، سایز همسایگی و ...
			\end{itemize}
		\end{enumerate}
	\end{enumerate}
	
	
\end{qsolve}



\begin{latin}
	\begin{thebibliography}{9}
		\bibitem{ref1}
		Data Mining: Practical Machine Learning Tools and Techniques, 2016.
		
		\bibitem{ref2}
		Neural Networks for Applied Sciences and Engineering, 2006 by Taylor \& Francis Group, LLC
		
		
		\bibitem{ref3}
		Frey BJ, Dueck D. Clustering by passing messages between data points. science. 2007 Feb 16;315(5814):972-6.
		
		\bibitem{ref4}
		Sculley D. Web-scale k-means clustering. InProceedings of the 19th international conference on World wide web 2010 Apr 26 (pp. 1177-1178).
		
		\bibitem{ref5}
		MacQueen J. Some methods for classification and analysis of multivariate observations. InProceedings of the fifth Berkeley symposium on mathematical statistics and probability 1967 Jun 21 (Vol. 1, No. 14, pp. 281-297).
		
		
		\bibitem{ref6}
		Comaniciu D, Meer P. Mean shift: A robust approach toward feature space analysis. IEEE Transactions on pattern analysis and machine intelligence. 2002 May;24(5):603-19.
		
		\bibitem{ref7}
		Ng A, Jordan M, Weiss Y. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems. 2001;14.
		
	\end{thebibliography} 
\end{latin}
	
	
	\newpage
	
	
	
	\item مجموعه داده ارائه شده در این پروژه شامل رویداد‌های سه‌سال متوالی از ۲۰۲۲ تا ۲۰۲۴ است که از سایت ویکی‌پدیا جمع‌آوری شده است. داده‌ی مربوطه را بارگزاری کنید و پیش‌پردازش‌های متنی شامل حذف کلمات ایست (\lr{Stop word})، واحدسازی کلمات (‫‪Tokenization‬‬) و تبدیل به بردار‌های \lr{GloVe} را روی آ
	ن انجام دهید.
	
	
	
	\begin{qsolve}
		فایل کد در مسیر \texttt{Code/Q1-P3.ipynb} موجود است. ابتدا کتابخانه‌های مورد نیاز برای کار با متن را نصب می‌کنیم، کتابخانه‌هایی مانند:
		
		\begin{latin}
			\begin{enumerate}
				\item \texttt{nltk}
				\item \texttt{textblob}
				\item \texttt{minisom}
			\end{enumerate}
		\end{latin}
	
	
	پس از نصب کتابخانه‌ها، دیتاست موجود (فایل \texttt{WikipediaEvents.csv}) را می‌خوانیم. ایعاد این دیتاست \texttt{1, 473} است.
	
	 \begin{center}
		\includegraphics*[width=0.7\linewidth]{pics/img9.png}
		\captionof{figure}{دیتاست ورودی}
		\label{دیتاست ورودی}
	\end{center}
	
	در ادامه، عملیات \lr{Pre-processing} شامل کوچک کوچک کردن تمامی حروف، حذف علائم نگارشی، اعداد و کاراکترهای \lr{newline} را حذف می‌کنیم. این عملیات‌ها در تابع \texttt{clean\_text} نوشته شده است. پس از اعمال تابع نام‌برده بر روی دیتاست موجود، همه متن به حروف کوچک تبدیل می‌شود، تاریخ تمامی سال‌ها حذف می‌شود و فقط ماه‌ مورد نظر باقی می‌ماند، همچنین کاراکتر‌های اضافی مانند \texttt{-} و \texttt{,} نیز حذف شده است. دیتاست پس از انجام این مراحل به صورت زیر می‌شود:
	
	\begin{center}
		\includegraphics*[width=0.7\linewidth]{pics/img10.png}
		\captionof{figure}{دیتاست ورودی پس از حذف علائم نگارشی و کوچک سازی کلمات}
		\label{دیتاست ورودی پس از حذف علائم نگارشی و کوچک سازی کلمات}
	\end{center}
	
	
	\end{qsolve}
	
	
	
	
	
	
	
	\begin{qsolve}
		برای پیدا کردن کلمات ایست، در دیتاست، از ماژول \texttt{stopwords} استفاده می‌کنیم. کلمات پرتکرار پیدا شده در این دیتاست به‌صورت زیر است:
		
		
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img11.png}
			\captionof{figure}{کلمات ایست}
			\label{کلمات ایست}
		\end{center}
		
		
		
		
		همچنین پس از پیدا کردن کلمات، برای حذف آنها تابع \texttt{remove\_stopwords} را نوشته‌ایم. پس از حذف، خروجی به صورت زیر می‌شود:
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img12.png}
			\captionof{figure}{خروجی پس از حذف کلمات ایست}
			\label{خروجی پس از حذف کلمات ایست}
		\end{center}
		
		پس از این مرحله، نوبت به \texttt{Tokenization} می‌رسد. برای انجام آن از ماژول \texttt{punkt} استفاده می‌کنیم. پس از انجام \texttt{Tokenization} خروجی به‌صورت زیر می‌شود:
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img13.png}
			\captionof{figure}{خروجی عملیات \lr{Tokenization}}
			\label{خروجی عملیات Tokenization}
		\end{center}
	
	
	پس از \texttt{Tokenization} می‌بایست تبدیل کلمات به بردار را انجام دهیم. برای انجام این کار، از مدل از‌پیش آموزش داده شده \texttt{GloVe} استفاده می‌کنیم. این مدل را می‌توان از \href{https://github.com/allenai/spv2/tree/master/model}{\textcolor{magenta}{اینجا}} دانلود کنید. پس از \lr{fine-tune} کردن مدل، از تابع نوشته شده \texttt{convert\_to\_vector} برای تبدیل کلمات به بردار استفاده می‌کنیم.
	\end{qsolve}
	
	
	
	
	
	
	
	
	\begin{qsolve}
		خروجی‌های \lr{Vectorizrd} شده را در فایلی با نام \texttt{word2vec\_out.csv} ذخیره می‌کنیم و خروجی آن به صورت زیر می‌شود:
		
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img14.png}
			\captionof{figure}{کلمات تبدیل شده به بردار}
			\label{کلمات تبدیل شده به بردار}
		\end{center}
		
در مرحله بعد، کلماتی که به بردار تبدیل کرده‌ایم را رسم می‌کنیم. ابعاد بردار‌های بدست آمده (۷۴۶۵،۱۰۰) است. می‌بایست بعد آن را کاهش دهیم تا بتوانیم آن را نمایش دهیم. برای کاهش بعد از ماژول \texttt{TSNE} استفاده کرده‌ایم. ابعاد بردار را به (۷۴۶۵،۲) کاهش می‌دهیم و آن را رسم می‌کنیم. خروجی آن به‌صورت زیر می‌شود:


	\begin{center}
		\includegraphics*[width=0.8\linewidth]{pics/img15.png}
		\captionof{figure}{نمایش بردارهای کلمات}
		\label{نمایش بردارهای کلمات}
	\end{center}
		
		
	\end{qsolve}
	
	
	
	
	
	 \begin{qsolve}
	 	به دلیل آنکه تعداد کلمات زیاد است و اسامی آن‌ها بر روی آن نوشته شده است و تصویر را شلوغ کرده است، لیبل‌ها را بر روی آن بر می‌داریم و دوباره آن را رسم می‌کنیم:
	 	
	 	\begin{center}
	 		\includegraphics*[width=0.8\linewidth]{pics/img16.png}
	 		\captionof{figure}{نمایش برداری کلمات بدون اسامی}
	 		\label{نمایش برداری کلمات بدون اسامی}
	 	\end{center}
	 	
	 	
	\end{qsolve}
	 	
	 	
	 	
	 	
	 	\item پارامتر‌های ورودی مدل \texttt{minisom} را توضیح دهید. پپارامترای شبکه خودسازمان‌ده خود را تنظیم کنید و شبکه را بر روی داده‌های مربوطه آموزش دهید. (مقادیر تمامی پارامتر‌ها را در گزارش خود اضافه کنید.) سپس به‌ازای هر داده‌ی ورودی واحد، منطبق (\lr{Best matching unit}) با آن را به‌دست آورید و به‌عنوان نمایه‌ی داده‌ی مربوطه ذخیره کنید.
	 	
	 	
	 \begin{qsolve}
	 	برای آموزش شبکه از کلاس \texttt{MiniSom} استفاده می‌کنیم. ورودی‌های این کلاس و مقادیر اولیه آن‌ها به صورت زیر هستند:
	 	
	 	\begin{latin}
	 		\texttt{\_\_init\_\_(self, x, y, input\_len, sigma=1.0, learning\_rate=0.5,}\\
	 		\texttt{decay\_function=asymptotic\_decay, neighborhood\_function='gaussian'}\\
	 		\texttt{topology='rectangular', activation\_distance='euclidean', random\_seed=None)}
	 	\end{latin}
	 	
	 	در ادامه به توضیح هر پارامتر می‌پردازیم.
	 \end{qsolve}
	 	
	 	
	 	
	 	
	 	
	 	
	 	
	 	
	 	
	 \begin{qsolve}
	 	
	 	\begin{enumerate}
	 		\item \lr{\texttt{x, y = (2, 3)}}
	 	
	 		\lr{\texttt{x, y}} ابعاد نورون‌های خروجی را مشخص می‌کند. مثلا اگر مقدار آن را به‌ترتیب ۱ و ۵ قرار دهیم بدین‌معناست که نرون‌های خروجی در ساختار پشت‌سر‌هم  در یک ساختار ۱ بعدی قرار می‌گیرند و ۵ نورون در خروجی شبکه داریم. پس در نتیجه ۵ خوشه داریم.
	 		
در این مسئله چون از تعداد خوشه‌ها اطلاعی نداریم مقدار آن را (۲،۳) درنظر گرفتیم. یعنی ساختار نورون‌های خروجی ۲ بعدی است و ۶ نورون (کلاس) در خروجی شبکه داریم.
	 		
	 		
	 		
	 		\item \lr{\texttt{input\_len=vectors\_2d.shape[1]}}
	 		
پارامتر \texttt{input\_len} طول (بعد) داده‌های ورودی شبکه‌را مشخص می‌کند و چون در مسئله ما داده‌ها ۲ بعدی هستند، پس این پارامتر را برابر با \texttt{vectors\_2d.shape[1]} یعنی مقدار ۲ قرار می‌دهیم.
			
			
			
			
			\item{\lr{\texttt{sigma=0.5}}}

پارامتر بعدی \texttt{sigma} است. این پارامتر، به نوعی نماینده انحراف معیار است و شعاع همسایگی را مشخص می‌کند. برای مثال در تکرار $t$ مقدار
$\sigma (t)$ به صورت زیر محاسبه می‌شود:

$$ \sigma (t)= \frac{\sigma}{1+\frac{t}{T}}, \qquad T=\frac{\text{\lr{number of iteration}}}{2}$$


هر چقدر مقدار این پارامتر را کوچکتر بگیریم، دقت خوشه‌بندی بالاتر می‌رود و خطای آموزش کمتر می‌شود. مقدار پیش‌فرض این پارامتر، ۱ است که ما در این مسئله آن را ۰٫۵ درنظر گرفتیم.



			\item{\lr{\texttt{learning\_rate=0.1}}}

 پارامتر \texttt{learning\_rate} نرخ یادگیری شبکه را مشخص می‌کند. این ضریب در اصلاح وزن نورون برنده شده خودش را نشان می‌دهد (در سوال اول به‌طور کامل توضیح داده شد). هر چقدر مقدار این پارامتر بزرگ باشد، شبکه ممکن است دچار ناپایداری شود. رنج نرمال این پارامتر در بازه ای بین ۰٫۰۱ تا ۰٫۱ قرار دارد. در این کلاس به طور پیش‌فرض مقدار نرخ‌یادگیری ۰٫۵ فرض شده است اما ما در این مثال مقدار آن را ۰٫۱ قرار دادیم.
 
 
 
 
 			\item{\lr{\texttt{decay\_function=asymptotic\_decay}}}
 			
 			این پارامتر، مقدار $\sigma$ و \texttt{learning\_rate} را در هر دوره کاهش می‌دهد. مقدار پیش فرض آن بر روی تابع \texttt{asymptotic\_decay} تنظیم شده است. این تابع به‌صورت زیر نوشته شده است:
 			
 			\begin{latin}
 				\texttt{def asymptotic\_decay(learning\_rate, t, max\_iter):}\\
 				\qquad \texttt{return learning\_rate / (1+t/(max\_iter/2))}
 			\end{latin}
	 		
	 		ورودی‌های آن \texttt{learning\_rate} و ماکزیمم تکرار (\texttt{max\_iter}) است.
	 		
	 		
	 		
	 		
	 		\item{\lr{\texttt{neighborhood\_function="gaussian"}}}
	 		
	 		این پارامتر، تابع همسایگی نام دارد، وظیفه آن نگاشت وزن ها تحت این توابع است. به صورت پیش‌فرض بر روی تابع \texttt{gaussian} تنظیم شده است اما می‌توان توابع \texttt{mexican\_hat}، \texttt{bubble} و \texttt{triangle} را نیز انتخاب کرد.
	 		
	 	\end{enumerate}
	\end{qsolve}
	
	
	
	
	
	\begin{qsolve}
		
	\begin{enumerate}
		\item{\lr{\texttt{topology="rectangular"}}}
		
		این پارامتر نوع توپولوژی نقشه را مشخص می‌کند، دو گزینه دارد که می‌توان انتخاب کرد. \texttt{hexagonal} و \texttt{rectangular} که به‌صورت پیش‌فرض بر روی \texttt{rectangular} تنظیم شده است.
	
	
	
		
		\item{\lr{\texttt{activation\_distance="euclidean"}}}
		این پارامتر تابع فاصله‌ای است که برای فعال شدن نقشه‌ها استفاده می‌شود مقدار پیش‌فرض آن بر روی \texttt{euclidean} تنظیم شده است اما می‌توان توابع \texttt{cosine}، \texttt{manhattan} و \texttt{chebyshev} را انتخاب کرد.
		
		
		
		
		\item{\lr{\texttt{random\_seed=10}}}
		
		این پارامتر رندوم بودن ورودی شبکه را تعیین می‌کند که به صورت پیش‌فرض بر روی \texttt{None} قرار دارد.
	\end{enumerate}
	
	
		
		پس از معرفی پارامتر‌های ورودی شبکه، با استفاده از کلاس \texttt{minisom} شبکه را در ۵۰۰۰۰ دوره آموزش می‌دهیم و خوشه‌های مناسب را پیدا می‌کنیم. این عملیات را برای ۵۰ داده رندوم از میان مجموعه داده‌ها مجددا تکرار می‌کنیم و نتایج آن را گزارش می‌دهیم.
		
		
		به ازای آموزش شبکه با همه داده‌ها در ۵۰۰۰۰ دوره آموزشی و با ۶ نرون دو بعدی در خروجی مقدار خطای شبکه به صورت زیر بدست آمده است:
		
		\begin{center}
			\includegraphics*[width=0.7\linewidth]{pics/img17.png}
			\captionof{figure}{خطای شبکه}
			\label{خطای شبکه}
		\end{center}
		
		خروجی شبکه پس از آموزش به‌صورت زیر به‌دست می‌آید:
		
		
		
		
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img18.png}
			\captionof{figure}{داده‌های خوشه‌بندی شده پس از آموزش}
			\label{داده‌های خوشه‌بندی شده پس از آموزش}
		\end{center}
		
	\end{qsolve}
	
	
	
	
	
	
	
	
	\item برای ۵۰ رویداد که به‌صورت تصادفی از مجموعه داده انتخاب شده‌اند، نقشه خروجی را رسم کنید. نقشه‌ی به‌دست آمده را تفسیر کنید.
	
	\begin{qsolve}
		
		ابتدا ۵۰ رویداد تصادفی را انتخاب و آن را رسم می‌کنیم:
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img20.png}
			\captionof{figure}{داده‌های رندوم انتخاب شده}
			\label{داده‌های رندوم انتخاب شده}
		\end{center}
		
		سپس شبکه را با داده‌های جدید (رندم) با همان پارامتر‌های قبل آموزش می‌دهیم. خروجی شبکه به صورت زیر می‌شود:
		
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img22.png}
			\captionof{figure}{داده‌های رندوم خوشه‌بندی شده}
			\label{داده‌های رندوم خوشه‌بندی شده}
		\end{center}
	
	\end{qsolve}
	
	
	
	\begin{qsolve}
		خطای آموزش نیز برای داده‌های رندوم به‌صورت زیر به‌دست می‌آید:
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img19.png}
			\captionof{figure}{خطای آموزش}
			\label{خطای آموزش برای داده‌های رندوم}
		\end{center}
		
	\end{qsolve}
	
	
	
	\item فرآیند جست‌و‌جو را به‌صورت زیر برای سه رویداد دلخواه از سه‌سال گذشته انجام دهید. (می‌توانید از پرسش‌های موجود درفایل \texttt{sample\_questions.txt} کمک بگیرید.) و خروجی مربوطه را در گزارش خود اضافه کنید.
	
	\begin{itemize}
		\item تبدیل پرسش به بردار
		\item پیداکردن نمایه‌ی متناسب با پرسش مربوطه
		\item پیدا کردن تمامی داده‌های خارجی نمایه‌ی مورد نظر
		\item محاسبه معیار شباهت کسینوسی و خروجی دادن بردار‌های داده‌های خارجی با شباهت بیشتر از آستانه. (چرا معیار کسینوسی در این مسئله انتخاب مناسبی است؟)
	\end{itemize}
	
	
	
	
	
	
	\begin{qsolve}
		در این قسمت ۱۴ سوال به‌صورت نمونه به ما داده شده است:
		
		\begin{latin}
			\texttt{Who won the 2022 soccer world cup?}\\
			\texttt{When did Sweden join NATO?}\\
			\texttt{Who joined NATO in 2023?}\\
			\texttt{Who joined NATO in 2024?}\\
			\texttt{Which is the 31st member of NATO?}\\
			\texttt{Which is the 32nd member of NATO?}\\
			\texttt{Who won the Cricket World Cup in 2023?}\\
			\texttt{Who defeated India in Cricket World Cup final in 2023?}\\
			\texttt{Name the former prime minister of Japan that was assassinated in 2022?}\\
			\texttt{When did Chandrayaan-3 land near the south pole of the Moon?}\\
			\texttt{Where did Chandrayaan-3 land on the Moon?}\\
			\texttt{Who acquired Twitter in 2022?}\\
			\texttt{Who owns Twitter?}\\
			\texttt{Who acquired Activision Blizzard in 2023?}
		\end{latin}
		
		
		در این قسمت در ابدا مشابه با قبل پرسش‌های داده شده را به بردار‌های \texttt{GloVe} تبدیل کرده و مجددا خوشه‌یابی را انجام می‌دهیم. بردار‌های تبدیل شده را در فایلی با نام \texttt{sample\_questions\_vector.csv} ذخیره می‌کنیم.
	
	
	خروجی این تبدیل به‌صورت زیر می‌شود
	\end{qsolve}
	
	
	
	
	\begin{qsolve}
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img23.png}
			\captionof{figure}{تبدیل سوالات به بردار}
			\label{تبدیل سوالات به بردار}
		\end{center}
		
		نمایش ۲بعدی بردار‌ها به‌صورت زیر می‌شود:
		
		\begin{center}
			\includegraphics*[width=0.8\linewidth]{pics/img24.png}
			\captionof{figure}{نمایش ۲بعدی بردار‌های سوال}
			\label{نمایش ۲بعدی بردار‌های سوال}
		\end{center}
	\end{qsolve}
	
	
	
	\begin{qsolve}
		خروجی خوشه‌بندی شده به‌صورت زیر می‌شود:
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img25.png}
			\captionof{figure}{سوالات خوشه‌بندی شده}
			\label{سوالات خوشه‌بندی شده}
		\end{center}
		
		
		سپس به سراغ معیار شباهت کسینوسی می‌رویم. معیار شباهت کینوسی، معیاریست برای بررسی شباهت میان دو بردار غیر صفر بر اساس کسینوس زاویه بین آنها که درنتیجه مقداری بین ۱− و ۱ بدست می‌آید. مقدار ۱− دوبردار متعامد و مقدار ۱ دو بردار مشابه را نشان می‌دهد.
		
		\begin{center}
			\includegraphics*[width=0.6\linewidth]{pics/img25.png}
			\captionof{figure}{سوالات خوشه‌بندی شده}
			\label{سوالات خوشه‌بندی شده}
		\end{center}
		
	\end{qsolve}
	
	
\end{enumerate}


%
%فرض کنید یک مجموعه داده دو کلاسه در اختیار دارید که کاملا به صورت خطی کلاس‌ها از هم جداپذیر هستند. یک شبکه چند لایه پرسپترونی (با طراحی دلخواه) طراحی نموده‌اید که لایه خروجی آن شامل دو نرون می‌باشد که تابع فعال‌ساز \texttt{softmax} بر آن اعمال می‌شود. در زمان آموزش، از تابع خطای \texttt{entropy cross binary} برای محاسبه خطا و بهینه‌سازی وزن ها استفاده می‌شود. آیا این امکان وجود دارد که خطای حاصل صفر شود؟ اگر امکان ندارد، با استدلال و اثبات ریاضی نشان دهید و اگر امکان دارد، با معرفی چهار داده (دو داده به ازای هر کلاس) و پرسپترون مد نظرتان نشان دهید که خطا می‌تواند دقیقا صفر شود.
%
%
%\begin{qsolve}
%	فایل کد از مسیر \texttt{code/Q1.ipynb} قابل مشاهده است.
%	
%	
%	در این سوال برای تولید دیتا از تابع \texttt{make\_blobs} از کتابخانه \texttt{sklearn} به صورت زیر استفاده شده است:‌
%	
%	\begin{latin}
%		\texttt{x, y = datasets.make\_blobs(n\_samples=200, centers=[(-1, -1), (1, 1)], cluster\_std=0.5)}
%	\end{latin}
%	
%	که داده هایی با تعداد ۲۰۰ نمونه و انحراف معیار ۰٫۵ حول نقطه های (1-,1-) و (۱و۱) می‌کند که کاملا جداپذیر خطی است «شکل \ref{داده‌های تولید شده برای مسئله}»
%	
%	\begin{center}
%		\includegraphics*[width=0.9\linewidth]{pics/img1.png}
%		\captionof{figure}{داده‌های تولید شده برای مسئله}
%		\label{داده‌های تولید شده برای مسئله}
%	\end{center}
%	
%	
%	برای حل این مسئله از شبکه‌ی پرسپترونی چند لایه ای با ساختار «شکل \ref{ساختار شبکه طراحی شده در سوال ۱}» استفاده شده است. با توجه به اینکه داده‌ها جداپذیر خطی هستند، می‌توان این مسئله را با یک نرون پرسپترونی نیز حل نمود اما با توجه به اینکه در صورت سوال گفته شده است شبکه ای چند لایه طراحی کنید، از شبکه چند لایه پرسپترونی استفاده کردیم.
%\end{qsolve}
%
%
%
%
%\begin{qsolve}
%	\begin{center}
%		\includegraphics*[width=0.5\linewidth]{pics/img2.pdf}
%		\captionof{figure}{ساختار شبکه طراحی شده}
%		\label{ساختار شبکه طراحی شده در سوال ۱}
%	\end{center}
%	
%	تابع فعال‌ساز لایه مخفی، \texttt{ReLU} درنظر گرفته شده است و در لایه خروجی نیز از \texttt{softmax} استفاده شده است. طبق خواسته مسئله، از \textbf{entropy cross binary} به‌عنوان تابع خطا استفاده شده است. همچنین از تابع بهینه‌ساز \texttt{ADAM} در این مسئله استفاده شده است.
%	
%	نتایج آموزش در ۲۰۰ دوره آموزشی به صورت زیر گزارش می‌شود:
%	
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img3.png}
%		\captionof{figure}{منحنی خطا و دقت برای داده‌های آموزش و تست}
%		\label{منحنی خطا و دقت برای داده‌های آموزش و تست}
%	\end{center}
%\end{qsolve}
%
%
%
%
%\begin{qsolve}
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img4.png}
%		\captionof{figure}{ماتریس پراکندگی}
%		\label{ماتریس پراکندگی سوال ۱}
%	\end{center}
%	
%	
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img5.png}
%		\captionof{figure}{ناحیه تصمیم}
%		\label{ناحیه تصمیم سوال ۱}
%	\end{center}
%
%\end{qsolve}
%




%
%\begin{center}
%	\includegraphics*[width=1\linewidth]{pics/img1.png}
%	\captionof{figure}{مسئله مورد بحث}
%	\label{مسئله مورد بحث در سوال۱}
%\end{center}
%
%
%
%\begin{enumerate}
%	\item شکل \lr{a-1} را برای دسته بندی مسئله دودویی درنظر بگیرید. معماری نورون مورد نظر را توضیح داده و وزن‌های آن را بدست آورید.
%	
%	\begin{qsolve}
%		به دلیل آنکه داده های این قسمت جداپذر خطی هستند، می‌توان برای طبقه بندی آنها، از شبکه پرسپترون تک لایه استفاده کرد.
%		
%		معماری این شبکه به صورت «شکل \ref{معماری شبکه پرسپترون تک لایه}» است.
%		
%		
%		\begin{center}
%			\includegraphics*[width=0.8\linewidth]{pics/img2.pdf}
%			\captionof{figure}{معماری شبکه پرسپترون تک لایه}
%			\label{معماری شبکه پرسپترون تک لایه}
%		\end{center}
%	\end{qsolve}
%	
%	\begin{qsolve}
%				در این شبکه، ورودی/خروجی ها با مربع های نارنجی، نورون ها با دایره سبز و تابع فعال ساز با مربع آبی نشان داده شده است. تعداد دیتا ورودی شبکه ۲ است. $x_1$ و $x_2$. بایاس این شبکه با $x_0$ نشان داده شده است. وزن های شبکه نیز با $w$ نشان داده شده است. بنابر این بردار ورودی و وزن‌های شبکه به صورت زیر است:
%		
%		$$
%			X=\begin{bmatrix}          
%				x_0=1\\
%				x_1\\
%				x_2
%				
%				\end{bmatrix} \\\       W=\begin{bmatrix}          
%												w_0\\
%												w_1\\
%												w_2
%											
%											\end{bmatrix}
%		$$
%		
%		طبق تئوری شبکه‌های عصبی می‌دانیم خروجی نرون به صورت زیر محاسبه می‌شود: (در اینجا برای انجام محاسبات ساده، تابع فعال‌ساز درنظر گرفته نشده است)
%		
%		$$
%			\hat{y}=W^T X = \sum_{i=0}^{2} {w_ix_i}=w_0x_0+w_1x_1+w_2x_2 \xrightarrow{x_0=1} w_0+w_1x_1+w_2x_2
%		$$
%		
%		طبق شکل \lr{a-1} دو نقطه از خط جدا کننده دو کلاس را داریم. بنابراین می‌توان معادله خط را به صورت زیر نوشت.
%		
%		می‌دانیم معادله خط به صورت زیر تعریف می‌شود:
%		$$
%			y-y_0=m(x-x_0)
%		$$
%		
%		که در آن $m$ شیب خط است و به صورت زیر $\frac{\Delta y}{\Delta x}$ تعریف می‌شود. با جاگذاری یک از نقاط در معادله خط، می‌توان معادله خط را بدست آورد.
%		
%		$$
%			P_1=\begin{bmatrix}          
%				2.5\\
%				0
%				
%			\end{bmatrix} \\\       P_2=\begin{bmatrix}          
%				0\\
%				2.8
%				
%			\end{bmatrix}
%		$$
%		
%		$$
%			m=\frac{2.8-0}{0-2.5}=-1.12 \rightarrow y-0=-1.12(x-2.5) \rightarrow \boxed{y=-1.12x+2.8}
%		$$
%		
%		حالا اگر معادله خروجی نورون را به صورت زیر مرتب کنیم، می‌توان از مقایسه با مقادله خط بدست آمده وزن‌های شبکه را تعیین کرد.
%		
%		\begin{eqnarray*}
%			x_1=\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1},
%			&x_2=\frac{-w_1}{w_2}x_1-\frac{w_0}{w_2}&
%		\end{eqnarray*}
%
%		
%		در اینجا به دلیل آنکه دو معادله و ۳ مجهول ($w_0,w_1,w_2$) داریم، نیاز است که یکی از وزن ها را فرض کرده و دو وزن دیگر را بدست آورد.
%		
%		\begin{eqnarray*}
%			\frac{-w_2}{w_1}=-1.12 \rightarrow w_2=1.12w_1\\
%			\frac{-w_0}{w_1}=2.8 \rightarrow w_0=-2.8w_1\\
%			\rightarrow
%			\begin{cases}
%				w_2-1.12w_1=0\\
%				-2.8w_1-w_0=0 
%			\end{cases}
%			\text{assume} &w_0&=2.8 \rightarrow \text{\hl{$w_1=-1$}}, \text{\hl{$w_2=-1.12$}}
%		\end{eqnarray*}
%		ذکر این نکته الزامیست که این جواب، یکتا نمی‌باشد و برحسب اینکه مقدار $w_0$ را چه انتخاب کنیم، مقدار ۲ وزن دیگر متفاوت می‌شود.
%	\end{qsolve}
%	
%	
%
%	
%	\item حال شکل \lr{b-1} را درنظر بگیرید. چرا مسئله جداپذیر خطی نیست؟ چگونه می‌توان آن را در قالب حل چند مسئله خطی حل نمود؟ معماری پیشنهادی خودتان را رسم و وزن‌های موجود در آن را با انجام محاسبات بدست آورید. معماری شما می‌تواند حاصل از کنار هم چیدن و پشت هم چیدن یک یا چند نورون پرسپترونی باشد.
%	
%	
%	
%	\begin{qsolve}
%		به مسائلی جداپذیر خطی گفته می‌شود که بتوان داده‌ها (کلاس‌ها) را با استفاده از فقط یک خط از هم جدا کرد. در این مثال، دو کلاس آبی و سبز را نمی‌توان فقط با یک خط از هم جدا کرد. بنابراین این مسئله \textbf{جداپذیر خطی نمی‌باشد.} برای حل این مسئله، چندین روش وجود دارد که در ادامه آنها را توضیح خواهم داد.
%		
%		\begin{enumerate}
%			\item \textbf{افزایش ابعاد ویژگی های مسئله: }\\
%			یعنی در این مثال که مسئله مورد بحث ما دو بعدی است، یک بعد به آن اضاف کنیم و آن را به عنوان یک مسئله ۳ بعدی حل کنیم و تلاش کنیم که یک صفحه برای جدا کردن دو کلاس پیدا کنیم.
%			
%			\item \textbf{دادن ورودی هایی با توان بالا به عنوان ورودی شبکه: }\\
%			در صورتی می‌توان از این روش استفاده کرد که مقدار دقیق تمام نمونه ها را داشته باشیم. در این صورت می‌توان ورودی ها را به توان های بالا (۲ و ۳ و...) رساند و امیدوار باشیم فضای قرارگیری ویژگی های جدید در صفحه به صورت جداپذیر خطی باشد. «این کار در سوال دوم همین سری تمرین انجام شده است و با به توان ۲ رساندن ویژگی های ورودی شبکه، مسئله ای که جداپذیر خطی نبود به جداپذیر خطی تبدیل می‌شود.» در این مسئله به دلیل نداشتن موقعیت دقیق نمونه های هرکلاس نمی‌توان از این روش استفاده کرد.
%			
%			\item \textbf{اضاف کردن لایه مخفی: }\\
%			در اضاف کردن تعداد لایه های مخفی شبکه، آزاد هستیم اما باید به این نکته توجه داشت که اگر بیشتر از یک لایه مخفی به شبکه اضاف کنیم، حل مسئله از نظر خطی بودن خارج شده و نواحی تصمیم گیری شامل خط راست نمی‌شود و نواحی پیچیده تری مانند منحنی ها را در بر می‌گیرد. اما اگر فقط یک لایه مخفی ۳ نورونی به شبکه اضاف کنیم، می‌توان مسئله ای که ذاتا جدا‌پذیر خطی نیست را به وسیله ۳ خط جدا کننده که تعداد این خط ها برابر است با تعداد نورون های لایه مخفی، حل نمود. ساختار این مدل در «شکل \ref{معماری شبکه ۳ لایه پیشنهادی}» آورده شده است.
%			
%			اما مشکلی که وجود دارد آن است که در صورت سوال از ما خواسته شده وزن های شبکه را بدست آوریم، اگر از این ساختار برای حل استفاده کنیم، به دلیل نداشتن مقدار ورودی ها نمی‌توان ۳ وزن لایه متصل به خروجی را بدست آورد.
%			
%			\item \textbf{شکستن مسئله به ۳ زیرمسئله خطی}: \\
%			برای حل این سوال از این روش استفاده شده است. در قسمت قبل دیدیم که یک نورون پرسپترونی می‌تواند یک خد جدا کننده در صفحه رسم کنید. در این مثال، برای جدا کردن این دو کلاس به ۳ خط نیاز داریم. پس داده‌های ورودی مسئله را به ۳ بخش جدا‌پذیر خطی تقسیم می‌کنیم. «شکل \ref{داده های شکسته شده به سه بخش}» اکنون می‌توان همانند قسمت قبل، وزن ها را برای هر سه زیر‌مسئله بدست آورد. در این قسمت ساختار و معماری شبکه همان ساختار قسمت \lr{a-1} است. «شکل \ref{معماری شبکه پرسپترون تک لایه}»
%			
%			
%		\end{enumerate}
%	\end{qsolve}
%	
%	
%	
%	\begin{qsolve}
%		\begin{center}
%			\includegraphics*[width=0.5\linewidth]{pics/img3.pdf}
%			\captionof{figure}{معماری شبکه ۳ لایه پیشنهادی}
%			\label{معماری شبکه ۳ لایه پیشنهادی}
%		\end{center}
%		
%		\begin{center}
%			\includegraphics*[width=1\linewidth]{pics/img4.pdf}
%			\captionof{figure}{داده های شکسته شده به سه بخش}
%			\label{داده های شکسته شده به سه بخش}
%		\end{center}
%		
%		
%		زیر مسئله شماره ۱ در قسمت قبل حل شده است و وزن های آن به صورت زیر بدست آورده شد:
%		\begin{eqnarray*}
%			w_0=2.8, &w_1=-1, &w_2=-1.12
%		\end{eqnarray*}
%		
%		برای دو زیر مسئله دیگر هم، همانند قسمت قبل وزن‌ها را بدست می‌اوریم.
%		
%		\begin{eqnarray*}
%			P_1=\begin{bmatrix}
%				2.5\\
%				0
%			\end{bmatrix}
%			P_2&=&\begin{bmatrix}
%				-1.5\\
%				1.7
%			\end{bmatrix}
%			\rightarrow m=\frac{\Delta y}{\Delta x}=\frac{y_2-y_1}{x_2-x_1}=\frac{-1.5-2.5}{-1.7-0}=\frac{-4}{-1.7}=\boxed{2.35}\\
%			y-0&=&2.35(x-2.5)\rightarrow \boxed{y=2.35x-5.88}\\
%			\rightarrow x_1&=&\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1} \rightarrow \begin{cases}
%				\frac{-w_2}{w_1}=2.35 \rightarrow w_2=-2.35w_1\\
%				\frac{-w_0}{w_1}=-5.88 \rightarrow w_0=5.88w_1 
%			\end{cases}\\
%			\text{Assume}\qquad w_0&=&5.88 \rightarrow \text{\hl{$w_1=1$}}, \text{\hl{$w_2=-2.35$}}
%		\end{eqnarray*}
%	\end{qsolve}
%	
%	
%	
%	
%	
%	
%	\begin{qsolve}
%		\begin{eqnarray*}
%			P_1=\begin{bmatrix}
%				-1.5\\
%				-1.7
%			\end{bmatrix}
%			P_2&=&\begin{bmatrix}
%				0\\
%				2.8
%			\end{bmatrix}
%			\rightarrow m=\frac{\Delta y}{\Delta x}=\frac{y_2-y_1}{x_2-x_1}=\frac{0+1.5}{2.8+1.7}=\frac{1.5}{4.5}=\boxed{0.33}\\
%			y-2.8&=&0.33(x-0)\rightarrow \boxed{y=0.33x+2.8}\\
%			\rightarrow x_1&=&\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1} \rightarrow \begin{cases}
%				\frac{-w_2}{w_1}=0.33 \rightarrow w_2=-0.33w_1\\
%				\frac{-w_0}{w_1}=2.8 \rightarrow w_0=-2.8w_1 
%			\end{cases}\\
%			\text{Assume}\qquad w_0&=&2.8 \rightarrow \text{\hl{$w_1=-1$}}, \text{\hl{$w_2=0.33$}}
%		\end{eqnarray*}
%	\end{qsolve}
%	
%	
%	
%	
%	
%	
%	
%	
%	\item  شگرد هسته\footnote{\lr{Kernel trick}} چیست و چگونه می‌توان قسمت قبل را با آن حل نمود؟ توضیح دهید.
%	
%	\begin{qsolve}
%		این تکنیک به عنوان یکی از روش های حل مسئله قبل معرفی شد و توضیح مختصری در مورد آن داده شد. در این قسمت توضیخات کامل آن را بیان خواهیم کرد.
%		
%		روش شگرد هسته، یکی از چندین روش موجود برای حل مسائل جداناپذیر خطی با روش های خطی است. بدین صورت که اگر ابعاد ویژگی های مسئله را R در نظر بگیریم و داده ها در R بعد جداناپذیر خطی باشند، ممکن است با افزایش بعد ویژگی ها (R+n) و اضاف کردن ویژگی ای جدید، داده ها جداپذیر خطی شوند و بتوان مسئله را با یک شبکه خطی حل نمود.
%		
%		در این سوال، ویژگی های ورودی ۲ بعدی هستند و داده ها در ۲ بعد جداناپذیر خطی هستند. بنابر این می‌توان یک ویژگی جدید در بعد سوم به ورودی ها اضاف کرد و خروجی را نمایش داد تا شاید داده ها جدا‌پذیر خطی شوند.
%		
%		این کار را انجام دادیم و اثباط کردیم که با افزایش بعد این مسئله، داده ها جداپذیر خطی می‌شوند. شکل زیر را به عنوان داده های ورودی مسئله در نظر بگیریم:
%		
%		\begin{center}
%			\includegraphics*[width=0.6\linewidth]{pics/img5.png}
%			\captionof{figure}{داده های ورودی مسئله در ۲ بعد}
%			\label{داده های ورودی مسئله در ۲ بعد}
%		\end{center}
%		
%		برای افزودن بعد سوم به این داده های ۲ بعدی، تابع زیر را نوشته ایم.
%	\end{qsolve}
%	
%	
%	\begin{qsolve}
%		
%		\begin{latin}
%			\texttt{def kernel\_trick(x):}\\
%			\texttt{return np.append(x, np.expand\_dims(x[:, 0]**2 ** x[:, 1]**2, axis=1), axis=1)}
%		\end{latin}
%		
%		این تابع ویژگی جدید ${(x_0^2)}^{x_1^2}$ را به داده های ورودی به عنوان بعد سوم اضافه می‌کند. بنابر در فضای ویژگی جدید، هر نقطه از بردار ویژگی شامل ۳ عضو است. (\textbf{این نکته لازم به ذکر است که ویژگی جدید تولید شده، با سعی و خطا بدست آمده است و می‌توان به جای آن هر ویژگی جدید دیگری را قرار داد})
%		
%		
%		پس از رسم فضای ویژگی جدید، مشاهده می‌شود که داده ها جداپذیر خطی شده اند و می‌توان آنها را با یک صفحه از هم جدا نمود.
%		
%		\begin{center}
%			\includegraphics*[width=0.8\linewidth]{pics/img6.png}
%			\captionof{figure}{فضای ویژگی های جدید در ۳ بعد}
%			\label{فضای ویژگی های جدید در ۳ بعد}
%		\end{center}
%	\end{qsolve}
%\end{enumerate}
%
%
%
%	
%
%		
%		
%		
%		
%			
%		
%		
%		
%		
%		
%			
%		
%		
%		
%		
%
%		
%	
%	
%	
%
%	
%	
%	
%	
%
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%
%
%
%
%
%
%
%
%
%
%
%
%%
%%
%%
%%
%%
%%
%%
%%\begin{qsolve}[]
%%	\begin{eqnarray*}
%%		p(t)&=&\sum_{n=-\infty}^{\infty}\delta(t-nT)\qquad \text{\lr{periodic in T}}\\
%%		a_k &=&\frac{1}{T}\int_{-T/2}^{T/2}f(t)e^{jk(2\pi/T)t}dt=
%%		\frac{1}{T}\int_{-T/2}^{T/2}\sum_{n=-\infty}^{\infty}\delta(t-nT)e^{jk(2\pi/T)t}dt
%%		\xrightarrow{n=0}\\
%%		&=&\frac{1}{T}\int_{-T/2}^{T/2}\delta(t)e^{jk(2\pi/T)t}dt=\frac{1}{T}
%%		\hspace{8em}\text{\hl{$a_k=\frac{1}{T}$}}
%%	\end{eqnarray*}
%%	تابع متناوب است و سری فوریه دارد، پس تبدیل فوریه آن همان سری فوریه آن به صورت
%%	جمع ضربه هاست.
%%	\begin{eqnarray*}
%%		P(j\omega)&=&\mathcal{F}\left\{\sum_{k=-\infty}^{\infty}a_ke^{jk(2\pi/T)t}\right\}=\sum_{k=-\infty}^{\infty}2\pi a_k\delta(\omega-k\frac{2\pi}{T})
%%		=\frac{2\pi}{T}\sum_{k=-\infty}^{\infty}\delta(\omega-k\frac{2\pi}{T})
%%	\end{eqnarray*}
%%\end{qsolve}
%%
%%با توجه به اینکه با استفاده از این سیگنال نمونه برداری قطار ضربه از یک سیگنال پیوسته زمان انجام میدهیم و با استفاده نتیجه بدست آمده در قسمت
%%قبل تبدیل فوریه سیگنال بدست آمده حاصل از ضرب این سیگنال در سیگنال دلخواه $x(t)$ را بدست آورید.
%%
%%\begin{qsolve}[]
%%	\begin{eqnarray*}
%%		x_p(t)&=&x(t)\times p(t)\Rightarrow X_p(j\omega)=\frac{1}{2\pi}X(j\omega)*P(j\omega)
%%		=\frac{1}{2\pi}\convolve[\omega]{P}{X}\\
%%		X_p(j\omega)&=&\frac{1}{2\pi}\sum_{k=-\infty}^{\infty}\intinf \frac{2\pi}{T}\delta(\tau-k\frac{2\pi}{T})X(j(\omega-\tau))d\tau
%%		=\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))
%%	\end{eqnarray*}
%%\end{qsolve}
%%
%%حال نتیجه بدست آمده از قسمت قبل را با تبدیل فوریه گسسته سیگنال نمونه برداری شده $x[n]=x(nT)$ مقایسه کنید. حال شرطی روی نرخ
%%نمونه برداری $F_s=\frac{1}{T_s}$ بدست آورید که تمام محتوای فرکانسی سیگنال اولیه پس از نمونه برداری حفظ شود. (شرط نایکوییست)
%%
%%\begin{qsolve}[]
%%	پاسخ بدست آمده متناوب است، پس میتوان آن را به صورت یک سری فوریه نوشت.
%%	یعنی داریم که : $X(j\omega)=\sum_{n=-\infty}^{\infty}C_ke^{jk\omega_0\omega}$
%%	\splitqsolve
%%	\begin{eqnarray*}
%%		X_p(j\omega)&=&\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))
%%		\xrightarrow{\text{\lr{periodic in $\frac{2\pi}{T}$}}}\sum_{n=-\infty}^{\infty}C_ne^{jnT\omega}\\
%%		C_n&=&\frac{1}{2\pi/T}\int_{-\pi/T}^{\pi/T}X_p(j\omega)e^{jnT\omega}d\omega
%%		=\frac{T}{2\pi}\int_{-\pi/T}^{\pi/T}\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))e^{jnT\omega}d\omega\\
%%		&=&\xrightarrow{k=0}\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}X(j\omega)e^{jnT\omega}d\omega
%%	\end{eqnarray*}
%%	حال اگر داشتیم که $X(j\omega)\{|\omega|\leq\frac{\pi}{T}\}=0$
%%	\begin{eqnarray*}
%%		C_n&=&\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}X(j\omega)e^{jnT\omega}d\omega=
%%		\frac{1}{2\pi}\int_{-\infty}^{\infty}X(j\omega)e^{jnT\omega}d\omega=x(nT)\\
%%		X_p(j\omega)&=&\sum_{n=-\infty}^{\infty}C_ne^{jnT\omega}=\sum_{n=-\infty}^{\infty}x(nT)e^{jnT\omega}
%%		=\sum_{n=-\infty}^{\infty}x[n]e^{jnT\omega}=X(e^{j\Omega})\when_{\Omega=\omega T}
%%	\end{eqnarray*}
%%	که این به ما شرط نایکوییست را میدهد. به صورتی که:
%%	\[
%%		\text{ : اگر داشته باشیم}
%%        \forall \omega \geq \omega_s = \frac{\pi}{T}: X(j\omega)=0\Longrightarrow X_p(j\omega)=X(e^{j\Omega})\when_{\Omega=\omega T}
%%	\]
%%\end{qsolve}
%%
%%فرض کنید به علت محدودیت هایی که داریم شرط نایکوییست برقرار نباشد در این حالت روشی پیشنهاد دهید که محتوای فرکانسی کمتری از سیگنال
%%در اثر نمونه برداری از بین برود.
%%
%%\begin{qsolve}[]
%%    ناچار ایم که مقداری اطلاعات از دست بدهیم، زیرا در حالت خالص نمونه برداری، شرط نایکوییست برقرار نیست 
%%    و دچار اعوجاج فرکانسی هستیم.
%%
%%    میتوانیم قبل از نمونه برداری اطلاعات فرکانسی را کم کنیم، اینگونه اطلاعات از بین رفته و 
%%    فرکانسی اطلاعات کاهش میابد، ولی حداقل اعوجاج فرکانسی نداریم و نویز با فرکانس پایین ایجاد نمیکنیم.
%%
%%    به این کار Anti-Aliasing میگوییم. به این صورت که قبل از نمونه برداری، یک \lr{low-pass filter} استفاده میکنیم.
%%
%%    \begin{center}
%%        \includegraphics*[width=0.8\linewidth]{pics/anti-aliasing.png}
%%        \captionof{figure}{\lr{anti aliasing diagram}}
%%    \end{center}
%%\end{qsolve}