\section{سوال اول - عملی نظری}

برای آموزش مدل‌های زبانی بزرگ (\lr{Large Language Model}) که حاوی میلیون‌ها و میلیارد‌ها پارامتر هستند، از حجم قابل توجهی داده استفاده می‌شود. اما در تمامی این مدل‌ها یک تاریخ قطع آموزش وجود دارد که مدل زبانی هیچ اطلاعاتی در خصوص داده‌های تولید شده‌ی پس از این زمان ندارد. به‌عنوان مثال، تاریخ قطع آموزش مدل \lr{GPT-3.5-turbu-instruction} سپتامبر ۲۰۲۱ است و از همین رو این مدل ممکن است به سوالات مربوط به رویداد‌های سال ۲۰۲۲، ۲۰۲۳ و ۲۰۲۴ پاسخ صحیح ندهد. چنین داده‌هایی که بعد از تاریخ قطع آموزش تولید شده‌اند و یا بخشی از داده‌ی آموزشی اولیه‌ی مدل زبانی نیستند را داده‌ی خارجی می‌گوییم. تکنیک تولید تقویت شده با بازیابی (\lr{RAG}) رویکردی است که با استخراج داده‌ی خارجی متناسب با فرمان، دریافت شده و افزودن آن به‌عنوان ورودی به مدل زبانی تلاش می‌کند که فرمان ورودی را تقویت کرده و به مدل زبانی کمک می‌کند تا جواب مرتبط و متناسبی بسازد. به‌عنوان مثال در پاسخ به یک فرمان متنی مانند «چه‌کسی شرکت توییتر را درسال ۲۰۲۲ خرید؟» تمامی داده‌های خارجی متناسب با این فرمان را استخراج می‌کند و آن‌ها را به‌عنوان ورودی به مدل زبانی \lr{GPT-3.5-turbo-instruct}
می‌دهد تا مدل زبانی بتواند با دانش دریافت شده پاسخ متناسبی تولید کند. این رویکرد نیاز به آموزش مجدد و با باز‌تنظیم (\lr{Fine tune})
مدل زبانی را برطرف می‌سازد. در این پروژه می‌خواهیم با استفاده از شبکه‌های خودسازمان‌ده این تکنیک را پیاده‌سازی کنیم.

\begin{center}
	\includegraphics*[width=0.8\linewidth]{pics/img1.png}
	\captionof{figure}{فرآیند کلی RAG در یک مدل زبانی بزرگ}
	\label{داده‌های تولید شده برای مسئله}
\end{center}


وظیفه اصلی RAG جست‌و‌جو معنایی (\lr{Semantic search}) در پایگاه داه‌های اطلاعاتی و بازیابی اطلاعات خارجی دارای تناسب محتوایی با فرمان داده‌شده به یک مدل زبانی است.برای تسهیل جست‌و‌جوی معنایی، ابتدا داده‌های خارجی استخراج شده به بازنمایی‌های عددی یا برداری تبدیل می‌شوند که به این بازنمایی، تعبیه‌ی متن (\lr{Text embedding}) می‌گوییم. در زمان بازیابی نیز ابتدا فرمان متنی به بازنمایی برداری تبدیل می‌شود و سپس نزدیک‌ترین بردار‌های داده‌ی خارجی متناسب با آن استخراج می‌شود. شکل «\textcolor{blue}{\ref{داده‌های تولید شده برای مسئله}}»
دیاگرام کلی این فرآیند را نشان می‌دهد. چالش اصلی این رویکرد این است که جست‌و‌جوی معنایی ذکر شده به دلیل نیازمندی به محاسبه‌ی فاصله‌ی بردار فرمان با حجم عظیمی از بردار‌های داده‌ی خارجی، به منابع پردازشی و محاسباتی زیاد و زمان قابل توجهی نیاز دارد. بنابر این پیدا کردن رویکردی که جست‌و‌جوی معنایی را به‌صورت کارا انجام دهد بسیار حائز اهمیت است.

برای افزایش کارایی جست‌و‌جو معنایی، یک رویکرد رایج این است که بردار‌های داده‌های خارجی را خوشه‌بندی کنیم و در زمان جست‌و‌جو نیز ایتدا خوشه مشابه با بردار فرمان ورودی را پیدا می‌کنیم و سپس شباهت بردار‌های داده‌های خارجی متعلق به آن خوشه با بردار فرمان را محاسبه می‌کنیم و اگر شباهت بردار‌ها از یک آستانه بیشتر باشد، آنها را به‌عنوان اطلاعات مرتبط درنظر می‌گیریم.

\begin{enumerate}
	\item در این پروژه قصد داریم برای خوشه‌بندی داده‌های خارجی از شبکه خود‌سازمان‌ده استفاده کنیم. بررسی کنید که در این شبکه‌ها نسبت به سایر روش‌های خوشه‌بندی که در یادگیری ماشین به‌کار گرفته می‌شود، چه مزایا و معایبی دارد؟ به نظر شما، چرا استفاده از شبکه خودسازمان‌ده به صورت با نظارت صورت نمی‌گیرد؟ فرآیند یادگیری این مدل‌ها را توضیح دهید.
	
	\begin{qsolve}
قبل از بررسی مزایا و معایب شبکه \lr{SOM} نیاز است که یک‌سری پیش‌نیاز ها را توضیح دهیم. پیش از هر چیزی ابتدا می‌بایست انواع الگوریتم های یادگیری ماشین و دلیل استفاده از آنها‌را توضیح دهیم. الگوریتم های یادگیری ماشین به ۳ دسته مختلف تقسیم می‌شوند:

	\begin{enumerate}
		\item یادگیری با نظارت (\lr{Supervised Learning})
		\item یادگیری نیمه نظارتی (\lr{Semi-supervised Learning})
		\item  یادگیری بدون نظارت (\lr{Unsupervised Learning})
	\end{enumerate}
در یادگیری بانظارت، داده و لیبل‌های متناظر با آنها را داریم. در یادگیری نیمه نظارتی، صرفا بخشی از داده‌ها لیبل دارند و لیبل بقیه داده‌ها مشخص نیست. دسته آخر که مورد بحث ماست، یادگیری بدون نظارت است که داده‌های موجود، لیبل ندارند و به ازای داده‌های مختلف، خروجی مناسب را نمی‌دانیم و از الگو‌های پنهان در داده‌ها اطلاعی نداریم. در این صورت است که به سمت الگوریتم‌های بدون نظارت می‌آییم تا به الگوریتم این اجازه را بدهیم که هرچه را می‌تواند یاد بگیرد و اطلاعات پنهان در داده‌ها را مشخص کند. الگوریتم‌های خوشه‌بندی در این دسته قرار می‌گیرند و دلیل قرارگیری در این دسته آن است که ما هیچ اطلاعاتی درمورد داده‌های ورودی نداریم و به دنبال ایجاد وابستگی میان آنها هستیم. الگوریتم‌های خوشه‌بندی این امکان را برای ما فراهم می‌سازد تا داده‌های شبیه به هم را در یک دسته قرار دهد. در این باره در صفحه ۱۴۱ \cite{ref1} گفته شده است:

« \textbf{تکنیک‌های خوشه‌بندی زمانی اعمال می‌شوند که کلاسی برای پیش‌بینی وجود نداشته باشد، بلکه زمانی که نمونه‌ها باید به گروه‌های طبیعی تقسیم شوند، اعمال می‌شوند.} »
	
	
پس اگر با داده‌هایی مواجه بودیم که اطلاعاتی در مورد آنها نمی‌دانیم،‌ خوشه‌یابی بهترین روش برای درک وابستگی‌ها میان داده‌هاست. الگوریتم‌های خوشه‌یابی را می‌توان به‌صورت زیر دسته‌بندی کرد:

	\begin{latin}
		\begin{enumerate}
			\item Density-based
			\item Distribution-based
			\item Centroid-based
			\item Hierarchical-based
		\end{enumerate}
	\end{latin}
	
	در الگوریتم‌های خوشه‌یابی مبتنی بر چگالی، داده‌ها بر اساس تراکم و غلظت داده‌ها در نقاط مختلف تقسیم‌بندی می‌شود. 
	\end{qsolve}
	
	
	
	\begin{qsolve}
در خوشه‌یابی توزیع شده، اساس خوشه یابی به‌صورت احتمالی است. یعنی برای تمام نقاط یک احتمال تعلق به یک خوشه خاص درنظر گرفته می‌شود که با دور شدن داده از مرکز آن خوشه، احتمال تعلق داده به خوشه مربوطه کاهش پیدا می‌کند.

پرکاربرد ترین و سریع‌ترین نوع خوشه‌یابی، خوشه‌یابی \lr{Centroid} است. این الگوریتم نقطه‌ها را بر اساس چندین مرکز در داده‌ها جدا می‌کند و هر نقطه بر اساس مجذور فاصله‌اش تا مرکز داده به یک خوشه اختصاص می‌یابد.

استفاده از خوشه‌بندی سلسله‌مراتبی محدود تر از سایر روش هاست. بدین صورت است که برای داده‌هایی که ذاتا به صورت سلسله‌مراتبی هستند استفاده می‌شود. مانند داده‌های مربوط به یک پایگاه داده.

الگوریتم های مختلفی برای خوشه‌یابی وجود دارد که می‌توان چندتا از آنها را به‌صورت زیر نام برد:

	\begin{latin}
		\begin{enumerate}
			\item SOM
			\item K-means
			\item DBSCAN
			\item Gaussian Mixture
			\item BIRCH
			\item Affinity Propagation
			\item Mean-Shift
			\item OPTICS
		\end{enumerate}
	\end{latin}
	
	در این سوال به بررسی دو مورد از مهم‌ترین الگوریتم ها یعنی \lr{SOM} و \lr{K-means} می‌پردازیم.
	
	\begin{enumerate}
		\item \lr{K-Means: }
	\end{enumerate}
	
	
	\end{qsolve}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\begin{latin}
		\begin{thebibliography}{9}
			\bibitem{ref1}
			Data Mining: Practical Machine Learning Tools and Techniques, 2016.
			
			\bibitem{ref2}
			Zeng GL. A deep-network piecewise linear approximation formula. IEEE Access. 2021 Aug 31;9:120665-74.
		\end{thebibliography} 
	\end{latin}
	
	
	
	
	\item مجموعه داده ارائه شده در این پروژه شامل رویداد‌های سه‌سال متوالی از ۲۰۲۲ تا ۲۰۲۴ است که از سایت ویکی‌پدیا جمع‌آوری شده است. داده‌ی مربوطه را بارگزاری کنید و پیش‌پردازش‌های متنی شامل حذف کلمات ایست (\lr{Stop word})، واحدسازی کلمات (‫‪Tokenization‬‬) و تبدیل به بردار‌های \lr{GloVe} را روی آ
	ن انجام دهید.
	
	
	
	
	
	\item پارامتر‌های ورودی مدل \texttt{minisom} را توضیح دهید. پارامتر‌ّای شبکه خودسازمان‌ده خود را تنظیم کنید و شبکه را بر روی داده‌های مربوطه آموزش دهید. (مقادیر تمامی پارامتر‌ها را در گزارش خود اضافه کنید.) سپس به‌ازای هر داده‌ی ورودی واحد، منطبق (\lr{Best matching unit}) با آن را به‌دست آورید و به‌عنوان نمایه‌ی داده‌ی مربوطه ذخیره کنید.
	
	
	
	
	
	\item برای ۵۰ رویداد که به‌صورت تصادفی از مجموعه داده انتخاب شده‌اند، نقشه خروجی را رسم کنید. نقشه‌ی به‌دست آمده را تفسیر کنید.
	
	
	
	\item فرآیند جست‌و‌جو را به‌صورت زیر برای سه رویداد دلخواه از سه‌سال گذشته انجام دهید. (می‌توانید از پرسش‌های موجود درفایل \texttt{sample\_questions.txt} کمک بگیرید.) و خروجی مربوطه را در گزارش خود اضافه کنید.
	
	\begin{itemize}
		\item تبدیل پرسش به بردار
		\item پیداکردن نمایه‌ی متناسب با پرسش مربوطه
		\item پیدا کردن تمامی داده‌های خارجی نمایه‌ی مورد نظر
		\item محاسبه معیار شباهت کسینوسی و خروجی دادن بردار‌های داده‌های خارجی با شباهت بیشتر از آستانه. (چرا معیار کسینوسی در این مسئله انتخاب مناسبی است؟)
	\end{itemize}
\end{enumerate}


%
%فرض کنید یک مجموعه داده دو کلاسه در اختیار دارید که کاملا به صورت خطی کلاس‌ها از هم جداپذیر هستند. یک شبکه چند لایه پرسپترونی (با طراحی دلخواه) طراحی نموده‌اید که لایه خروجی آن شامل دو نرون می‌باشد که تابع فعال‌ساز \texttt{softmax} بر آن اعمال می‌شود. در زمان آموزش، از تابع خطای \texttt{entropy cross binary} برای محاسبه خطا و بهینه‌سازی وزن ها استفاده می‌شود. آیا این امکان وجود دارد که خطای حاصل صفر شود؟ اگر امکان ندارد، با استدلال و اثبات ریاضی نشان دهید و اگر امکان دارد، با معرفی چهار داده (دو داده به ازای هر کلاس) و پرسپترون مد نظرتان نشان دهید که خطا می‌تواند دقیقا صفر شود.
%
%
%\begin{qsolve}
%	فایل کد از مسیر \texttt{code/Q1.ipynb} قابل مشاهده است.
%	
%	
%	در این سوال برای تولید دیتا از تابع \texttt{make\_blobs} از کتابخانه \texttt{sklearn} به صورت زیر استفاده شده است:‌
%	
%	\begin{latin}
%		\texttt{x, y = datasets.make\_blobs(n\_samples=200, centers=[(-1, -1), (1, 1)], cluster\_std=0.5)}
%	\end{latin}
%	
%	که داده هایی با تعداد ۲۰۰ نمونه و انحراف معیار ۰٫۵ حول نقطه های (1-,1-) و (۱و۱) می‌کند که کاملا جداپذیر خطی است «شکل \ref{داده‌های تولید شده برای مسئله}»
%	
%	\begin{center}
%		\includegraphics*[width=0.9\linewidth]{pics/img1.png}
%		\captionof{figure}{داده‌های تولید شده برای مسئله}
%		\label{داده‌های تولید شده برای مسئله}
%	\end{center}
%	
%	
%	برای حل این مسئله از شبکه‌ی پرسپترونی چند لایه ای با ساختار «شکل \ref{ساختار شبکه طراحی شده در سوال ۱}» استفاده شده است. با توجه به اینکه داده‌ها جداپذیر خطی هستند، می‌توان این مسئله را با یک نرون پرسپترونی نیز حل نمود اما با توجه به اینکه در صورت سوال گفته شده است شبکه ای چند لایه طراحی کنید، از شبکه چند لایه پرسپترونی استفاده کردیم.
%\end{qsolve}
%
%
%
%
%\begin{qsolve}
%	\begin{center}
%		\includegraphics*[width=0.5\linewidth]{pics/img2.pdf}
%		\captionof{figure}{ساختار شبکه طراحی شده}
%		\label{ساختار شبکه طراحی شده در سوال ۱}
%	\end{center}
%	
%	تابع فعال‌ساز لایه مخفی، \texttt{ReLU} درنظر گرفته شده است و در لایه خروجی نیز از \texttt{softmax} استفاده شده است. طبق خواسته مسئله، از \textbf{entropy cross binary} به‌عنوان تابع خطا استفاده شده است. همچنین از تابع بهینه‌ساز \texttt{ADAM} در این مسئله استفاده شده است.
%	
%	نتایج آموزش در ۲۰۰ دوره آموزشی به صورت زیر گزارش می‌شود:
%	
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img3.png}
%		\captionof{figure}{منحنی خطا و دقت برای داده‌های آموزش و تست}
%		\label{منحنی خطا و دقت برای داده‌های آموزش و تست}
%	\end{center}
%\end{qsolve}
%
%
%
%
%\begin{qsolve}
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img4.png}
%		\captionof{figure}{ماتریس پراکندگی}
%		\label{ماتریس پراکندگی سوال ۱}
%	\end{center}
%	
%	
%	\begin{center}
%		\includegraphics*[width=1\linewidth]{pics/img5.png}
%		\captionof{figure}{ناحیه تصمیم}
%		\label{ناحیه تصمیم سوال ۱}
%	\end{center}
%
%\end{qsolve}
%




%
%\begin{center}
%	\includegraphics*[width=1\linewidth]{pics/img1.png}
%	\captionof{figure}{مسئله مورد بحث}
%	\label{مسئله مورد بحث در سوال۱}
%\end{center}
%
%
%
%\begin{enumerate}
%	\item شکل \lr{a-1} را برای دسته بندی مسئله دودویی درنظر بگیرید. معماری نورون مورد نظر را توضیح داده و وزن‌های آن را بدست آورید.
%	
%	\begin{qsolve}
%		به دلیل آنکه داده های این قسمت جداپذر خطی هستند، می‌توان برای طبقه بندی آنها، از شبکه پرسپترون تک لایه استفاده کرد.
%		
%		معماری این شبکه به صورت «شکل \ref{معماری شبکه پرسپترون تک لایه}» است.
%		
%		
%		\begin{center}
%			\includegraphics*[width=0.8\linewidth]{pics/img2.pdf}
%			\captionof{figure}{معماری شبکه پرسپترون تک لایه}
%			\label{معماری شبکه پرسپترون تک لایه}
%		\end{center}
%	\end{qsolve}
%	
%	\begin{qsolve}
%				در این شبکه، ورودی/خروجی ها با مربع های نارنجی، نورون ها با دایره سبز و تابع فعال ساز با مربع آبی نشان داده شده است. تعداد دیتا ورودی شبکه ۲ است. $x_1$ و $x_2$. بایاس این شبکه با $x_0$ نشان داده شده است. وزن های شبکه نیز با $w$ نشان داده شده است. بنابر این بردار ورودی و وزن‌های شبکه به صورت زیر است:
%		
%		$$
%			X=\begin{bmatrix}          
%				x_0=1\\
%				x_1\\
%				x_2
%				
%				\end{bmatrix} \\\       W=\begin{bmatrix}          
%												w_0\\
%												w_1\\
%												w_2
%											
%											\end{bmatrix}
%		$$
%		
%		طبق تئوری شبکه‌های عصبی می‌دانیم خروجی نرون به صورت زیر محاسبه می‌شود: (در اینجا برای انجام محاسبات ساده، تابع فعال‌ساز درنظر گرفته نشده است)
%		
%		$$
%			\hat{y}=W^T X = \sum_{i=0}^{2} {w_ix_i}=w_0x_0+w_1x_1+w_2x_2 \xrightarrow{x_0=1} w_0+w_1x_1+w_2x_2
%		$$
%		
%		طبق شکل \lr{a-1} دو نقطه از خط جدا کننده دو کلاس را داریم. بنابراین می‌توان معادله خط را به صورت زیر نوشت.
%		
%		می‌دانیم معادله خط به صورت زیر تعریف می‌شود:
%		$$
%			y-y_0=m(x-x_0)
%		$$
%		
%		که در آن $m$ شیب خط است و به صورت زیر $\frac{\Delta y}{\Delta x}$ تعریف می‌شود. با جاگذاری یک از نقاط در معادله خط، می‌توان معادله خط را بدست آورد.
%		
%		$$
%			P_1=\begin{bmatrix}          
%				2.5\\
%				0
%				
%			\end{bmatrix} \\\       P_2=\begin{bmatrix}          
%				0\\
%				2.8
%				
%			\end{bmatrix}
%		$$
%		
%		$$
%			m=\frac{2.8-0}{0-2.5}=-1.12 \rightarrow y-0=-1.12(x-2.5) \rightarrow \boxed{y=-1.12x+2.8}
%		$$
%		
%		حالا اگر معادله خروجی نورون را به صورت زیر مرتب کنیم، می‌توان از مقایسه با مقادله خط بدست آمده وزن‌های شبکه را تعیین کرد.
%		
%		\begin{eqnarray*}
%			x_1=\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1},
%			&x_2=\frac{-w_1}{w_2}x_1-\frac{w_0}{w_2}&
%		\end{eqnarray*}
%
%		
%		در اینجا به دلیل آنکه دو معادله و ۳ مجهول ($w_0,w_1,w_2$) داریم، نیاز است که یکی از وزن ها را فرض کرده و دو وزن دیگر را بدست آورد.
%		
%		\begin{eqnarray*}
%			\frac{-w_2}{w_1}=-1.12 \rightarrow w_2=1.12w_1\\
%			\frac{-w_0}{w_1}=2.8 \rightarrow w_0=-2.8w_1\\
%			\rightarrow
%			\begin{cases}
%				w_2-1.12w_1=0\\
%				-2.8w_1-w_0=0 
%			\end{cases}
%			\text{assume} &w_0&=2.8 \rightarrow \text{\hl{$w_1=-1$}}, \text{\hl{$w_2=-1.12$}}
%		\end{eqnarray*}
%		ذکر این نکته الزامیست که این جواب، یکتا نمی‌باشد و برحسب اینکه مقدار $w_0$ را چه انتخاب کنیم، مقدار ۲ وزن دیگر متفاوت می‌شود.
%	\end{qsolve}
%	
%	
%
%	
%	\item حال شکل \lr{b-1} را درنظر بگیرید. چرا مسئله جداپذیر خطی نیست؟ چگونه می‌توان آن را در قالب حل چند مسئله خطی حل نمود؟ معماری پیشنهادی خودتان را رسم و وزن‌های موجود در آن را با انجام محاسبات بدست آورید. معماری شما می‌تواند حاصل از کنار هم چیدن و پشت هم چیدن یک یا چند نورون پرسپترونی باشد.
%	
%	
%	
%	\begin{qsolve}
%		به مسائلی جداپذیر خطی گفته می‌شود که بتوان داده‌ها (کلاس‌ها) را با استفاده از فقط یک خط از هم جدا کرد. در این مثال، دو کلاس آبی و سبز را نمی‌توان فقط با یک خط از هم جدا کرد. بنابراین این مسئله \textbf{جداپذیر خطی نمی‌باشد.} برای حل این مسئله، چندین روش وجود دارد که در ادامه آنها را توضیح خواهم داد.
%		
%		\begin{enumerate}
%			\item \textbf{افزایش ابعاد ویژگی های مسئله: }\\
%			یعنی در این مثال که مسئله مورد بحث ما دو بعدی است، یک بعد به آن اضاف کنیم و آن را به عنوان یک مسئله ۳ بعدی حل کنیم و تلاش کنیم که یک صفحه برای جدا کردن دو کلاس پیدا کنیم.
%			
%			\item \textbf{دادن ورودی هایی با توان بالا به عنوان ورودی شبکه: }\\
%			در صورتی می‌توان از این روش استفاده کرد که مقدار دقیق تمام نمونه ها را داشته باشیم. در این صورت می‌توان ورودی ها را به توان های بالا (۲ و ۳ و...) رساند و امیدوار باشیم فضای قرارگیری ویژگی های جدید در صفحه به صورت جداپذیر خطی باشد. «این کار در سوال دوم همین سری تمرین انجام شده است و با به توان ۲ رساندن ویژگی های ورودی شبکه، مسئله ای که جداپذیر خطی نبود به جداپذیر خطی تبدیل می‌شود.» در این مسئله به دلیل نداشتن موقعیت دقیق نمونه های هرکلاس نمی‌توان از این روش استفاده کرد.
%			
%			\item \textbf{اضاف کردن لایه مخفی: }\\
%			در اضاف کردن تعداد لایه های مخفی شبکه، آزاد هستیم اما باید به این نکته توجه داشت که اگر بیشتر از یک لایه مخفی به شبکه اضاف کنیم، حل مسئله از نظر خطی بودن خارج شده و نواحی تصمیم گیری شامل خط راست نمی‌شود و نواحی پیچیده تری مانند منحنی ها را در بر می‌گیرد. اما اگر فقط یک لایه مخفی ۳ نورونی به شبکه اضاف کنیم، می‌توان مسئله ای که ذاتا جدا‌پذیر خطی نیست را به وسیله ۳ خط جدا کننده که تعداد این خط ها برابر است با تعداد نورون های لایه مخفی، حل نمود. ساختار این مدل در «شکل \ref{معماری شبکه ۳ لایه پیشنهادی}» آورده شده است.
%			
%			اما مشکلی که وجود دارد آن است که در صورت سوال از ما خواسته شده وزن های شبکه را بدست آوریم، اگر از این ساختار برای حل استفاده کنیم، به دلیل نداشتن مقدار ورودی ها نمی‌توان ۳ وزن لایه متصل به خروجی را بدست آورد.
%			
%			\item \textbf{شکستن مسئله به ۳ زیرمسئله خطی}: \\
%			برای حل این سوال از این روش استفاده شده است. در قسمت قبل دیدیم که یک نورون پرسپترونی می‌تواند یک خد جدا کننده در صفحه رسم کنید. در این مثال، برای جدا کردن این دو کلاس به ۳ خط نیاز داریم. پس داده‌های ورودی مسئله را به ۳ بخش جدا‌پذیر خطی تقسیم می‌کنیم. «شکل \ref{داده های شکسته شده به سه بخش}» اکنون می‌توان همانند قسمت قبل، وزن ها را برای هر سه زیر‌مسئله بدست آورد. در این قسمت ساختار و معماری شبکه همان ساختار قسمت \lr{a-1} است. «شکل \ref{معماری شبکه پرسپترون تک لایه}»
%			
%			
%		\end{enumerate}
%	\end{qsolve}
%	
%	
%	
%	\begin{qsolve}
%		\begin{center}
%			\includegraphics*[width=0.5\linewidth]{pics/img3.pdf}
%			\captionof{figure}{معماری شبکه ۳ لایه پیشنهادی}
%			\label{معماری شبکه ۳ لایه پیشنهادی}
%		\end{center}
%		
%		\begin{center}
%			\includegraphics*[width=1\linewidth]{pics/img4.pdf}
%			\captionof{figure}{داده های شکسته شده به سه بخش}
%			\label{داده های شکسته شده به سه بخش}
%		\end{center}
%		
%		
%		زیر مسئله شماره ۱ در قسمت قبل حل شده است و وزن های آن به صورت زیر بدست آورده شد:
%		\begin{eqnarray*}
%			w_0=2.8, &w_1=-1, &w_2=-1.12
%		\end{eqnarray*}
%		
%		برای دو زیر مسئله دیگر هم، همانند قسمت قبل وزن‌ها را بدست می‌اوریم.
%		
%		\begin{eqnarray*}
%			P_1=\begin{bmatrix}
%				2.5\\
%				0
%			\end{bmatrix}
%			P_2&=&\begin{bmatrix}
%				-1.5\\
%				1.7
%			\end{bmatrix}
%			\rightarrow m=\frac{\Delta y}{\Delta x}=\frac{y_2-y_1}{x_2-x_1}=\frac{-1.5-2.5}{-1.7-0}=\frac{-4}{-1.7}=\boxed{2.35}\\
%			y-0&=&2.35(x-2.5)\rightarrow \boxed{y=2.35x-5.88}\\
%			\rightarrow x_1&=&\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1} \rightarrow \begin{cases}
%				\frac{-w_2}{w_1}=2.35 \rightarrow w_2=-2.35w_1\\
%				\frac{-w_0}{w_1}=-5.88 \rightarrow w_0=5.88w_1 
%			\end{cases}\\
%			\text{Assume}\qquad w_0&=&5.88 \rightarrow \text{\hl{$w_1=1$}}, \text{\hl{$w_2=-2.35$}}
%		\end{eqnarray*}
%	\end{qsolve}
%	
%	
%	
%	
%	
%	
%	\begin{qsolve}
%		\begin{eqnarray*}
%			P_1=\begin{bmatrix}
%				-1.5\\
%				-1.7
%			\end{bmatrix}
%			P_2&=&\begin{bmatrix}
%				0\\
%				2.8
%			\end{bmatrix}
%			\rightarrow m=\frac{\Delta y}{\Delta x}=\frac{y_2-y_1}{x_2-x_1}=\frac{0+1.5}{2.8+1.7}=\frac{1.5}{4.5}=\boxed{0.33}\\
%			y-2.8&=&0.33(x-0)\rightarrow \boxed{y=0.33x+2.8}\\
%			\rightarrow x_1&=&\frac{-w_2}{w_1}x_2-\frac{w_0}{w_1} \rightarrow \begin{cases}
%				\frac{-w_2}{w_1}=0.33 \rightarrow w_2=-0.33w_1\\
%				\frac{-w_0}{w_1}=2.8 \rightarrow w_0=-2.8w_1 
%			\end{cases}\\
%			\text{Assume}\qquad w_0&=&2.8 \rightarrow \text{\hl{$w_1=-1$}}, \text{\hl{$w_2=0.33$}}
%		\end{eqnarray*}
%	\end{qsolve}
%	
%	
%	
%	
%	
%	
%	
%	
%	\item  شگرد هسته\footnote{\lr{Kernel trick}} چیست و چگونه می‌توان قسمت قبل را با آن حل نمود؟ توضیح دهید.
%	
%	\begin{qsolve}
%		این تکنیک به عنوان یکی از روش های حل مسئله قبل معرفی شد و توضیح مختصری در مورد آن داده شد. در این قسمت توضیخات کامل آن را بیان خواهیم کرد.
%		
%		روش شگرد هسته، یکی از چندین روش موجود برای حل مسائل جداناپذیر خطی با روش های خطی است. بدین صورت که اگر ابعاد ویژگی های مسئله را R در نظر بگیریم و داده ها در R بعد جداناپذیر خطی باشند، ممکن است با افزایش بعد ویژگی ها (R+n) و اضاف کردن ویژگی ای جدید، داده ها جداپذیر خطی شوند و بتوان مسئله را با یک شبکه خطی حل نمود.
%		
%		در این سوال، ویژگی های ورودی ۲ بعدی هستند و داده ها در ۲ بعد جداناپذیر خطی هستند. بنابر این می‌توان یک ویژگی جدید در بعد سوم به ورودی ها اضاف کرد و خروجی را نمایش داد تا شاید داده ها جدا‌پذیر خطی شوند.
%		
%		این کار را انجام دادیم و اثباط کردیم که با افزایش بعد این مسئله، داده ها جداپذیر خطی می‌شوند. شکل زیر را به عنوان داده های ورودی مسئله در نظر بگیریم:
%		
%		\begin{center}
%			\includegraphics*[width=0.6\linewidth]{pics/img5.png}
%			\captionof{figure}{داده های ورودی مسئله در ۲ بعد}
%			\label{داده های ورودی مسئله در ۲ بعد}
%		\end{center}
%		
%		برای افزودن بعد سوم به این داده های ۲ بعدی، تابع زیر را نوشته ایم.
%	\end{qsolve}
%	
%	
%	\begin{qsolve}
%		
%		\begin{latin}
%			\texttt{def kernel\_trick(x):}\\
%			\texttt{return np.append(x, np.expand\_dims(x[:, 0]**2 ** x[:, 1]**2, axis=1), axis=1)}
%		\end{latin}
%		
%		این تابع ویژگی جدید ${(x_0^2)}^{x_1^2}$ را به داده های ورودی به عنوان بعد سوم اضافه می‌کند. بنابر در فضای ویژگی جدید، هر نقطه از بردار ویژگی شامل ۳ عضو است. (\textbf{این نکته لازم به ذکر است که ویژگی جدید تولید شده، با سعی و خطا بدست آمده است و می‌توان به جای آن هر ویژگی جدید دیگری را قرار داد})
%		
%		
%		پس از رسم فضای ویژگی جدید، مشاهده می‌شود که داده ها جداپذیر خطی شده اند و می‌توان آنها را با یک صفحه از هم جدا نمود.
%		
%		\begin{center}
%			\includegraphics*[width=0.8\linewidth]{pics/img6.png}
%			\captionof{figure}{فضای ویژگی های جدید در ۳ بعد}
%			\label{فضای ویژگی های جدید در ۳ بعد}
%		\end{center}
%	\end{qsolve}
%\end{enumerate}
%
%
%
%	
%
%		
%		
%		
%		
%			
%		
%		
%		
%		
%		
%			
%		
%		
%		
%		
%
%		
%	
%	
%	
%
%	
%	
%	
%	
%
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%	
%
%
%
%
%
%
%
%
%
%
%
%
%%
%%
%%
%%
%%
%%
%%
%%\begin{qsolve}[]
%%	\begin{eqnarray*}
%%		p(t)&=&\sum_{n=-\infty}^{\infty}\delta(t-nT)\qquad \text{\lr{periodic in T}}\\
%%		a_k &=&\frac{1}{T}\int_{-T/2}^{T/2}f(t)e^{jk(2\pi/T)t}dt=
%%		\frac{1}{T}\int_{-T/2}^{T/2}\sum_{n=-\infty}^{\infty}\delta(t-nT)e^{jk(2\pi/T)t}dt
%%		\xrightarrow{n=0}\\
%%		&=&\frac{1}{T}\int_{-T/2}^{T/2}\delta(t)e^{jk(2\pi/T)t}dt=\frac{1}{T}
%%		\hspace{8em}\text{\hl{$a_k=\frac{1}{T}$}}
%%	\end{eqnarray*}
%%	تابع متناوب است و سری فوریه دارد، پس تبدیل فوریه آن همان سری فوریه آن به صورت
%%	جمع ضربه هاست.
%%	\begin{eqnarray*}
%%		P(j\omega)&=&\mathcal{F}\left\{\sum_{k=-\infty}^{\infty}a_ke^{jk(2\pi/T)t}\right\}=\sum_{k=-\infty}^{\infty}2\pi a_k\delta(\omega-k\frac{2\pi}{T})
%%		=\frac{2\pi}{T}\sum_{k=-\infty}^{\infty}\delta(\omega-k\frac{2\pi}{T})
%%	\end{eqnarray*}
%%\end{qsolve}
%%
%%با توجه به اینکه با استفاده از این سیگنال نمونه برداری قطار ضربه از یک سیگنال پیوسته زمان انجام میدهیم و با استفاده نتیجه بدست آمده در قسمت
%%قبل تبدیل فوریه سیگنال بدست آمده حاصل از ضرب این سیگنال در سیگنال دلخواه $x(t)$ را بدست آورید.
%%
%%\begin{qsolve}[]
%%	\begin{eqnarray*}
%%		x_p(t)&=&x(t)\times p(t)\Rightarrow X_p(j\omega)=\frac{1}{2\pi}X(j\omega)*P(j\omega)
%%		=\frac{1}{2\pi}\convolve[\omega]{P}{X}\\
%%		X_p(j\omega)&=&\frac{1}{2\pi}\sum_{k=-\infty}^{\infty}\intinf \frac{2\pi}{T}\delta(\tau-k\frac{2\pi}{T})X(j(\omega-\tau))d\tau
%%		=\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))
%%	\end{eqnarray*}
%%\end{qsolve}
%%
%%حال نتیجه بدست آمده از قسمت قبل را با تبدیل فوریه گسسته سیگنال نمونه برداری شده $x[n]=x(nT)$ مقایسه کنید. حال شرطی روی نرخ
%%نمونه برداری $F_s=\frac{1}{T_s}$ بدست آورید که تمام محتوای فرکانسی سیگنال اولیه پس از نمونه برداری حفظ شود. (شرط نایکوییست)
%%
%%\begin{qsolve}[]
%%	پاسخ بدست آمده متناوب است، پس میتوان آن را به صورت یک سری فوریه نوشت.
%%	یعنی داریم که : $X(j\omega)=\sum_{n=-\infty}^{\infty}C_ke^{jk\omega_0\omega}$
%%	\splitqsolve
%%	\begin{eqnarray*}
%%		X_p(j\omega)&=&\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))
%%		\xrightarrow{\text{\lr{periodic in $\frac{2\pi}{T}$}}}\sum_{n=-\infty}^{\infty}C_ne^{jnT\omega}\\
%%		C_n&=&\frac{1}{2\pi/T}\int_{-\pi/T}^{\pi/T}X_p(j\omega)e^{jnT\omega}d\omega
%%		=\frac{T}{2\pi}\int_{-\pi/T}^{\pi/T}\frac{1}{T}\sum_{k=-\infty}^{\infty}X(j(\omega-k\frac{2\pi}{T}))e^{jnT\omega}d\omega\\
%%		&=&\xrightarrow{k=0}\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}X(j\omega)e^{jnT\omega}d\omega
%%	\end{eqnarray*}
%%	حال اگر داشتیم که $X(j\omega)\{|\omega|\leq\frac{\pi}{T}\}=0$
%%	\begin{eqnarray*}
%%		C_n&=&\frac{1}{2\pi}\int_{-\pi/T}^{\pi/T}X(j\omega)e^{jnT\omega}d\omega=
%%		\frac{1}{2\pi}\int_{-\infty}^{\infty}X(j\omega)e^{jnT\omega}d\omega=x(nT)\\
%%		X_p(j\omega)&=&\sum_{n=-\infty}^{\infty}C_ne^{jnT\omega}=\sum_{n=-\infty}^{\infty}x(nT)e^{jnT\omega}
%%		=\sum_{n=-\infty}^{\infty}x[n]e^{jnT\omega}=X(e^{j\Omega})\when_{\Omega=\omega T}
%%	\end{eqnarray*}
%%	که این به ما شرط نایکوییست را میدهد. به صورتی که:
%%	\[
%%		\text{ : اگر داشته باشیم}
%%        \forall \omega \geq \omega_s = \frac{\pi}{T}: X(j\omega)=0\Longrightarrow X_p(j\omega)=X(e^{j\Omega})\when_{\Omega=\omega T}
%%	\]
%%\end{qsolve}
%%
%%فرض کنید به علت محدودیت هایی که داریم شرط نایکوییست برقرار نباشد در این حالت روشی پیشنهاد دهید که محتوای فرکانسی کمتری از سیگنال
%%در اثر نمونه برداری از بین برود.
%%
%%\begin{qsolve}[]
%%    ناچار ایم که مقداری اطلاعات از دست بدهیم، زیرا در حالت خالص نمونه برداری، شرط نایکوییست برقرار نیست 
%%    و دچار اعوجاج فرکانسی هستیم.
%%
%%    میتوانیم قبل از نمونه برداری اطلاعات فرکانسی را کم کنیم، اینگونه اطلاعات از بین رفته و 
%%    فرکانسی اطلاعات کاهش میابد، ولی حداقل اعوجاج فرکانسی نداریم و نویز با فرکانس پایین ایجاد نمیکنیم.
%%
%%    به این کار Anti-Aliasing میگوییم. به این صورت که قبل از نمونه برداری، یک \lr{low-pass filter} استفاده میکنیم.
%%
%%    \begin{center}
%%        \includegraphics*[width=0.8\linewidth]{pics/anti-aliasing.png}
%%        \captionof{figure}{\lr{anti aliasing diagram}}
%%    \end{center}
%%\end{qsolve}