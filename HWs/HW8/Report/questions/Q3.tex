\section{سوال سوم - تئوری}

ترنسفورمرها نسبت به شبکه‌های \lr{sqe2seq} قابلیت موازی‌سازی بیشتری دارند. با ذکر جزئیات توضیح دهید.


\begin{qsolve}
مدل‌های \lr{Seq2Seq} معمولاً از \lr{RNN} یا \lr{LSTM} استفاده می‌کنند که پردازش ترتیبی دارند. اما در مقابل ترنسفرمرها به صورت موازی داده‌ها را پردازش می‌کنند.

\begin{enumerate}
	\item \textbf{در مدل‌های \lr{seq2seq}}\\
	اگر فرض شود دنباله ای با طول $n$ داریم، هر گام زمانی در \lr{RNN} یا \lr{LSTM} باید به ترتیب پردازش شود و برای هر گام زمانی، یک پردازش زمانی $O(1)$ انجام می‌شود. بنابراین، کل پیچیدگی زمانی برای پردازش توالی ورودی $O(n)$ است. 
	
هر گام زمانی باید منتظر تکمیل گام قبلی باشد، بنابراین پردازش‌ها نمی‌توانند به‌طور موازی انجام شوند. در بهترین حالت، هر گام زمانی می‌تواند به‌طور موازی با پردازش داخلی خود (مثل محاسبات داخل سلول‌های \lr{LSTM}) انجام شود، اما این قابلیت موازی‌سازی به‌طور کلی محدود است.

در مقابل در \item \textbf{ترنسفرمرها}\\
اگر مجددا فرض شود توالی ای ورودی با طول $n$ داریم، در لایه \lr{Self Attention}، هر نشانه می‌تواند به تمام نشانه‌های دیگر در توالی توجه کند. این کار با محاسبه ماتریس توجه انجام می‌شود که پیچیدگی زمانی $O(n^2)$ دارد.
محاسبات ماتریسی برای محاسبه توجه به‌طور موازی قابل انجام است. بنابراین، پیچیدگی زمانی یک لایه خود توجهی $O(n^2)$است.

محاسبات ماتریسی در لایه \lr{Self Attention} (مثل محاسبه ماتریس $KQ^T$ برای توجه) به‌طور کامل به‌صورت موازی انجام می‌شوند.
\end{enumerate}
	
\end{qsolve}