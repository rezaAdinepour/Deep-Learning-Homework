حملات خصمانه\footnote{\lr{‫‪Adversarial‬‬ Attack}} نوعی از حملات بر روی مدل‌های یادگیری ماشین به منظور فریب دادن مدل با استفاده از ورودی‌های دستکاری شده است. هدف اصلی این حملات تغییر خروجی مدل به صورت اشتباه است. به سوالات زیر پاسخ دهید و به منبع یا منابعی که استفاده کردید ارجاع دهید.


\begin{center}
	\includegraphics*[width=0.8\linewidth]{pics/img1.png}
	\captionof{figure}{تغییر نمونه ورودی}
	\label{تغییر نمونه ورودی}
\end{center}


\section{سوال اول - تئوری}
یکی از اولین و ساده ترین روش‌های حمله خصمانه، \lr{FGSM} است که توسط یان گودفلو و همکارانش\footnote{\lr{‫‪Examples‬‬‫‪Adversarial‬‬ ‫‪Harnessing‬‬ ‫‪and‬‬ ‫‪Explaining}‬‬} معرفی شد. هدف این روش، ایجاد یک نمونه خصمانه است که تفاوت بسیار کمی با ورودی اصلی داشته باشد اما مدل را به اشتباه بیندازد. \lr{PGD} یک روش قوی‌تر و بهبود یافته نسبت به \lr{FGSM} است که توسط \lr{Madry} و همکارانش\footnote{‫‪\lr{Attacks‬‬‫‪Adversarial‬‬ ‫‪to‬‬ ‫‪Resistant‬‬ ‫‪Models‬‬ ‫‪Learning‬‬ ‫‪Deep‬‬ ‫‪Towards}‬‬} معرفی شده. این روش به جای انجام یک مرحله، بروز رسانی‌های متعددی را انجام می‌دهد و در هر مرحله تغییرات را در محدوده مشخصی پروجکت می‌کند تا اطمینان حاصل شود که نمونه خصمانه بیش از حد از ورودی اصلی فاصله نگیرد. این دو روش را مطالعه و خلاصه‌ای از آن‌ها بنویسید.


\begin{qsolve}
	
\end{qsolve}



%\begin{latin}
%	\begin{thebibliography}{9}
%		\bibitem{ref1}
%		"Rethinking Attention with Performers" by Choromanski et al. (2021), which introduces the Performer model using kernel-based approximations to reduce the complexity of self-attention.
%	\end{thebibliography} 
%\end{latin}








































%
%یکی از دلایل نیاز به مکانیزم توجه، گلوگاهی بود که بین رمزگذار و رمزگشا در مدل های \lr{seq2seq} به وجود می‌آمد. این مشکل را توضیح دهید و نشان دهید چطور مکانیزم توجه این مشکل را حل کرد. یکی دیگر از مشکلات، عدم توجه مدل به گذشته دور بود. به طور مثال در یک متن به کلمات نزدیک‌تر اهمیت بیشتری داده می‌شد تا کلمات دورتر و وزن کلمات دورتر به صورت نمایی کاهش پیدا می‌کرد. آیا استفاده از \lr{lstm} و یا \lr{lstm} دوطرفه می‌تواند این مشکل را به طور کامل رفع کند؟ توضیح دهید.
%	
%
%
%
%
%\begin{qsolve}
%در مدل‌های \lr{Seq2Seq}، انکودر دنباله ورودی را پردازش کرده و آن را به یک بردار متنی با طول ثابت تبدیل می‌کند. این بردار متنی باید تمام اطلاعات مربوط به دنباله ورودی را در خود ذخیره کند.
%سپس دیکودر این بردار با طول ثابت را می‌گیرد و دنباله خروجی را تولید می‌کند. برای دنباله‌های ورودی طولانی، فشرده‌سازی تمام اطلاعات به یک بردار با طول ثابت دشوار است. این منجر به از دست رفتن اطلاعات می‌شود، بردار متنی با طول ثابت ممکن است نتواند تمام جزئیات لازم برای تولید دنباله خروجی منسجم و دقیق را ذخیره کند همین موضوع به عنوان یکی از چالش ها م مشکلات مدل‌های \lr{RNN} مطرح می‌شود.
%
%مکانیزم توجه برای کاهش مشکل گلوگاه در شبکه‌های \lr{RNN} معرفی شد. مکانیزم توجه بر خلاف روش‌های قبلی، به جای اتکا به یک بردار متنی با طول ثابت، به دیکودر این اجازه را می‌دهد که برای هر خروجی یک بردار متنی پویا ایجاد کند که این بردار متنی پویا یک جمع وزنی از تمام وضعیت‌های پنهان (از گذشته‌های دور تا الان)انکودر است.
%
%مسئله دیگری در مدل‌های \lr{Seq2Seq}، به‌ویژه با \lr{RNN}ها، دشواری در پردازش وابستگی‌های بلندمدت بود. \lr{RNN}های سنتی و حتی \lr{LSTM}ها تمایل دارند که به ورودی‌های جدید، بیشتر از ورودی‌های دورتر اهمیت دهند. 
%
%\lr{LSTM}
%ها برای کاهش مشکل محو شدن گرادیان طراحی شده‌اند که به ضبط وابستگی‌های طولانی‌تر نسبت به \lr{RNN}های معمولی کمک می‌کند. با این حال، تأثیر ورودی های دورتر همچنان تمایل دارد که با گذشت زمان کاهش یابد، هرچند نه به اندازه‌ای که در \lr{RNN}های استاندارد دیده می‌شود.
%
%در \lr{BiLSTM}ها دنباله را در هر دو جهت جلو و عقب پردازش می‌کنند و بنابراین اطلاعات را از هر دو زمینه گذشته و آینده فراهم می‌کنند.
%این رویکرد دوطرفه توانایی مدل را در ضبط وابستگی‌ها در هر دو جهت بهبود می‌بخشد.
%با این وجود، \lr{BiLSTM}ها همچنان به بردارهای با طول ثابت متکی هستند و با وابستگی‌های بسیار طولانی مشکل دارند.
%
%در عمل این موضوع به عنوان یکی از ضعف‌های این نوع شبکه‌ها محسوب می‌شود و شبکه \lr{Transformer} و به‌ویژه مکانیزم توجه این مشکل را حل نموده و وابستگی‌های طولانی مدت را در دنباله سیگنال ورودی، بیشتر از سایر شبکه‌ها درک می‌کند.
%
%مکانیزم توجه به دیکودر اجازه می‌دهد تا به هر قسمت از دنباله ورودی به‌طور مستقیم دسترسی داشته باشد، بدون توجه به موقعیت آن. این دسترسی مستقیم به این معنی است که ورودی دور نیز می‌توانند بر ورودی فعلی تاثیرگذار باشد.
%	
%\end{qsolve}