حملات خصمانه\footnote{\lr{‫‪Adversarial‬‬ Attack}} نوعی از حملات بر روی مدل‌های یادگیری ماشین به منظور فریب دادن مدل با استفاده از ورودی‌های دستکاری شده است. هدف اصلی این حملات تغییر خروجی مدل به صورت اشتباه است. به سوالات زیر پاسخ دهید و به منبع یا منابعی که استفاده کردید ارجاع دهید.


\begin{center}
	\includegraphics*[width=0.8\linewidth]{pics/img1.png}
	\captionof{figure}{تغییر نمونه ورودی}
	\label{تغییر نمونه ورودی}
\end{center}


\section{سوال اول - تئوری}
یکی از اولین و ساده ترین روش‌های حمله خصمانه، \lr{FGSM} است که توسط یان گودفلو و همکارانش\footnote{\lr{‫‪Examples‬‬‫‪Adversarial‬‬ ‫‪Harnessing‬‬ ‫‪and‬‬ ‫‪Explaining}‬‬} معرفی شد. هدف این روش، ایجاد یک نمونه خصمانه است که تفاوت بسیار کمی با ورودی اصلی داشته باشد اما مدل را به اشتباه بیندازد. \lr{PGD} یک روش قوی‌تر و بهبود یافته نسبت به \lr{FGSM} است که توسط \lr{Madry} و همکارانش\footnote{‫‪\lr{Attacks‬‬‫‪Adversarial‬‬ ‫‪to‬‬ ‫‪Resistant‬‬ ‫‪Models‬‬ ‫‪Learning‬‬ ‫‪Deep‬‬ ‫‪Towards}‬‬} معرفی شده. این روش به جای انجام یک مرحله، بروز رسانی‌های متعددی را انجام می‌دهد و در هر مرحله تغییرات را در محدوده مشخصی پروجکت می‌کند تا اطمینان حاصل شود که نمونه خصمانه بیش از حد از ورودی اصلی فاصله نگیرد. این دو روش را مطالعه و خلاصه‌ای از آن‌ها بنویسید.


\begin{qsolve}
	\begin{itemize}
		\item \textbf{روش \lr{FGSM}}:
		
		این روش در سال ۲۰۱۵ در مقاله \lr{\textit{Explaining and Harnessing Adversarial Examples}} معرفی شد که یکی از ساده ترین روش های حملات خصمانه است. همانطور که در صورت سوال نیز توضیح داده شد، هدف اصلی \lr{FGSM} ایجاد نمونه‌های خصمانه‌ای است که از نظر بصری تفاوت زیادی با ورودی اصلی نداشته باشند ولی باعث شوند مدل یادگیری ماشین خطا کند.
	\end{itemize}
\end{qsolve}


\begin{qsolve}
	در ادامه، روش انجام این الگوریتم را به‌صورت خلاصه توضیح می‌دهیم:
	
	\begin{itemize}
		\item FGSM با اضافه کردن یک اختلال به داده‌های ورودی اصلی کار می‌کند.
		\item این اختلال با استفاده از گرادیان تابع هزینه نسبت به داده‌های ورودی محاسبه می‌شود.
		\item به طور خاص، می‌توان فرمول این اختلال را به‌صورت زیر بیان نمود:
		\[
		\eta = \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y))
		\]
		که در آن:
		\begin{itemize}
			\item \(\epsilon\) یک مقدار عددی کوچک است که میزان اختلال را کنترل می‌کند.
			\item \(\nabla_x J(\theta, x, y)\) گرادیان تابع هزینه \(J\) نسبت به ورودی \(x\) است.
			\item \(\text{sign}\) تابع علامت است که علامت گرادیان را استخراج می‌کند.
		\end{itemize}
		\item سپس نمونه‌ی خصمانه با اضافه کردن این اختلال به ورودی اصلی ایجاد می‌شود:
		\[
		x' = x + \eta
		\]
	\end{itemize}
	
	از مزایا و معایب \lr{FGSM} می‌توان به موارد زیر اشاره کرد:
	
	\textbf{مزایا}: از نظر محاسباتی کارآمد و ساده است و به همین دلیل انتخاب محبوبی برای مطالعات اولیه در مورد حملات خصمانه است.
	
	\textbf{معایب}: سادگی FGSM می‌تواند به عنوان یک نقطه ضعف نیز عمل کند، زیرا اختلالات ایجاد شده ممکن است در مقابل مدل‌های مقاوم‌تر به اندازه‌ی کافی مؤثر نباشند.
	
	
	\begin{itemize}
		\item \textbf{روش \lr{PGD}}:
		
همانطور که در صورت سوال نیز گفته شد، روش \lr{PGD} یک روش بهبود یافته نسبت به \lr{FGSM} است که درسال ۲۰۱۷ در مقاله‌ی \lr{\textit{Towards Deep Learning Models Resistant to Adversarial Attacks}} معرفی شد. هدف اصلی این روش، ایجاد نمونه‌های خصمانه قوی‌تر با انجام چندین به‌روزرسانی گرادیانی و اطمینان از ماندن اختلالات در یک محدوده‌ی مشخص است.

در ادامه، روش انجام این الگوریتم را توضیح می‌دهیم:

	\begin{itemize}
		\item PGD به طور مکرر نمونه‌ی خصمانه را با گرفتن چندین گام گرادیانی اصلاح می‌کند.
		\item هر گام شامل مراحل زیر است:
		\begin{enumerate}
			\item \textbf{محاسبه‌ی گرادیان}: گرادیان تابع هزینه نسبت به نمونه‌ی خصمانه‌ی فعلی محاسبه می‌شود.
			\item \textbf{گام به‌روزرسانی}: نمونه‌ی خصمانه با حرکت در جهت گرادیان به‌روزرسانی می‌شود:
			\[
			x_{t+1} = x_t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x_t, y))
			\]
			که \(\alpha\) اندازه‌ی گام است.
			\item \textbf{پروژه‌سازی}: اطمینان حاصل می‌شود که نمونه‌ی خصمانه‌ی به‌روز شده در یک کره‌ی \(\epsilon\) حول ورودی اصلی باقی می‌ماند:
			\[
			x_{t+1} = \text{clip}(x_{t+1}, x - \epsilon, x + \epsilon)
			\]
			این مرحله اطمینان می‌دهد که اختلال از محدوده‌ی مجاز تجاوز نمی‌کند.
		\end{enumerate}
	\end{itemize}
	\end{itemize}
\end{qsolve}



\begin{qsolve}
	این فرآیند برای تعداد معینی از تکرارها یا تا همگرایی تکرار می‌شود.
	
	
	از مزایا و معایب \lr{PGD} می‌توان به موارد زیر اشاره کرد:
	
	\begin{enumerate}
		\item \textbf{مزایا}: PGD به دلیل اینکه فضای اختلالات ممکن را با تکرارهای متعدد به طور جامع‌تری کاوش می‌کند، به عنوان یک روش حمله‌ی قوی‌تر نسبت به FGSM شناخته می‌شود.
		
		
		\item \textbf{معایب}: طبیعت تکراری PGD باعث می‌شود که از نظر محاسباتی پرهزینه‌تر از FGSM باشد.
	\end{enumerate}
\end{qsolve}



\begin{latin}
	\begin{thebibliography}{9}
		\bibitem{ref1}
		Goodfellow, I. J., Shlens, J., \& Szegedy, C. (2015). Explaining and harnessing adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR) \href{https://arxiv.org/abs/1412.6572}{\textcolor{magenta}{[Link]}}
		
		
		
		\bibitem{ref2}
		Madry, A., Makelov, A., Schmidt, L., Tsipras, D., \& Vladu, A. (2018). Towards deep learning models resistant to adversarial attacks. In Proceedings of the International Conference on Learning Representations (ICLR) \href{https://arxiv.org/abs/1706.06083}{\textcolor{magenta}{[Link]}}
	\end{thebibliography} 
\end{latin}








































%
%یکی از دلایل نیاز به مکانیزم توجه، گلوگاهی بود که بین رمزگذار و رمزگشا در مدل های \lr{seq2seq} به وجود می‌آمد. این مشکل را توضیح دهید و نشان دهید چطور مکانیزم توجه این مشکل را حل کرد. یکی دیگر از مشکلات، عدم توجه مدل به گذشته دور بود. به طور مثال در یک متن به کلمات نزدیک‌تر اهمیت بیشتری داده می‌شد تا کلمات دورتر و وزن کلمات دورتر به صورت نمایی کاهش پیدا می‌کرد. آیا استفاده از \lr{lstm} و یا \lr{lstm} دوطرفه می‌تواند این مشکل را به طور کامل رفع کند؟ توضیح دهید.
%	
%
%
%
%
%\begin{qsolve}
%در مدل‌های \lr{Seq2Seq}، انکودر دنباله ورودی را پردازش کرده و آن را به یک بردار متنی با طول ثابت تبدیل می‌کند. این بردار متنی باید تمام اطلاعات مربوط به دنباله ورودی را در خود ذخیره کند.
%سپس دیکودر این بردار با طول ثابت را می‌گیرد و دنباله خروجی را تولید می‌کند. برای دنباله‌های ورودی طولانی، فشرده‌سازی تمام اطلاعات به یک بردار با طول ثابت دشوار است. این منجر به از دست رفتن اطلاعات می‌شود، بردار متنی با طول ثابت ممکن است نتواند تمام جزئیات لازم برای تولید دنباله خروجی منسجم و دقیق را ذخیره کند همین موضوع به عنوان یکی از چالش ها م مشکلات مدل‌های \lr{RNN} مطرح می‌شود.
%
%مکانیزم توجه برای کاهش مشکل گلوگاه در شبکه‌های \lr{RNN} معرفی شد. مکانیزم توجه بر خلاف روش‌های قبلی، به جای اتکا به یک بردار متنی با طول ثابت، به دیکودر این اجازه را می‌دهد که برای هر خروجی یک بردار متنی پویا ایجاد کند که این بردار متنی پویا یک جمع وزنی از تمام وضعیت‌های پنهان (از گذشته‌های دور تا الان)انکودر است.
%
%مسئله دیگری در مدل‌های \lr{Seq2Seq}، به‌ویژه با \lr{RNN}ها، دشواری در پردازش وابستگی‌های بلندمدت بود. \lr{RNN}های سنتی و حتی \lr{LSTM}ها تمایل دارند که به ورودی‌های جدید، بیشتر از ورودی‌های دورتر اهمیت دهند. 
%
%\lr{LSTM}
%ها برای کاهش مشکل محو شدن گرادیان طراحی شده‌اند که به ضبط وابستگی‌های طولانی‌تر نسبت به \lr{RNN}های معمولی کمک می‌کند. با این حال، تأثیر ورودی های دورتر همچنان تمایل دارد که با گذشت زمان کاهش یابد، هرچند نه به اندازه‌ای که در \lr{RNN}های استاندارد دیده می‌شود.
%
%در \lr{BiLSTM}ها دنباله را در هر دو جهت جلو و عقب پردازش می‌کنند و بنابراین اطلاعات را از هر دو زمینه گذشته و آینده فراهم می‌کنند.
%این رویکرد دوطرفه توانایی مدل را در ضبط وابستگی‌ها در هر دو جهت بهبود می‌بخشد.
%با این وجود، \lr{BiLSTM}ها همچنان به بردارهای با طول ثابت متکی هستند و با وابستگی‌های بسیار طولانی مشکل دارند.
%
%در عمل این موضوع به عنوان یکی از ضعف‌های این نوع شبکه‌ها محسوب می‌شود و شبکه \lr{Transformer} و به‌ویژه مکانیزم توجه این مشکل را حل نموده و وابستگی‌های طولانی مدت را در دنباله سیگنال ورودی، بیشتر از سایر شبکه‌ها درک می‌کند.
%
%مکانیزم توجه به دیکودر اجازه می‌دهد تا به هر قسمت از دنباله ورودی به‌طور مستقیم دسترسی داشته باشد، بدون توجه به موقعیت آن. این دسترسی مستقیم به این معنی است که ورودی دور نیز می‌توانند بر ورودی فعلی تاثیرگذار باشد.
%	
%\end{qsolve}