\section{سوال دوم - عملی}

آیا تا کنون به روند عملکرد بهینه‌سازها فکر کرده‌اید؟ آیا می‌توان آن را یک شبکه‌ی بازرخدادی در نظر گرفت؟ در این پروژه هدف طراحی و پیاده‌سازی یک بهینه‌ساز می‌باشد. برای درک بهتر، توضیحات ریاضیاتی زیر داده می‌شود.



روند یادگیری پارامترهای ($\theta$) یک شبکه عمیق ($f$) با الگوریتم های مرسوم نزول در راستای گرادیان (نظیر \lr{SGD}) را می‌توان به ازای ورودی‌های آموزشی $x$ بصورت رابطه‌ی (۱) در نظر گرفت:

\begin{equation}
	\theta_{i+1} = \theta_i - \alpha \nabla f(x; \theta_i)
\end{equation}

حال اگر فرض شود که به جای نرخ یادگیری ثابت $\alpha$ از یک تابع (شبکه‌ی عمیق) نظیر $g$ با پارامترهای قابل یادگیری $\phi$ استفاده کنیم، می‌توان رابطه‌ی (۱) را بصورت رابطه‌ی (۲) بازنویسی نمود:

\begin{equation}
	\theta_{i+1} = \theta_i + g(\nabla f(x; \theta_i); \phi)
\end{equation}

در نهایت می‌توان پارامتر $\theta_i$ را نیز به عنوان یک ورودی دیگر به $g$ در نظر گرفت و رابطه‌ی (۲) را نیز بصورت زیر بازنویسی کرد:

\begin{equation}
	\theta_{i+1} = g(\nabla f(x; \theta_i), \theta_i; \phi)
\end{equation}

حال می‌توان نتیجه گرفت که اگر تابع $g$ را یک شبکه‌ی بازرخدادی (نظیر \lr{LSTM} یا \lr{GRU}) در نظر گرفت، امکان ارئه‌ی یک بهینه‌ساز وجود دارد، که کل فرایند یاد شده را می‌توان با دو حلقه (بیرونی\footnote{\lr{Outer loop}} و درونی\footnote{\lr{Inner loop}}) انجام داد که معماری کلی آن در ادامه ضمیمه شده است. پیمایش یکبار حلقه‌ی بیرونی معادل است با یک تکرار (\lr{Epoch}) برای آموزش شبکه‌ی $g$ و پیمایش یکبار حلقه درونی معادل است با تولید یک داده آموزشی برای شبکه‌ی $g$.




در این سوال هدف طراحی و پیاده سازی یک بهینه‌ساز بر اساس توضیحات فوق می‌باشد:

\begin{enumerate}
	\item
مجموعه داده اول را به عنوان مجموعه آموزشی در نظر بگیرید، آن را نمایش داده و پس درهم سازی به ۵۰ زیر مجموعه تقسیم کنید بطوریکه در هر مجموعه داده از هر کلاس به تعداد برابر نمونه موجود باشد و انتخاب نمونه‌ها نیز با احتمال یکنواخت صورت گرفته باشد. حال، یک شبکه‌ی \lr{MLP} دلخواه طراحی نمایید و آن را $f$ بنامید که $f_1, f_2, \ldots$ شبکه‌های \lr{MLP} با معماری یکسان هستند و صرفا مقادیردهی اولیه‌ی آن‌ها هر بار متفاوت است.
	
	
	
	
	
	
	
	\item 
یک شبکه‌ی بازرخدادی مبتنی بر \lr{GRU} طراحی نمایید و آن را $g$ بنامید که وظیفه‌ی آن بهینه‌سازی وزن‌های قابل یادگیری معماری $f$ برای هدف مورد نظر می‌باشد. با استفاده از ۵۰ زیر مجموعه‌ی ایجاد شده در قسمت قبل، شبکه‌ی $g$ را آموزش دهید. روند پیاده‌سازی آموزش، معماری طراحی شده و سایر جزئیات مورد نظر را در گزارش درج نمایید. توجه داشته باشید به ازای هر حلقه‌ی درونی (در هر حلقه‌ی بیرونی)، یک مقداردهی کاملا جدید برای شبکه‌ی $f$ صورت می‌گیرد. در هر زیر مجموعه نسبت آموزش به آزمون ۲:۸ است.

	\item
مجموعه داده‌ی دوم را نیز بارگذاری کرده و آن را نمایش دهید و تفاوت‌های آن را با مجموعه داده‌ی اول بیان کنید. حال آن را به ۳۰ زیر مجموعه همانند توضیحات قسمت ۱ تقسیم کنید. در نهایت، به ازای هر مجموعه داده، یک شبکه با معماری $f$ در نظر گرفته و با شبکه‌ی $g$ آن را بهینه‌سازی کنید. میانگین دقت و خطا را گزارش نمایید.




	\item
\textbf{(اختیاری)} با مطالعه و تحقیق روشی ارائه دهید تا بتوان عملکرد بهینه‌ساز $g$ را بصورت کمی و کیفی ارزیابی نموده و همچنین بتوان آن را با بهینه‌ساز \lr{ADAM} مقایسه نمود.








\end{enumerate}

















































%شبکه‌های عمیق از عدم تفسیر‌پذیری رنج می‌برند. تلاش برای حل این مشکل، دو ایده \lr{Deconvolutional} و \lr{Up-convolutional} مظرح شده است. بررسی کنید و توضیح دهید هرکدام از دو روش، به چه صورت منجر به تفسیرپذیری می‌شوند؟
%
%
%
%\begin{qsolve}
%	پیش از توضیح دادن این دو روش که چگونه به تفسیرپذیری کمک می‌کنند، ابتدا این دو روش را مختثرا توضیح می‌دهیم.
%	
%	
%	\begin{enumerate}
%		\item \textbf{شبکه \lr{Deconvolutional} یا \lr{Transposed convolutional layer}: }\\
%در لایه‌های کانولوشن ویژگی‌های مهم تصویر با استفاده از یک کرنل استخراج می‌شود و خروجی به عنوان \lr{Feature map} شناخته می‌شود. ابعاد تصویر (ممکن است) کاهش یابد و اطلاعات مهم تصویر حفظ می‌شود.
%
%
%	\begin{center}
%		\includegraphics*[width=0.6\linewidth]{pics/img4.png}
%		\captionof{figure}{لایه کانولوشن}
%		\label{لایه کانولوشن}
%	\end{center}
%	
%	
%	
%	لایه \lr{Deconvolution} دقیقا برعکس لایه‌های کانولوشنی عمل می‌کند. یعنی از یک \lr{Feature map} می‌توان به تصویر رسید. الگ.ریتم \lr{Deconv} با نگاشت نقشه‌های ویژگی‌ به فضای ورودی، این امکان را فراهم می‌کند 
%
%
%
%
%
%	
%	
%		\item \textbf{\lr{ :Up-Convolution}}
%لایه \lr{Up-convolution} نیز همانند \lr{Deconvolution} ابعاد ورودی را زیاد می‌کند و هدف آن تولید یک تصویر بزرگتر از ورودی آن است.
%
%	\begin{center}
%		\includegraphics*[width=0.6\linewidth]{pics/img5.png}
%		\captionof{figure}{لایه \lr{Up-convolution}}
%		\label{لایه آپ‌کانولوشن}
%	\end{center}
%	\end{enumerate}
%	
%	
%	در بسیاری از مراجه این دو تکنیک را معادل‌با هم می‌گیرند چرا که در هر دو روش هدف افزایش ابعاد ورودی است و این کار دقیقا برعکس کانولوشن انجام می‌شود.
%\end{qsolve}
%
%
%
%
%\begin{qsolve}
%	لایه \lr{Deconvolution} و \lr{Up-convolution}با نمایش نقشه‌های ویژگی به فضای ورودی، به ما امکان می‌دهد ببینیم چه نوع الگوهای ورودی نورون‌های خاصی را فعال می‌کنند. در \lr{Up-convolution} 
%\end{qsolve}
%
%
%
%
%\begin{latin}
%	\begin{thebibliography}{9}
%		\bibitem{ref1}
%		Durall R, Keuper M, Keuper J. Watch your up-convolution: Cnn based generative deep neural networks are failing to reproduce spectral distributions. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2020 (pp. 7890-7899).
%		
%	\end{thebibliography} 
%\end{latin}
