\section{سوال دوم - نظری}


شبکه‌های عمیق از عدم تفسیر‌پذیری رنج می‌برند. تلاش برای حل این مشکل، دو ایده \lr{Deconvolutional} و \lr{Up-convolutional} مظرح شده است. بررسی کنید و توضیح دهید هرکدام از دو روش، به چه صورت منجر به تفسیرپذیری می‌شوند؟



\begin{qsolve}
	پیش از توضیح دادن این دو روش که چگونه به تفسیرپذیری کمک می‌کنند، ابتدا این دو روش را مختثرا توضیح می‌دهیم.
	
	
	\begin{enumerate}
		\item \textbf{شبکه \lr{Deconvolutional} یا \lr{Transposed convolutional layer}: }\\
در لایه‌های کانولوشن ویژگی‌های مهم تصویر با استفاده از یک کرنل استخراج می‌شود و خروجی به عنوان \lr{Feature map} شناخته می‌شود. ابعاد تصویر (ممکن است) کاهش یابد و اطلاعات مهم تصویر حفظ می‌شود.


	\begin{center}
		\includegraphics*[width=0.6\linewidth]{pics/img4.png}
		\captionof{figure}{لایه کانولوشن}
		\label{لایه کانولوشن}
	\end{center}
	
	
	
	لایه \lr{Deconvolution} دقیقا برعکس لایه‌های کانولوشنی عمل می‌کند. یعنی از یک \lr{Feature map} می‌توان به تصویر رسید. الگ.ریتم \lr{Deconv} با نگاشت نقشه‌های ویژگی‌ به فضای ورودی، این امکان را فراهم می‌کند 





	
	
		\item \textbf{\lr{ :Up-Convolution}}
لایه \lr{Up-convolution} نیز همانند \lr{Deconvolution} ابعاد ورودی را زیاد می‌کند و هدف آن تولید یک تصویر بزرگتر از ورودی آن است.

	\begin{center}
		\includegraphics*[width=0.6\linewidth]{pics/img5.png}
		\captionof{figure}{لایه \lr{Up-convolution}}
		\label{لایه آپ‌کانولوشن}
	\end{center}
	\end{enumerate}
	
	
	در بسیاری از مراجه این دو تکنیک را معادل‌با هم می‌گیرند چرا که در هر دو روش هدف افزایش ابعاد ورودی است و این کار دقیقا برعکس کانولوشن انجام می‌شود.
\end{qsolve}




\begin{qsolve}
	لایه \lr{Deconvolution} و \lr{Up-convolution}با نمایش نقشه‌های ویژگی به فضای ورودی، به ما امکان می‌دهد ببینیم چه نوع الگوهای ورودی نورون‌های خاصی را فعال می‌کنند. در \lr{Up-convolution} 
\end{qsolve}




\begin{latin}
	\begin{thebibliography}{9}
		\bibitem{ref1}
		Durall R, Keuper M, Keuper J. Watch your up-convolution: Cnn based generative deep neural networks are failing to reproduce spectral distributions. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2020 (pp. 7890-7899).
		
	\end{thebibliography} 
\end{latin}












