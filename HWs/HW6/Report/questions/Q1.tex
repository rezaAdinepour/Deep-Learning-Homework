\section{سوال اول - شبکه‌های مولد تقابلی}
شبکه‌های مولد تقابلی\footnote{\lr{‫‪Networks‬‬‫‪Adverserial‬‬ ‫‪Generative‬‬}} همانطور که در کلاس با آنها آشنا شدید شامل دو زیرشبکه‌ی تولیدکننده\footnote{\lr{‫‪Generator‬‬}} و تمایزگر\footnote{\lr{‫‪Discriminator‬‬}} هستند که به صورت تقابلی آموزش داده می‌شوند تا داده‌های جدید تولید کنند. تولید جدید هدفی است که در تمامی مدل‌های مولد مد نظر قرار دارد و به شکل‌های مختلف از جمله ترجمه‌ی تصویر به تصویر، تبدیل دامنه و تولید شرطی صورت می‌گیرد. یکی از این اشکال، تولید تصویر با دریافت فرمان زبانی است که امروزه نیز نمونه‌های کاربردی آن همچون \lr{Imagen} و \lr{Dall-E} در دسترس عموم قرار دارند. در این تمرین به طور خاص به پیاده سازی این وظیفه با شبکه‌ی مولد تقابلی پشته‌ای یا \lr{SatckGAN} می‌پردازیم.



\begin{enumerate}
	\item
	به مراجعه به مقاله \href{https://arxiv.org/abs/1612.03242}{\textcolor{magenta}{\lr{StackGAN}}} کلیت ساختار و چگونگی عملکرد این شبکه را توضیح دهید. توضیح دهید که شبکه‌ی تعریف شده در هر گام\footnote{\lr{Stage}} به چه منظور استفاده می‌شود. به طور خاص ذکر کنید که ورودی شبکه‌ی تولیدکننده در هر دو گام چه تفاوتی با ورودی یک شبکه‌ی مولد تقابلی ساده\footnote{\lr{Vanilla GAN}} دارد؟ همچنین بررسی کنید که آموزش این شبکه به چه صورت انجام می‌شود.
	
	\begin{center}
		\includegraphics*[width=0.9\linewidth]{pics/img1.png}
		\captionof{figure}{معماری کلی شبکه مولد تقابلی پشته ای}
		\label{معماری کلی شبکه مولد تقابلی پشته ای}
	\end{center}
	
	\begin{qsolve}
همانطور که در توضیحات سوال بیان شد، تولید تصاویر با کیفیت بالا از متن، چالشی است که این مقاله آن را حل نموده است. این مقاله با شکستن کار به دو مرحله:
	
	\begin{latin}
		\begin{enumerate}
			\item Stage-I GAN
			\item Stage-II GAN
		\end{enumerate}
	\end{latin}
	
	چالش تولید تصویر با وضوح و دقت بالا از داده‌های متنی را حل می‌کند.
	
	\begin{enumerate}
		\item 
		\textbf{مرحله اول: }\\
		
		\begin{itemize}
			\item \textbf{هدف: }مرحله اول \lr{GAN} مسئول تولید یک تصویر \lr{(64x64)} با وضوح پایین ست که شکل و رنگ اساسی شیء توصیف شده توسط متن را ترسیم می‌کند.
			
			\item \textbf{ورودی: }
			\begin{itemize}
				\item \textbf{بردار تبدیل شده متن:} توضیحات متنی با استفاده از یک انکودر از پیش ‌آموزش داده شده $\phi_t$ به یک بردار عددی تبدیل می‌شوند.
				
				\item \textbf{متغیرهای شرطی گوسی:} متغیرهای شرطی از توزیع گوسی که توسط تعبیه متنی پارامتره شده‌اند، نمونه‌برداری می‌شوند.
				
متغیرهای شرطی گوسی $\hat{c}_0$ برای تعبیه متنی از $N(\mu_0(\phi_t), \Sigma_0(\phi_t))$ نمونه‌برداری می‌شوند تا معنای $\phi_t$ را با تغییرات مختلف به تصویر بکشند. این می‌تواند پایداری آموزش و تنوع نمونه را فراهم کند، به عنوان مثال: انواع مختلف حالات و ظاهرها.


				
				\item \textbf{بردار نویز:} یک بردار نویز تصادفی $z$ با متغیرهای شرطی ترکیب می‌شود.
			\end{itemize}
			
			\item \textbf{خروجی: }یک تصویر با وضوح پایین که شکل اصلی و رنگ‌های اساسی متناظر با توضیحات متنی را به تصویر می‌کشد.
			
			
			
مشروط بر $\hat{c}_0$ و متغیر تصادفی $z$، مرحله اول \lr{GAN} متمایزکننده $D_0$ و تولیدکننده $G_0$ را با جایگزینی حداکثر کردن $L_{D_0}$ در معادله (1) و حداقل کردن $L_{G_0}$ در معادله (2) آموزش می‌دهد:
		
		\begin{equation}
			L_{D_0} = \mathbb{E}_{(I_0,t) \sim p_{data}} [\log D_0(I_0, \phi_t)] + \mathbb{E}_{z \sim p_z, t \sim p_{data}} [\log(1 - D_0(G_0(z, \hat{c}_0), \phi_t))],
		\end{equation}
			
		\begin{equation}
			L_{G_0} = \mathbb{E}_{z \sim p_z, t \sim p_{data}} [\log(1 - D_0(G_0(z, \hat{c}_0), \phi_t))] + \lambda D_{KL}(N(\mu_0(\phi_t), \Sigma_0(\phi_t)) \| N(0, I)),
		\end{equation}

تصویر واقعی $I_0$ و توضیح متنی $t$ از توزیع داده‌های واقعی $p_{data}$ هستند.		
		\end{itemize}
		
		
		
		\item 
		\textbf{مرحله دوم: }\\
		
		\begin{itemize}
			\item \textbf{هدف: }مرحله دوم \lr{GAN} تصویر با وضوح پایین تولید شده در مرحله اول را تصحیح می‌کند تا یک تصویر \lr{(256x256)} با وضوح بالا با جزئیات بیشتر و واقع‌گرایانه‌تر تولید کند.
		\end{itemize}
	\end{enumerate}
	\end{qsolve}
	
	
	\begin{qsolve}
		\begin{itemize}
			\item \textbf{ورودی: }
			\begin{itemize}
				\item \textbf{خروجی مرحله اول:} تصویر با وضوح پایین از مرحله اول.
				
				\item \textbf{تعبیه متنی: }تعبیه متنی دوباره برای شرطی‌سازی فرآیند تولید استفاده می‌شود، که اطمینان حاصل کند جزئیات تصویر با متن همخوانی دارد یا خیر.
				
				\item \textbf{متغیرهای شرطی گوسی:} متغیرهای شرطی جدید از توزیع گوسی که توسط تعبیه متنی پارامتره شده‌اند، نمونه‌برداری می‌شوند.
			\end{itemize}
			
			\item \textbf{خروجی: }یک تصویر با وضوح بالا و جزئیات دقیق که واقع‌گرایانه است که توضیحات متنی را به درستی منعکس می‌کند.
		\end{itemize}
		
		
		\begin{equation}
			L_D = \mathbb{E}_{(I,t) \sim p_{data}} [\log D(I, \phi_t)] + \mathbb{E}_{s_0 \sim p_{G_0}, t \sim p_{data}} [\log(1 - D(G(s_0, \hat{c}), \phi_t))],
		\end{equation}
		
		\begin{equation}
			L_G = \mathbb{E}_{s_0 \sim p_{G_0}, t \sim p_{data}} [\log(1 - D(G(s_0, \hat{c}), \phi_t))] + \lambda D_{KL}(N(\mu(\phi_t), \Sigma(\phi_t)) \| N(0, I)),
		\end{equation}
		
		\textbf{مرحله دوم \lr{GAN}} یاد می‌گیرد تا اطلاعات مفید در تعبیه متنی که توسط مرحله اول \textbf{\lr{GAN}} نادیده گرفته شده است را به دست آورد.
		
		\begin{itemize}
			\item تولیدکننده مرحله دوم یک شبکه انکودر-دکودر با بلوک‌های \lr{residual} است.
			\item ویژگی‌های تصویر و ویژگی‌های متن در طول بعد کانال ترکیب می‌شوند تا نمایش‌های چندوجهی در سراسر ویژگی‌های تصویر و متن را یاد بگیرند.
			\item در نهایت، از یک سری لایه‌های دیکودر برای تولید یک تصویر با وضوح بالا $W \times H$ استفاده می‌شود.
			\item به جای استفاده از متمایزکننده ساده، از متمایزکننده \lr{matching-aware} برای هر دو مرحله استفاده می‌شود.
		\end{itemize}
		
		
		
		
		\textbf{تفاوت‌های ورودی در مقایسه با \lr{GAN} های ساده: }\\
		\begin{enumerate}
			\item \textbf{\lr{GAN} های ساده:}
			\begin{itemize}
				\item \textbf{ورودی:} یک بردار نویز ساده به عنوان ورودی برای شبکه تولیدکننده استفاده می‌شود.
				\item \textbf{خروجی:}یک فرآیند تولید تک‌مرحله‌ای که به طور مستقیم سعی در ایجاد تصویر با وضوح بالا دارد.
			\end{itemize}
			
			
			\item \textbf{\lr{StackGAN:}}
			\begin{itemize}
				\item \textbf{ورودی مرحله اول: }ترکیبی از بردار نویز و متغیرهای شرطی مشتق شده از تعبیه‌های متنی برای تولید یک تصویر با وضوح پایین.
				
				\item \textbf{ورودی مرحله دوم: }از خروجی مرحله اول استفاده می‌کند و تعبیه‌های متنی را با متغیرهای شرطی جدید برای تصحیح تصویر و افزودن جزئیات مجدداً استفاده می‌کند.
			\end{itemize}
		\end{enumerate}
	\end{qsolve}
	
	
	
	
	
	
	
	
	
	
	
	\item 
شبکه‌های مولد تقابلی در مقایسه با سایر شبکه‌ها از سه مشکل اساسی رنج می‌برند: این سه مشکل عبارتند از فروپاشی مد\footnote{\lr{Mode Collapse}}، عدم همگرایی و ناپدید شدن گرادیان. به طور مختصر توضیح دهید که هر کدام به چه صورتی و چه راهکارهایی برای رفع آنها مطرح شده است؟

	\begin{qsolve}
		\begin{enumerate}
			\item \textbf{فروپاشی مد:}\\
فروپاشی مد زمانی رخ می‌دهد که مولد تعداد محدودی خروجی تولید می‌کند و عملاً به چند مد (یا حتی یک مد) از توزیع داده‌ها فرو می‌ریزد. این به این معناست که \lr{GAN} نمی‌تواند تمام تنوع توزیع داده‌های واقعی را به تصویر بکشد. برای حل این مشکل می‌توان از موارد زیر استفاده نمود:

			\begin{itemize}
				\item \textbf{تمایز دسته‌ای (\lr{Minibatch Discrimination}): }این تکنیک به تشخیص‌دهنده اجازه می‌دهد تا نمونه‌های متعدد را با هم بررسی و مقایسه کند، و مولد را تشویق می‌کند تا خروجی‌های متنوع‌تری تولید کند.
				
				\item \textbf{\lr{GAN} های بدون بازگشت (\lr{Unrolled GANs}): }این رویکرد فرآیند آموزش را تغییر می‌دهد تا بهینه‌سازی تشخیص‌دهنده را \lr{Unrolled} کند و اثر به‌روزرسانی‌های آینده را در نظر بگیرد که به جلوگیری از فروپاشی مد کمک می‌کند.
				
				\item \textbf{مجموعه‌ای از مولدها (\lr{Ensemble of Generators}): }استفاده از مولدهای متعدد می‌تواند به پوشش مدهای مختلف توزیع داده‌ها کمک کند و بنابراین فروپاشی مد را کاهش دهد.
			\end{itemize}
			
			
			
			\item \textbf{عدم همگرایی:}\\
عدم همگرایی یا هم‌گرایی ناکامل زمانی رخ می‌دهد که فرآیند آموزش \lr{GAN} نوسان کند یا نتواند به یک حالت پایدار برسد. این ناپایداری ناشی از این است که مولد و تشخیص‌دهنده دائماً سعی در شکست دادن یکدیگر دارند. برای اینکه شبکه ناپایدار نشود می‌توان راهکار های زیر را انجام داد:

			\begin{itemize}
				\item \textbf{معماری‌های بهبود یافته: }استفاده از تکنیک‌هایی مانند نرمال‌سازی طیفی که با کنترل ثابت \lr{Lipschitz} تشخیص‌دهنده، آموزش را پایدار می‌کند.
				
				\item \textbf{توابع هدف اصلاح شده: }توابع زیان جایگزین، مانند زیان \lr{Wasserstein} (استفاده شده در \lr{Wasserstein GANs} یا \lr{WGANs})، می‌تواند هم‌گرایی را با ارائه یک گرادیان معنادارتر برای مولد حتی زمانی که تشخیص‌دهنده نزدیک به بهینه است، بهبود بخشد.
				
				\item \textbf{تکنیک‌های تنظیم (\lr{Regularization}): }تکنیک‌هایی مانند \lr{gradient penalty} (استفاده شده در \lr{WGAN-GP}) می‌تواند با جریمه کردن نُرم گرادیان تشخیص‌دهنده، آموزش را پایدار تر کند و از ناپایداری جلوگیری کند.
			\end{itemize}
			
			
			
			
			\item \textbf{محوشدن گرادیان:}\\
ناپدید شدن گرادیان‌ زمانی رخ می‌دهد که گرادیان‌های بازگشتی از طریق شبکه بسیار کوچک می‌شوند و منجر به یادگیری آهسته یا عدم یادگیری شبکه مولد می‌شوند. این مشکل زمانی که تشخیص‌دهنده بسیار قوی شود و به راحتی بین داده‌های واقعی و جعلی تمایز قائل شود، بیشتر می‌شود. برای قلبه به این مشکل راه‌حل های زیر پیشنهاد می‌شود:
		\end{enumerate}
	\end{qsolve}
	
	\begin{qsolve}
		\begin{itemize}
			\item \textbf{استفاده از \lr{WGAN}: }
چارچوب \lr{Wasserstein GAN} مشکل ناپدید شدن گرادیان‌ها را با اطمینان از اینکه گرادیان‌ها ناپدید نمی‌شوند، حل می‌کند. این کار را با استفاده از تابع زیان متفاوت بر اساس فاصله \lr{Wasserstein} انجام می‌دهد که سیگنال گرادیان پایدارتر و مفیدتری ارائه می‌دهد.
			
			
			\item \textbf{تطبیق ویژگی (\lr{Feature Matching}): }
این تکنیک شامل آموزش مولد برای تطبیق آمار ویژگی‌های استخراج شده از یک لایه واسطه تشخیص‌دهنده است، به جای اینکه به طور مستقیم سعی کند تشخیص‌دهنده را فریب دهد.
			
			
			\item \textbf{تعادل در آموزش (\lr{Training Balance}): }
اطمینان از اینکه مولد و تشخیص‌دهنده ظرفیت‌های یادگیری متعادلی دارند و به طور متعادل آموزش داده می‌شوند می‌تواند از قوی شدن بیش از حد تشخیص‌دهنده نسبت به مولد جلوگیری کند و یادگیری هردو شبکه را در یک حد نگه دارد.
		\end{itemize}
	\end{qsolve}




	
	
	
	
	
	\item 
یک ایده‌ی رایج برای بهبود عملکرد شبکه‌های مولد تقابلی استفاده از عملگر \lr{PixelShuffle} است. نحوه‌ی عملکرد این عملگر و تأثیر آن را بررسی کنید. بررسی کنید که این عملگر اولین بار در چه وظیفه‌ای و به چه منظور تعریف شد؟ همچنین بررسی کنید که به طور خاص در معماری \lr{StackGAN} در کدام زیرشبکه‌ها قابل استفاده است و چه عملکردی خواهد داشت؟

	\begin{qsolve}
عملگر \lr{PixelShuffle} که به عنوان هم‌ترازی زیرپیکسلی نیز شناخته می‌شود، تکنیکی برای بهبود وضوح مکانی تصویر است. این عملگر عناصر یک تنسور را به گونه‌ای بازآرایی می‌کند که یک تنسور با وضوح مکانی بالاتر و عمق کانال کمتر تولید شود. این عملگر یک تنسور ورودی با شکل $(B, C \times r^2, H, W)$ را دریافت می‌کند و آن را به تنسوری با شکل $(B, C, H \times r, W \times r)$ تبدیل می‌کند، که در آن $B$ اندازه دسته، $C$ تعداد کانال‌ها، $H$ و $W$ ارتفاع و عرض تصویر و $r$ ضریب بزرگ‌نمایی است.

روابط ریاضی آن را می‌توان به‌صورت زیر بیان نمود:

با توجه به تنسور ورودی $X$ با شکل $(B, C \times r^2, H, W)$، عملگر \lr{PixelShuffle} آن را به تنسور خروجی $Y$ با شکل $(B, C, H \times r, W \times r)$ بازآرایی می‌کند:

		\begin{equation}
		Y(b, c, h \times r + i, w \times r + j) = X(b, c \times r^2 + i \times r + j, h, w)
		\end{equation}
		
برای بازه‌های:
		\begin{latin}
			\begin{enumerate}
				\item $b \in [0, B)$
				\item $c \in [0, C)$
				\item $h \in [0, H)$
				\item $w \in [0, W)$
				\item $i, j \in [0, r)$
			\end{enumerate} 
		\end{latin}
		
عملگر \lr{PixelShuffle} تاثیرات مفید زیادی بر \lr{GAN}ها دارد، به‌ویژه در وظایفی که نیاز به تولید تصاویر با وضوح بالا دارند برای مثال می‌توان به موارد زیر اشاره نمود:
	\end{qsolve}
	
	\begin{qsolve}
		\begin{enumerate}
			\item \textbf{وضوح بهبود یافته:}
با بازآرایی موثر عناصر تنسور، \lr{PixelShuffle} وضوح تصویر را افزایش می‌دهد که برای تولید تصاویر با کیفیت بالا حیاتی است.
			
			\item \textbf{کارایی:}
\lr{PixelShuffle} از لحاظ محاسباتی کارا است زیرا نیاز به لایه‌های بزرگ‌نمایی و کانولوشن‌های اضافی را کاهش می‌دهد و شبکه را سریع‌تر و با منابع کمتر می‌سازد.

			
			\item \textbf{استفاده بهتر از ویژگی‌ها:}
این عملگر به استفاده بهتر از ویژگی‌های یادگرفته شده توسط شبکه کمک می‌کند زیرا مستقیماً وضوح مکانی را بدون از دست دادن جزئیات یادگرفته شده بهبود می‌بخشد.
		\end{enumerate}
		

عملگر \lr{PixelShuffle} ابتدا در زمینه وظایف سوپررزولوشن، به‌ویژه در مقاله \lr{Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network" (ESPCN)}
توسط \lr{Wenzhe Shi} و همکاران تعریف شد. هدف این بود که با یادگیری یک عملگر کانولوشن زیرپیکسلی کارا، وضوح تصویر را بدون هزینه محاسباتی قابل توجهی افزایش دهد.


	\textbf{استفاده در معماری \lr{StackGAN} در زیرشبکه دوم: }
	
	\begin{enumerate}
		\item در مرحله دوم معماری \lr{PixelShuffle} هدف بهبود رزولوشن تصاویر تولید شده در مرحله اول است حال اگر با عملگر \lr{PixelShuffle} ادغام شود، مدل می‌تواند تصاویر با جزئیات بیشتر و کیفیت بالاتری تولید کند.
		
		
		\item \lr{PixelShuffle} پیچیدگی محاسباتی عملیات بزرگ‌نمایی را کاهش می‌دهد که منجر به زمان‌های آموزش سریع‌تر می‌شود.
	\end{enumerate}

		
	\end{qsolve}
	
	
	
	
	
	
	\item 
معیار \lr{FID (Frechet Inception Score)} یک معیار برای ارزیابی کیفیت و تنوع تصاویر تولید شده توسط مدل‌های مولد است. توضیح دهید که این معیار به چه صورت محاسبه می‌شود، به چه ویژگی‌هایی از مدل و یا داده وابسته است و آیا معیار قابل اتکایی برای مقایسه‌ی مدل‌های مولد محسوب می‌شود؟
	
	\begin{qsolve}
امتیاز \lr{FID} میزان شباهت بین توزیع تصاویر واقعی و توزیع تصاویر تولید شده را اندازه‌گیری می‌کند. مراحل محاسبه \lr{FID} به شرح زیر است:

	\begin{enumerate}
		\item \textbf{استخراج ویژگی‌ها:}\\
از مدل \lr{Inception v3} پیش‌آموزش دیده برای استخراج ویژگی‌ها از تصاویر واقعی و تصاویر تولید شده استفاده می‌شود. فرض می‌کنیم بردار‌های ویژگی به‌صورت $\{f_i\}_{i=1}^N$ برای تصاویر واقعی و $\{g_j\}_{j=1}^M$ برای تصاویر تولید شده باشد
		
		
		\item \textbf{برازش توزیع گاوسی چندمتغیره:}\\
فرض می‌شود که ویژگی‌های استخراج شده از یک توزیع گاوسی چندمتغیره پیروی می‌کنند. میانگین و کواریانس این توزیع‌ها برای تصاویر واقعی و تصاویر تولید شده محاسبه می‌شود:

برای تصاویر واقعی: میانگین $ \mu_r \) و کواریانس \( \Sigma_r $

برای تصاویر تولید شده: میانگین $ \mu_g \) و کواریانس \( \Sigma_g $

و درنهایت امتیاز \lr{FID} به‌صورت زیر محاسبه می‌شود:
	\end{enumerate}
	\end{qsolve}
	
	
	\begin{qsolve}
		
		$$ 
			\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r 	\Sigma_g)^{1/2}) 
		$$
		
در اینجا، $ ||\mu_r - \mu_g||^2 $ اختلاف مربعی بین میانگین‌های توزیع‌های ویژگی‌های واقعی و تولید شده است، و $ \text{Tr}(\cdot) $ قطر ماتریس را نشان می‌دهد.



	\begin{enumerate}
		\item \textbf{ویژگی‌هایی که lr{FID} به آن‌ها بستگی دارد: }\\
امتیاز \lr{FID} به چند ویژگی مدل و داده بستگی دارد:

		\begin{itemize}
			\item \textbf{کیفیت تصاویر تولید شده:}\\
تصاویر تولید شده با کیفیت بالا که مشابه تصاویر واقعی هستند منجر به امتیاز \lr{FID} پایین‌تر می‌شوند که نشان‌دهنده عملکرد بهتر مدل مولد است.

		
			\item \textbf{تنوع تصاویر تولید شده:}\\
امتیاز \lr{FID} همچنین تنوع تصاویر تولید شده را نیز در نظر می‌گیرد. اگر تصاویر تولید شده طیف وسیعی از توزیع تصاویر واقعی را پوشش دهند، امتیاز کمتر خواهد بود. فروپاشی مد (که در آن مدل تصاویر کمتر متنوع تولید می‌کند) منجر به افزایش امتیاز \lr{FID} می‌شود.

			\item \textbf{مدل \lr{Inception} پیش‌آموزش دیده:}\\
انتخاب مدل پیش‌آموزش دیده (\lr{Inception v3}) و لایه‌ای که ویژگی‌ها از آن استخراج می‌شود، تاثیر زیادی بر امتیاز \lr{FID} دارد. ویژگی‌ها باید به اندازه کافی تمایزپذیر باشند تا جنبه‌های مهم تصاویر را ثبت کنند.

			\item \textbf{تعداد تصاویر:}\\
تعداد تصاویر استفاده شده برای محاسبه میانگین‌ها و کواریانس‌ها نیز می‌تواند بر امتیاز \lr{FID} تاثیر بگذارد. اندازه نمونه بزرگتر می‌تواند برآورد دقیق‌تری از توزیع‌های زیرین ارائه دهد.
			
		\end{itemize}
		
		
		
		
		\item \textbf{قابلیت اطمینان \lr{FID} برای مقایسه مدل‌های مولد:}\\
امتیاز \lr{FID} به طور کلی به عنوان معیاری قابل اعتماد برای مقایسه مدل‌های مولد شناخته می‌شود، اما دارای محدودیت‌هایی نیز هست:

		\begin{itemize}
			\item \textbf{حساسیت به انتخاب مدل:}\\
انتخاب شبکه پیش‌آموزش دیده و لایه‌ای که برای استخراج ویژگی‌ها استفاده می‌شود می‌تواند بر امتیاز تاثیر بگذارد. لایه‌های مختلف ممکن است جنبه‌های مختلف تصاویر را ثبت کنند.


			\item \textbf{وابستگی به مجموعه داده:}\\
			\lr{FID} وابسته به مجموعه داده خاص و توزیع ویژگی‌های درون آن مجموعه داده است. بنابراین، مقایسه‌ها باید در بستر همان مجموعه داده انجام شود.
			
			
			\item \textbf{سوگیری با اندازه نمونه کوچک:}\\
			\lr{FID} می‌تواند با اندازه نمونه کوچک مغرضانه باشد. نمونه‌های بزرگتر و نماینده‌تر منجر به امتیازهای قابل اعتمادتر می‌شوند.
			
			
			\item \textbf{نادیده گرفتن کیفیت ادراکی:}\\
در حالی که \lr{FID} شباهت‌های آماری بین توزیع‌ها را ثبت می‌کند، ممکن است کیفیت ادراکی را به طور کامل منعکس نکند. دو مجموعه تصاویر با امتیازهای \lr{FID} مشابه می‌توانند سطح‌های مختلفی از کیفیت ادراکی داشته باشند.
		\end{itemize}
		
	\end{enumerate}
	\end{qsolve}



	
	
	
	
	
	\item 
مدل را بر روی این داده‌ها آموزش دهید. معماری نهایی هر یک از چهار زیرشبکه به همراه نمودار خطای تولیدکننده و تمایزگر در هر گام آموزش را در گزارش خود بیاورید. پس از پایان آموزش ۱۰ تصویر را به صورت تصادفی از خروجی مدل در \lr{stage} اول و دوم تولید کنید.
	
	
برای این پروژه از مجموعه داده‌ی \lr{CUB-2011} استفاده می‌کنیم که شامل یازده هزار تصویر از ۲۰۰ گونه پرنده می‌باشد و به ازای هر تصویر یک توصیف متنی نیز وجود دارد. \href{https://www.kaggle.com/datasets/veeralakrishna/200-bird-species-with-11788-images}{\textcolor{magenta}{مجموعه‌ی داده}} در سایت \lr{Kaggle} و توصیفات متنی نیز در \href{https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view?resourcekey=0-sZrhftoEfdvHq6MweAeCjA}{\textcolor{magenta}{این لینک}} موجود است. همچنین برای توصیفات متنی از پیش تعبیه‌ی آماده شده در فایل \lr{char-CNN-RNN-embeddings.pickle} وجود دارد که می‌تواند جایگزین ساختن داده باشد. استفاده از پیش تعبیه‌ها نیز که منجر به کارایی بهتر مدل شوند دارای ۵ امتیاز اضافی می‌باشد.
	
	\begin{center}
		\includegraphics*[width=0.9\linewidth]{pics/img2.png}
		\captionof{figure}{نمونه خروجی مدل \lr{StackGAN} برای مجموعه داده \lr{CUB}}
		\label{نمونه خروجی مدل StackGAN برای مجموعه داده CUB}
	\end{center}
	
	
	\begin{qsolve}
		شبکه طراحی شده است و در ۶۰۰ دوره آموزش داده شده است.
		
		خروجی های تولید شده این شبکه به ازای متن های مشخص به صورت زیر است:
		
		\begin{center}
			\includegraphics*[width=0.9\linewidth]{pics/img13.jpg}
			\captionof{figure}{خروجی ۱}
			\label{خروجی ۱}
		\end{center}
		
		\begin{center}
			\includegraphics*[width=0.9\linewidth]{pics/img14.jpg}
			\captionof{figure}{خروجی ۲}
			\label{خروجی ۲}
		\end{center}
	\end{qsolve}
	
	
	
	\begin{qsolve}
		\begin{center}
			\includegraphics*[width=0.9\linewidth]{pics/img15.jpg}
			\captionof{figure}{خروجی ۳}
			\label{خروجی ۳}
		\end{center}
	\end{qsolve}
\end{enumerate}