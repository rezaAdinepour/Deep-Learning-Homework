\section{سوال دوم - عملی}
مجموعه داده ضمیمه شده را بارگزاری کرده و آن را نمایش دهید. تفکیک مجموعه داده را با نسبت ۱:۲:۷ به‌ترتیب برای آموزش، آزمون و اعتبارسنجی درنظر بگیرید.

\textbf{تمامی کدهای سوالات عملی پیوست شده است. همچنین می‌توانید کد‌ها را از گیتهاب بنده به لینک زیر مشاهده و بررسی کنید: }

\begin{latin}
	\texttt{\href{https://github.com/rezaAdinepour/Deep-Learning-Homework}{github.com/rezaAdinepour/Deep-Learning-Homework}} 
\end{latin}

\textbf{همچنین به دلیل آنکه تمامی کد ها در فایل های \texttt{notebook} به طور کامل توضیح داده شده است، در این گزارش به دلیل جلوگیری از طولانی شدن آن، از آوردن کد پرهیز کرده ایم.}


\begin{qsolve}
	
	پس از load کردن دیتاست و تقسیم کردن آن به ۳ دسته آموزش، تست و اعتبار سنجی، آن را رسم کردیم. خروجی به صورت زیر است:
	
	\begin{center}
		\includegraphics*[width=1\linewidth]{pics/img7.png}
		\captionof{figure}{دیتاست مسئله}
		\label{دیتاست مسئله}
	\end{center}
	
	همانطور که مشاهده می‌شود، داده ها جداپذیر حطی نیستند پس نمی‌توان به صورت معمولی آن را با شبکه پرسپترون حل نمود. در این سوال تلاش خواهیم کرد با روش‌های معرفی شده در سوال ۱، به صورت عملی با شبکه پرسپترون، یک مسئله غیر خطی را حل نماییم.
\end{qsolve}



\begin{enumerate}
	\item با یک نورون پرسپترونی و صرفا بر اساس ویژگی های ورودی، وزن‌های نورون خود را با آموزش بدست آورید و دسته‌بندی را انجام دهید. و معیار های صحت\footnote{\lr{Accuracy}} و امتیاز F1 را به ازای هر دسته گزارش نمایید. همچنین در نهایت وزن‌های معماری‌تان را به همراه طرحواره آن گزارش کنید.
	
	
	
	\begin{qsolve}
		شبکه پرسپترون تک لایه با استفاده از کتابخانه \texttt{PyTorch} تعریف شده است. از تابع Sigmoid به عنوان تابع فعال‌ساز استفاده شده است. در این شبکه از تابع بهینه‌ساز \lr{stochastic gradient descent} استفاده شده است و برای محاسبه خطا، از \lr{mean squared error} به عنوان تابع هزینه استفاده شده است.
		
		پس از ۵۰۰ ایپاک آموزش شبکه، خروجی شبکه به صورت زیر شده است:
		\begin{latin}
			\texttt{Epoch 500/500, Loss: 0.3373, Accuracy: 0.5263, F1 Score: 0.5115}\\
			\texttt{---------------------------------------------------------------}\\
			\texttt{Validation Loss: 0.3900, Validation Accuracy: 0.5173, Validation F1 Score: 0.4689
			}\\
			\texttt{---------------------------------------------------------------}
		\end{latin}
		
		همچنین نمودار‌های دقت و خطا بر حسب تعداد \lr{Epoch} به صورت زیر شده است:
		
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img8.png}
			\captionof{figure}{نمودار های خطا و دقت}
			\label{نمودار های خطا و دقت}
		\end{center}
		
		همانطور که از نمودار ها و خروجی شبکه مشاهده می‌شود، نشان از نوسان بالا در شبکه را دارد که کاملا طبیعیست چرا که داده ها جداپذیر خطی نیستند و شبکه هرچه تلاش می‌کند تا دو کلاس را از هم تفکیک کند نمی‌تواند و دچار نوسان می‌شود. مقدار \lr{Loss} از ۰٫۳ پایین تر نمی‌آید همچنین مشاهده می‌شود دقت روی مجموعه داده اعتبارسنجی بعد از ایپاک ۳۰۰ به شدت افت شده است.
		
		
		ماتریس پراکندگی\footnote{\lr{Confusion matrix}} برای مجموعه داده‌های آموزش، تست و اعتبارسنجی به صورت زیر شده است:
		
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img9.png}
			\captionof{figure}{ماتریس پراکندگی}
			\label{ماتریس پراکندگی ۱}
		\end{center}
	\end{qsolve}
	
	
	
	
	
	
	
	\begin{qsolve}
		از خروجی ماتریس‌های پراکندگی نیز مشاهده می‌شود که تعداد نمونه های که اشتباه شناسایی شده اند (نمونه های روی قطر فرعی) زیاد هستند که نشان از عدم آموزش شبکه دارد.
		
		پس از فاز آموزش شبکه، نوبت به تست شبکه آموزش داده شده به وسیله مجموعه داده‌های تست می‌رسد. خروجی شبکه بر روی داده های تست به صورت زیر شده است:
		
		\begin{latin}
			\texttt{Test loss: 0.3493, Test Accuracy: 0.5289, Test F1 Score: 0.5289}
		\end{latin}
		
		وزن‌های نهایی شبکه پس از آموزش به صورت زیر به‌دست آمده است:
		\begin{latin}
			\texttt{Final weights: tensor([[ 0.1273, -0.1831]], device='cuda:0')}
		\end{latin}
	\end{qsolve}
	
	
	
	
	
	
	\item به ورودی قسمت قبل، توان بالای ورودی ها تا توان سوم را افزوده و نتیجه حاصل را ضمن گزارش تحلیل نموده و توجیه کنید.
	
	\begin{qsolve}
		در این قسمت، ابتدا توان دوم و سوم ورودی را محاسبه می‌کنیم و هر دو آن را به شبکه می‌دهیم. بر اساس آنچه که در سوال اول توضیح دادیم، ممکن است با به توان رساندن ویژگی های ورودی داده هایی که جداپذیر خطی نیستند، جداپذیر خطی شوند. در این مثال داده هارا پس از به توان رساندن رسم کردیم:
		
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img10.png}
			\captionof{figure}{ویژگی های به توان رسانده شده}
			\label{ویژگی های به توان رسانده شده}
		\end{center}
		
		همانطور که از «شکل \ref{ویژگی های به توان رسانده شده}» مشاهده می‌شود، توان دوم داده‌های ورودی جداپذیر خطی شده است و امکان حل آن با یک نرون پرسپترون وجود دارد اما توان سوم داده‌های ورودی همچنان جداناپذیر خطی است. توان چهارم نیز تست شد، آن هم جداناپذیر خطی بود اما به دلیل آنکه در صورت سوال خواسته نشده است. گزارش آن آورده نشده است.
		
		انتظار داریم اگر به ورودی شبکه، توان دوم داده‌های دیتاست را بدهیم، شبکه بتواند مسئله را حل نماید و مقدار Loss آن مینیمم و Accuracy آن ماکزیمم شود. این‌بار داده‌های جدید را با همان شبکه قبلی مجدد Train می‌کنیم و خروجی‌های آن را گزارش می‌دهیم.
		
		پس از آموزش شبکه با ویژگی های جدید، خروجی شبکه به صورت زیر به‌دست آمده است:
		
		\begin{latin}
			\texttt{Epoch 500/500, Loss: 0.0760, Accuracy: 0.9051, F1 Score: 0.8139}\\
		\end{latin}
	\end{qsolve}
	
	
	\begin{qsolve}
		\begin{latin}
			\texttt{---------------------------------------------------------------}\\
			\texttt{Validation Loss: 0.1162, Validation Accuracy: 0.7977, Validation F1 Score: 0.7822}\\
			\texttt{---------------------------------------------------------------}\\
		\end{latin}
		
				مشاهده می‌شود که مقدار Loos گزارش شده بر روی داده های آموزش بسیار کوچکتر از حالت قبل است و تقریبا نزدیک به صفر. همچنین Accuracy شبکه نیز نسبت به حال قبل مقدار ۴۰ درصد افزایش یافته است. مقادیر گزارش شده بر روی داده‌های اعتبارسنجی نیز نسبت به حال قبل بهبود یافته اند.
		
		نمودار Loss و Accuracy خروجی شبکه به صورت زیر گزارش می‌شود:
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img11.png}
			\captionof{figure}{نمودار‌های خطا و دقت}
			\label{نمودار‌های خطا و دقت ۲}
		\end{center}
		
		نوسانات کمتری نسبت به قسمت قبل در نمودار Accuracy مشاهده می‌شود و دیگر دقت برای مجموعه داده های اعتبارسنجی افت نکرده است. همچنین مقدار خطا نیز با شیب بیشتری نسبت به حالت قبل کاهش پیدا کرده است.
		
		خروجی ماتریس پراکندگی شبکه نیز به صورت زیر است:
		\begin{center}
			\includegraphics*[width=1\linewidth]{pics/img12.png}
			\captionof{figure}{ماتریس پراکندگی}
			\label{ماتریس پراکندگی ۲}
		\end{center}
		
		مشاهده می‌شود که نمونه های درست طبقه‌بندی شده (نمونه های روی قطر اصلی) نسبت به حال قبل بیشتر است و همچنین نمونه های اشتباه نیز به مراتب کمتر از حالت قبل است.
		
	\end{qsolve}
	
	
		
	\begin{qsolve}
		خروجی شبکه بر روی مجموعه داده‌های Test به صورت زیر شده است:
		
		\begin{latin}
			\texttt{Test loss: 0.0965, Test Accuracy: 0.8300, Test F1 Score: 0.6127}\\
		\end{latin}
		
		خروجی شبکه بر روی داده های تست نیز بهبود زیادی نسبت به حالت قبل داشته است.
		
		همچنین وزن‌های نهایی شبکه به صورت زیر گزارش می‌شود:
		\begin{latin}
			\texttt{Final weights: tensor([[-0.0438, -0.0453]], device='cuda:0')}\\
		\end{latin}
		
		
		مجددا تمام مراحل را برای توان سوم داده‌های ورودی تکرار می‌کنیم. مطابق با «شکل \ref{ویژگی های به توان رسانده شده}» انتظار داریم خروجی شبکه پس از آموزش تقریبا همانند توان اول داده ها باشد، چرا که در هر دو حالت داده‌ها جداناپذیر خطی هستند. خروجی شبکه آموزش دیده بر روی توان سوم داده‌های ورودی به صورت زیر گزارش می‌شود:
		
		\begin{latin}
			\texttt{Epoch 500/500, Loss: 0.4831, Accuracy: 0.4759, F1 Score: 0.3548}\\
			\texttt{---------------------------------------------------------------}\\
			\texttt{Validation Loss: 0.4572, Validation Accuracy: 0.5167, Validation F1 Score: 0.3793}
		\end{latin}
		
		
		
		
	\end{qsolve}
	
	
	
	
\end{enumerate}







































%سیستم زیر را در نظر بگیرید:
%
%\begin{figure}[h]
%	\centering
%	\includegraphics*[width=0.6\linewidth]{pics/q2_1.png}
%\end{figure}
%
%سیگنال ورودی $x_c(t)$ دارای تبدیل فوریە ی زیر است.
%
%\begin{figure}[h]
%	\centering
%	\includegraphics*[width=0.3\linewidth]{pics/q2_2.png}
%\end{figure}
%
%که در آن :
%
%\[
%	\Omega_0=2\pi 1000rad/s
%\]
%
%سیستم گسسته زمان یک فیلتر پایین گذر ایده آل با پاسخ فرکانسی زیر است.
%
%\[
%	H(e^{j\omega})=\begin{cases}
%		1 & |\omega|<\omega_c \\
%		0 & \text{otherwise}
%	\end{cases}
%\]
%
%الف) کمینه نرخ نمونه برداری که در آن aⅼiasing رخ نمیدهد چقدر است؟
%
%\begin{qsolve}[]
%    کمینه نرخ نمونه برداری از حد نایکوییست بدست میاید بدین صورت که:
%    \[
%        \omega_s=\frac{2\pi}{T_s}\geq\omega_{nq}=2\omega_{max}=2\Omega_0\Rightarrow
%        T_s\leq\frac{\pi}{\Omega_0}=\frac{1}{2000}s=0.5ms
%    \]
%\end{qsolve}
%
%ب) اگر $\omega_c=\pi/2$ کمینه نرخ نمونه برداری به طوری که $y_t(t)=x_c(t)$؟
%
%
%\begin{qsolve}[]
%    اگر شرط نایکوییست ارضا شود، خواهیم داشت که:
%    \[
%        X(e^{j\Omega})=X_p(j\omega)\when_{\Omega=\omega T}    
%    \]
%    حال میخواهیم بعد از گسسته شدن، فیلتر اثری روی سیگنال نگذارد، یعنی کل محتوای فرکانسی زیر 
%    $\omega_c$ باشد. آنگاه:
%    \[
%      \Omega_{max}=\omega_{max}T=\Omega_0T\leq\omega_c=\frac{\pi}{2}\Rightarrow T\leq\frac{\pi}{2\Omega_0}
%      =\frac{1}{4000}s=0.25ms  
%    \]
%    که این شرط محدودیت بیشتری از شرط نایکوییست اعمال میکند.
%\end{qsolve}
