
\فصل{بررسی و مقایسه مقالات}\label{بررسی و مقایسه مقالات}

\قسمت{مقاله \مرجع{bao2023all}}

مقاله \lr{"All are Worth Words: A ViT Backbone for Diffusion Models"} به بررسی استفاده از ویژن ترنسفورمرها به عنوان هسته اصلی مدل‌های انتشار می‌پردازد. این مقاله نشان می‌دهد که \lr{ViT} می‌تواند به طور موثر جایگزین معماری‌های متداول مبتنی بر \lr{CNN} مانند \lr{U-Net} شود و در برخی موارد عملکرد بهتری داشته باشد.


\زیرقسمت{معماری شبکه}
معماری مدل ارائه شده در این مقاله به صورت شکل «\رجوع{شکل:معماری مدل ارائه شده در bao2023all}»:

\شروع{شکل}[ht]
\centerimg{img9}{10cm}
\شرح{معماری مدل ارائه شده در \مرجع{bao2023all}}
\برچسب{شکل:معماری مدل ارائه شده در bao2023all}
\پایان{شکل}



\زیرقسمت{چکیده}
در این مقاله، یک معماری ساده و عمومی مبتنی بر \lr{ViT} به نام \lr{U-ViT} ارائه شده است. \lr{U-ViT} تمام ورودی‌ها از جمله زمان، شرط و پچ‌های تصویر نویزی را به عنوان توکن‌ها در نظر می‌گیرد و اتصالات بلند بین لایه‌های سطحی و عمیق را الهام گرفته از \lr{U-Net} به کار می‌گیرد. این مقاله \lr{U-ViT }را در دو کاربرد ارزیابی می‌کند: تولید تصویر بدون شرط، تولید تصویر شرطی بر اساس کلاس و تولید متن به تصویر. نتایج نشان می‌دهد که \lr{U-ViT} قابل مقایسه با \lr{U-Net} مبتنی بر \lr{CNN} با اندازه مشابه است و در برخی موارد برتر نیز می‌باشد. مدل‌های انتشار نهان با \lr{U-ViT} به نمرات \lr{FID} رکوردشکن \lr{2.29} در تولید تصویر شرطی بر اساس کلاس در \lr{ImageNet 256×256} و \lr{5.48} در تولید متن به تصویر در \lr{MS-COCO} دست یافته‌اند.



\زیرقسمت{اهمیت}
\begin{itemize}
	\item \textbf{عملکرد برتر}:
	
	\lr{U-ViT} در وظایف مختلف مانند تولید تصویر بدون شرط، تولید تصویر شرطی بر اساس کلاس و تولید متن به تصویر عملکرد قابل مقایسه یا حتی برتری نسبت به U-Net مبتنی بر CNN نشان داده است.
	
	\item \textbf{ساختار ساده‌تر}:
	
	 \lr{U-ViT} با استفاده از ساختار ساده‌تری نسبت به \lr{U-Net}، پیچیدگی محاسباتی را کاهش داده و همچنان عملکرد بهینه‌ای ارائه می‌دهد.
	 
	\item \textbf{کاربرد وسیع}:
	
	 استفاده از \lr{ViT} به عنوان هسته اصلی مدل‌های انتشار می‌تواند در حوزه‌های مختلفی مانند پردازش تصویر و تولید متن به تصویر به کار گرفته شود و نتایج قابل توجهی به همراه داشته باشد.
	 
	\item \textbf{پایداری در آموزش}:
	
	 \lr{ViT} به دلیل استفاده از مکانیزم‌های توجه به خود، توانایی بهتری در مدیریت وابستگی‌های بلندمدت دارد که این ویژگی باعث پایداری بیشتر در فرایند آموزش مدل‌های انتشار می‌شود.
\end{itemize}








\قسمت{مقاله \مرجع{peebles2023scalable}}
مقاله \lr{"Scalable Diffusion Models with Transformers"} به بررسی استفاده از ترانسفورمرها برای توسعه مدل‌های انتشار مقیاس‌پذیر می‌پردازد. این مقاله نشان می‌دهد که مدل‌های ترانسفورمر می‌توانند به طور مؤثری جایگزین معماری‌های مبتنی بر \lr{CNN} شوند و در بسیاری از موارد عملکرد بهتری ارائه دهند.

\زیرقسمت{معماری شبکه}
معماری مدل ارائه شده در این مقاله به‌صورت شکل «\رجوع{شکل:معماری مدل ارائه شده در peebles2023scalable}» است:

\شروع{شکل}[ht]
\centerimg{img6}{15cm}
\شرح{معماری مدل ارائه شده در \مرجع{peebles2023scalable}}
\برچسب{شکل:معماری مدل ارائه شده در peebles2023scalable}
\پایان{شکل}


\زیرقسمت{چکیده}
در این مقاله، مدل‌های انتشار ترانسفورمری معرفی شده‌اند که یک هسته مبتنی بر ترانسفورمر را برای مدل‌های انتشار ارائه می‌دهند و عملکردی برتر نسبت به \lr{U-Net} های مبتنی بر \lr{CNN} نشان می‌دهند. مدل‌های \lr{DiT} مقیاس‌پذیری و ویژگی‌های عالی ترانسفورمرها را به نمایش می‌گذارند. این مدل‌ها با کاهش کارایی محاسباتی و حفظ کیفیت تولید، قادر به تولید تصاویر با کیفیت بالا هستند.

\زیرقسمت{اهمیت}
\begin{itemize}
	\item \textbf{عملکرد برتر}:
	
	مدل‌های \lr{DiT} در مقایسه با مدل‌های انتشار قبلی از جمله \lr{U-Net} مبتنی بر \lr{CNN} عملکرد بهتری دارند و \lr{FID} کمتری را در تولید تصاویر نشان می‌دهند.
	
	\item \textbf{کاهش کارایی محاسباتی}:
	
	مدل‌های \lr{DiT} با استفاده از \lr{Transformers}، به طور قابل توجهی نیاز به محاسبات را کاهش می‌دهند و همچنان کیفیت بالایی را ارائه می‌دهند.
	
	\item \textbf{مقیاس‌پذیری}:
	
	این مدل‌ها به دلیل استفاده از معماری \lr{Transformer}، به خوبی با افزایش اندازه مدل و تعداد توکن‌ها مقیاس می‌شوند.
	
	\item \textbf{استفاده بهینه از منابع}:
	
	مدل‌های \lr{DiT} با استفاده از تکنیک‌های نوین، مانند \lr{adaLN-Zero}، کارایی محاسباتی بهتری را ارائه می‌دهند و نیاز به منابع محاسباتی کمتری دارند.
	
	\item \textbf{کاربرد گسترده}:
	
	مدل‌های \lr{DiT} می‌توانند به عنوان یک ستون فقرات مقیاس‌پذیر برای تولید متن به تصویر مانند \lr{DALL-E 2} و \lr{Stable Diffusion} به کار گرفته شوند.
\end{itemize}







\قسمت{اندازه مدل}
مدل‌های \lr{DiT} شامل 33 میلیون تا 675 میلیون پارامتر و 0.4 تا 119 گیگافلاپس هستند. این مدل‌ها از ادبیات \lr{ViT} گرفته شده‌اند که نشان داده‌اند افزایش همزمان عمق و عرض به خوبی عمل می‌کند.

\قسمت{دیکودر ترانسفورمر}
دیکودر ترانسفورمر یک ارتقاء معماری است که \lr{U-Net} را با ویژن ترنسفورمرها جایگزین می‌کند و نشان می‌دهد که سوگیری استقرایی \lr{U-Net} برای عملکرد مدل‌های انتشار ضروری نیست.

\قسمت{آموزش و استنتاج}
در طول آموزش، یک مدل انتشار تصویری را که به آن نویز اضافه شده است، جاسازی توصیفی و یک جاسازی از زمان فعلی را دریافت می‌کند. سیستم یاد می‌گیرد که از جاسازی توصیفی برای حذف نویز در مراحل زمانی متوالی استفاده کند. در مرحله استنتاج، یک تصویر با شروع از نویز خالص و یک جاسازی توصیفی تولید می‌کند و نویز را به صورت تکراری بر اساس آن جاسازی حذف می‌کند.

\قسمت{معیارهای ارزیابی}
کیفیت خروجی \lr{DiT} بر اساس فاصله فریشیت\پانویس{FID} ارزیابی می‌شود که اندازه‌گیری می‌کند چگونه توزیع نسخه تولید شده یک تصویر با توزیع تصویر اصلی مقایسه می‌شود (عدد کمتر بهتر است).

معیار \lr{FID} با توجه به بودجه پردازش بهبود می‌یابد. بر روی تصاویر \lr{ImageNet} با وضوح 256 در 256 پیکسل، یک \lr{DiT} کوچک با 6 گیگافلاپس قدرت محاسباتی به \lr{FID 68.4} دست می‌یابد، یک \lr{DiT} بزرگ با 80.7 گیگافلاپس به \lr{FID 23.3} می‌رسد و بزرگترین \lr{DiT} با 119 گیگافلاپس به \lr{FID 9.62} دست می‌یابد. یک مدل انتشار نهان که از \lr{U-Net} استفاده می‌کند (104 گیگافلاپس) به \lr{FID 10.56} دست می‌یابد.






\قسمت{مدل‌های \lr{DiT-XL/2}: نسخه‌های آموزش‌دیده}

مدل‌های \lr{DiT-XL/2} مجموعه‌ای از مدل‌های مولد هستند که توسط \lr{Meta} منتشر شده‌اند. این مدل‌ها بر روی مجموعه داده \lr{ImageNet}، یک پایگاه داده بزرگ بصری که برای استفاده در تحقیقات تشخیص شیء بصری طراحی شده است، آموزش دیده‌اند. \lr{XL/2} به وضوحی که مدل‌ها در آن آموزش دیده‌اند اشاره دارد و دو نسخه در دسترس است: یکی برای تصاویر با وضوح 512×512 و دیگری برای تصاویر با وضوح 256×256.

\قسمت{وضوح 512×512 در \lr{ImageNet}}
مدل \lr{DiT-XL/2} که بر روی \lr{ImageNet} با وضوح 512×512 آموزش دیده است، از مقیاس‌های هدایت بدون استفاده از طبقه‌بند 6.0 استفاده می‌کند. فرآیند آموزش این مدل 3 میلیون گام به طول انجامید. این مدل با وضوح بالا برای مدیریت تصاویر پیچیده با جزئیات دقیق طراحی شده است.

\قسمت{وضوح 256×256 در \lr{ImageNet}}
مدل \lr{DiT-XL/2} که بر روی \lr{ImageNet} با وضوح 256×256 آموزش دیده است، از مقیاس‌های هدایت بدون استفاده از طبقه‌بند 4.0 استفاده می‌کند. فرآیند آموزش این مدل 7 میلیون گام به طول انجامید. این مدل برای تصاویر با وضوح استاندارد بهینه‌سازی شده و از نظر منابع محاسباتی کارآمدتر است.

\قسمت{مقایسه \lr{FID} دو وضوح}
مدل \lr{DiT-XL/2} که در وضوح 256×256 آموزش دیده است، تمامی مدل‌های انتشار قبلی را با دستیابی به \lr{FID-50K} برابر با 2.27 شکست داده است. این بهبود قابل توجهی نسبت به بهترین \lr{FID-50K} قبلی برابر با 3.60 به دست آمده توسط مدل \lr{LDM} (256×256) است. از نظر کارایی محاسباتی، مدل \lr{DiT-XL/2} نیز برتر است و فقط به 119 گیگافلاپس نیاز دارد در حالی که مدل \lr{LDM-4} به 103 گیگافلاپس و \lr{ADM-U} به 742 گیگافلاپس نیاز دارد.

\شروع{شکل}[ht]
\centerimg{img7}{15cm}
\شرح{مقایسه مدل‌های مختلف \lr{DiT}}
\برچسب{شکل:مقایسه مدل‌های مختلف DiT}
\پایان{شکل}

در وضوح \lr{512×512}، مدل \lr{DiT-XL/2} بار دیگر تمامی مدل‌های انتشار قبلی را شکست می‌دهد و بهترین \lr{FID} قبلی 3.85 که توسط \lr{ADM-U} به دست آمده بود را به 3.04 بهبود می‌بخشد. از نظر کارایی محاسباتی، مدل \lr{DiT-XL/2} فقط به 525 گیگافلاپس نیاز دارد که به طور قابل توجهی کمتر از 2813 گیگافلاپس \lr{ADM-U} است.