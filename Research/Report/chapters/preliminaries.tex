
\فصل{مفاهیم اولیه}\label{فصل۲:مفاهیم اولیه}
همانطور که در فصل \ref{فصل۱:مقدمه} بیان شد، برای درک بهتر مدل‌های انتشار، ابتدا می‌بایست پیش‌نیاز ها و مفاهیم اولیه این زمینه را بیان کنیم. در ادامه به معرفی و بررسی مدل‌های انتشار و ساختار های معروف و مورد استفاده در مقالات مرتبط می‌پردازیم.






\قسمت{مقدمه ای بر مدل‌ها انتشار}
مدل‌های انتشار نوعی مدل مولد هستند که یک زنجیره مارکوف را شبیه‌سازی می‌کنند تا از یک توزیع اولیه ساده به توزیع داده‌ها پیچیده انتقال یابند. این فرآیند شبیه به حرکت براونی یک ذره است که هر مرحله آن یک حرکت تصادفی کوچک است. به همین دلیل به آن‌ها "مدل‌های انتشار" گفته می‌شود.

مدل‌های انتشار در کاربردهای مختلفی مانند نویززدایی\پانویس{Denoising}، افزایش وضوح تصویر\پانویس{Super-resolution} و ترمیم تصاویر\پانویس{Inpainting} استفاده می‌شود. یکی از مزایای کلیدی مدل‌های انتشار توانایی آن‌ها در تولید نمونه‌های با کیفیت بالا است که آن‌ها را به‌ویژه برای وظایفی مانند سنتز تصویر بسیار مفید می‌سازد.

به‌طور کلی فرآیند انتشار از دو مرحله تشکیل می‌شود:
\شروع{شمارش}
\فقره انتشار پیش‌رو
\فقره انتشار پس‌رو
\پایان{شمارش}
که در ادامه به توضیح هر کدام می‌پردازیم.


\زیرقسمت{فرآیند انتشار پیشرو}
فرآیند انتشار پیشرو یک زنجیره مارکوف از مراحل انتشار است که در آن به تدریج و به صورت تصادفی نویز به داده‌های اصلی اضافه می‌کنیم.

با فرض اینکه یک نقطه داده از یک توزیع داده واقعی $ \mathbf{x_0} \sim q(\mathbf{x}) $ نمونه‌برداری شده باشد، یک فرآیند انتشار پیشرو را تعریف کنیم که در آن مقدار کمی نویز گاوسی به نمونه در $ T $ مرحله اضافه می‌کنیم، به‌طوری که یک دنباله از نمونه‌های نویزی $ \mathbf{x_1}, \ldots, \mathbf{x_T} $ تولید می‌شود. اندازه مراحل با یک برنامه تغییرپذیری $\{\beta_t \in (0, 1)\}_{t=1}^{T}$ کنترل می‌شود.

\begin{equation}
	q(\mathbf{x_t}|\mathbf{x_{t-1}}) = \mathcal{N}(\mathbf{x_t}; \sqrt{1 - \beta_t} \mathbf{x_{t-1}}, \beta_t \mathbf{I})
\end{equation}

\begin{equation}
	q(\mathbf{x_{1:T}}|\mathbf{x_0}) = \prod_{t=1}^{T} q(\mathbf{x_t}|\mathbf{x_{t-1}})
\end{equation}

با بزرگتر شدن $ t $، نمونه داده $ \mathbf{x_0} $ به تدریج ویژگی‌های قابل تشخیص خود را از دست می‌دهد  در نهایت، زمانی که $ T \to \infty $، $\mathbf{x_T}$ معادل یک توزیع گاوسی همسانگرد\پانویس{Isotropic} خواهد بود.






یکی از ویژگی‌های جالب فرآیند فوق این است که می‌توانیم $\mathbf{x_t}$ را در هر لحظه زمانی دلخواه $t$ با استفاده از ترفند بازپارامتری‌سازی\پانویس{Reparameterization} به صورت بسته نمونه‌برداری کنیم. اگر $\alpha_t = 1 - \beta_t$ و $\bar{\alpha_t} = \prod_{i=1}^{t} \alpha_i$ باشد داریم:

\begin{equation}
	\begin{aligned}
		\mathbf{x_t} &= \sqrt{\alpha_t} \mathbf{x_{t-1}} + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon_{t-1}} \\
		&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x_{t-2}} + \sqrt{1 - \alpha_t \alpha_{t-1}} \boldsymbol{\tilde{\epsilon_{t-2}}} \\
		&= \cdots \\
		&= \sqrt{\bar{\alpha_t}} \mathbf{x_0} + \sqrt{1 - \bar{\alpha_t}} \boldsymbol{\epsilon} \\
		q(\mathbf{x_t}|\mathbf{x_0}) &= \mathcal{N}(\mathbf{x_t}; \sqrt{\bar{\alpha_t}} \mathbf{x_0}, (1 - \bar{\alpha_t}) \mathbf{I})
	\end{aligned}
\end{equation}

توجه به این نکته مهم است که وقتی دو توزیع گاوسی با واریانس‌های مختلف $\mathcal{N}(0, \sigma_1^2 \mathbf{I})$ و $\mathcal{N}(0, \sigma_2^2 \mathbf{I})$ را باهم ترکیب می‌کنیم، توزیع جدید $\mathcal{N}(0, (\sigma_1^2 + \sigma_2^2) \mathbf{I})$ است. در اینجا انحراف استاندارد ترکیبی برابر است با:
\begin{equation}
	\sqrt{(1 - \alpha_t) + \alpha_t (1 - \alpha_{t-1})} = \sqrt{1 - \alpha_t \alpha_{t-1}}
\end{equation}
معمولاً، می‌توانیم گام به‌روزرسانی بزرگتری داشته باشیم وقتی که نمونه‌ها نویز بیشتری پیدا می‌کنند، بنابراین $\beta_1 < \beta_2 < \cdots < \beta_T$ و در نتیجه $\bar{\alpha_1} > \cdots > \bar{\alpha_T}$.







\زیرقسمت{فرایند انتشار معکوس}
فرآیند انتشار معکوس سعی می‌کند فرآیند انتشار پیشرو را به صورت معکوس انجام دهد و از داده‌های نویزی، داده‌های اصلی را با کیفیت بالا تولید کند.


اگر بتوانیم فرآیند فوق را معکوس کنیم و از $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ نمونه‌برداری کنیم، قادر خواهیم بود نمونه واقعی را از یک ورودی نویز گاوسی بازسازی کنیم:
\begin{equation}
	\mathbf{x_T} \sim \mathcal{N}(0, \mathbf{I})
\end{equation}

اگر مقدار $\beta_t$ به اندازه کافی کوچک باشد، $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ نیز گاوسی خواهد بود. متاسفانه، نمی‌توانیم به راحتی $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ را تخمین بزنیم زیرا نیاز به استفاده از کل مجموعه داده‌ها دارد و بنابراین باید یک مدل $ p_\theta $ را بیاموزیم تا این احتمالات شرطی را تقریب بزند تا بتوانیم فرآیند انتشار معکوس را اجرا کنیم.

\begin{equation}
	p_\theta(\mathbf{x_{0:T}}) = p(\mathbf{x_T}) \prod_{t=1}^{T} p_\theta(\mathbf{x_{t-1}}|\mathbf{x_t})
\end{equation}

\begin{equation}
	p_\theta(\mathbf{x_{t-1}}|\mathbf{x_t}) = \mathcal{N}(\mathbf{x_{t-1}}; \mu_\theta(\mathbf{x_t}, t), \Sigma_\theta(\mathbf{x_t}, t))
\end{equation}


\شروع{شکل}[ht]
\centerimg{img3}{10cm}
\شرح{نمونه‌ای از آموزش مدل انتشار برای داده‌های ۲ بعدی \مرجع{sohl2015deep}}
\برچسب{شکل:نمونه‌ای از آموزش مدل انتشار برای داده‌های ۲ بعدی}
\پایان{شکل}



قابل توجه است که احتمال شرطی معکوس زمانی که بر روی $\mathbf{x_0}$ شرطی شود، به‌صورت زیر قابل محاسبه است:
\begin{equation}
	q(\mathbf{x_{t-1}}|\mathbf{x_t}, \mathbf{x_0}) = \mathcal{N}(\mathbf{x_{t-1}}; \tilde{\mu}(\mathbf{x_t}, \mathbf{x_0}), \tilde{\beta_t} \mathbf{I})
\end{equation}

با استفاده از قاعده بیز\پانویس{Bayes’ rule}، می‌توان نوشت:

\begin{equation}
	\begin{aligned}
		q(\mathbf{x_{t-1}}|\mathbf{x_t}, \mathbf{x_0}) &= \frac{q(\mathbf{x_t}|\mathbf{x_{t-1}}, \mathbf{x_0}) q(\mathbf{x_{t-1}}|\mathbf{x_0})}{q(\mathbf{x_t}|\mathbf{x_0})} \\
		&\propto \exp \left( -\frac{1}{2} \left( \frac{(\mathbf{x_t} - \sqrt{\alpha_t}\mathbf{x_{t-1}})^2}{\beta_t} + \frac{(\mathbf{x_{t-1}} - \sqrt{\bar{\alpha_t}}\mathbf{x_0})^2}{1 - \bar{\alpha_{t-1}}} \right. \right. \\
		&\qquad \left. \left. - \frac{(\mathbf{x_t} - \sqrt{\bar{\alpha_t}}\mathbf{x_0})^2}{1 - \bar{\alpha_t}} \right) \right) \\
		&= \exp \left( -\frac{1}{2} \left( \frac{\mathbf{x_t}^2 - 2\sqrt{\alpha_t}\mathbf{x_t}\mathbf{x_{t-1}} + \alpha_t\mathbf{x_{t-1}}^2}{\beta_t} \right. \right. \\
		&\qquad + \frac{\mathbf{x_{t-1}}^2 - 2\sqrt{\bar{\alpha_t}}\mathbf{x_{t-1}}\mathbf{x_0} + \bar{\alpha_t}\mathbf{x_0}^2}{1 - \bar{\alpha_{t-1}}} \\
		&\qquad \left. \left. - \frac{\mathbf{x_t}^2 - 2\sqrt{\bar{\alpha_t}}\mathbf{x_t}\mathbf{x_0} + \bar{\alpha_t}\mathbf{x_0}^2}{1 - \bar{\alpha_t}} \right) \right) \\
		&= \exp \left( -\frac{1}{2} \left( \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{t-1}}} \right)\mathbf{x_{t-1}}^2 \right. \right. \\
		&\qquad - \left( \frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x_t} + \frac{2\sqrt{\bar{\alpha_t}}}{1 - \bar{\alpha_{t-1}}}\mathbf{x_0} \right)\mathbf{x_{t-1}} \\
		&\qquad \left. \left. + \frac{2\sqrt{\bar{\alpha_t}}}{1 - \bar{\alpha_t}}\mathbf{x_t} + C(\mathbf{x_t}, \mathbf{x_0}) \right) \right)
	\end{aligned}
\end{equation}





با پیروی از تابع چگالی گاوسی استاندارد\پانویس{Standard Gaussian Density Function}، میانگین و واریانس به صورت زیر بیان می‌شوند. (به یاد داریم که $\alpha_t = 1 - \beta_t$ و $\bar{\alpha_t} = \prod_{i=1}^{t} \alpha_i$):



\begin{equation}
	\begin{aligned}
		\tilde{\beta_t} &= \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{ت-1}}} \right)^{-1} = \left( \frac{\alpha_t - \bar{\alpha_t} + \beta_t}{\beta_t (1 - \bar{\alpha_{ت-1}})} \right)^{-1} = \beta_t \frac{1 - \bar{\alpha_{ت-1}}}{1 - \bar{\alpha_t}} \
	\end{aligned}
\end{equation}





\begin{equation}
	\begin{aligned}
		\tilde{\mu_t}(\mathbf{x_t}, \mathbf{x_0}) &= \left( \frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x_t} + \frac{\sqrt{\bar{\alpha_{ت-1}}}}{1 - \bar{\alpha_{ت-1}}} \mathbf{x_0} \right) \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{ت-1}}} \right)^{-1} \\
		&= \left( \sqrt{\alpha_t} \mathbf{x_t} + \sqrt{\bar{\alpha_t}} \mathbf{x_0} \right) \left( \frac{1 - \bar{\alpha_{ت-1}}}{1 - \bar{\alpha_t}} \right) \\
		&= \frac{\sqrt{\alpha_t (1 - \bar{\alpha_t})}}{1 - \bar{\alpha_t}} \mathbf{x_t} + \frac{\sqrt{\bar{\alpha_t} \beta_t}}{1 - \bar{\alpha_t}} \mathbf{x_0}
	\end{aligned}
\end{equation}




\قسمت{شبکه عصبی کانولوشنی \lr{U-Net}}

در معماری‌های رایج شبکه‌های کانولوشنی، کانال‌های ویژگی به منظور استخراج اطلاعات معنایی به طور متناوب از لایه‌های ادغام\پانویس{Pooling} عبور می‌کنند. لایه‌های ادغام با کاهش اطلاعات فضایی میدان دید لایه‌های بعدی را افزایش می‌دهند و از این طریق شبکه می‌تواند اطلاعات معنایی را از تصاویر استخراج کند. هرچه از ورودی یک شبکه کانولوشنی به سمت خروجی آن حرکت کنیم، اطلاعات فضایی کاسته شده و بر اطلاعات معنایی افزوده می‌شود؛ بنابراین به نظر می‌رسد بین اطلاعات فضایی و معنایی در این شبکه‌ها مصالحه\پانویس{Trade-off} ای وجود دارد. که افزایش هریک، موجب کاهش دیگری می‌شود. شبکه یو-نت به این چالش بزرگ از طریق معماری خاص خود و اتصالات پرشی رسیدگی می‌کند. این شبکه دارای یک ساختار متقارن-کدگذار-کدگشا می‌باشد «شکل \رجوع{شکل:ساختار شبکه یو-نت}» که در آن کانال‌های ویژگی در سطوح مختلفی از بخش کدگذار، از طریق اتصال پرشی، به کانال‌های ویژگی کدگشا الحاق\پانویس{‫‪Concatenate‬‬} می‌شود. این ساختار برخلاف ساختار شبکه تمام کانولوشنی \مرجع{long2015fully}، که خروجی نهایی را در یک مرحله نمونه افزایی\پانویس{Up-Sampling} ایجاد می‌کند، باعث می‌شود بازیابی اطلاعات فضایی بهتر صورت بگیرد. تفاوت دیگر این دو شبکه این است که اتصالات پرشی در شبکه یو-نت به جای استفاده از عمل جمع، از عمل الحاق استفاده می‌کند. این ساختار متقارن، به همراه تکنیک‌های افزونگی داده، باعث می‌شود شبکه یو-نت بتواند علاوه بر کسب دقت بالا، از تعداد بسیار کمی داده‌های آموزشی را یاد بگیرد.


\شروع{شکل}[ht]
\centerimg{img2}{15cm}
\شرح{ساختار شبکه \lr{U-Net} \مرجع{ronneberger2015u}}
\برچسب{شکل:ساختار شبکه یو-نت}
\پایان{شکل}


\زیرقسمت{ساختار شبکه}
شکل «\رجوع{شکل:ساختار شبکه یو-نت}» ساختار شبکه یو-نت را نشان می‌دهد. این شبکه از یک بخش کدگذار«سمت چپ» و یک بخش کدگشا «سمت راست» تشکیل شده است. هر مرحله از بخش کدگذار، شامل دو لایه کانولوشنی \(3 \times 3\) با یک تابع فعالسازی\پانویس{Activation Function} \lr{ReLU} در هرکدام و یک لایه ادغام حداکثر\پانویس{Max Pooling} با سایز \(2 \times 2\) و گام \(2\) می‌باشد. در هر مرحله از بخش کدگشا، ابتدا یک لایه نمونه‌افزایی به همراه یک لایه کانولوشنی \(2 \times 2\) «کانولوشن افزاینده\پانویس{Up-Convolution}» تعداد کانال‌های ویژگی را نصف و ابعاد آن‌ها را دو برابر می‌کند. سپس کانال‌های ویژگی با کانال‌های برش داده شده متناطر از بخش کدگذار الحاق می‌شوند. در ادامه دولایه کانولوشنی، مشابه آنچه در بخش کدگذار وجود دارد، روی کانال‌های ویژگی اعمال می‌شود. در لایه‌های کانولوشنی این شبکه عمل گسترش مرز با صفر\پانویس{Zero Padding} انجام نمی‌شود و بنابراین برای الحاق کانال‌های ویژگی، همانطور که اشاره شد، کانال‌های ویژگی بخش کدگذار بریده می‌شوند. در انتهای بخش کدگشا یک لایه کانولوشنی \(1 \times 1\) تعداد کانال‌های ویژگی «۶۴» را به تعداد کلاس‌های مسئله تبدیل می‌کند.






\قسمت{ویژن ترنسفرمر}




































%
%\قسمت{ایده اصلی مقاله}
%در این مقاله، به مسئله Placement با رویکرد یک مسئله شبکه عصبی نگاه شده است و تلاش شده است که بهینه ترین حالت ممکن پیدا شود. خروجی این مقاله منجر به ارائه یک چارچوب\پاورقی{Framework} متن باز\پاورقی{Open Source} مبتنی بر CPU و GPU با استفاده از شبکه‌های عمیق و زبان برنامه نویسی \texttt{Python} و \texttt{C++} و کتابخانه های \texttt{PyTorch} و \texttt{CUDA} شده است که بدون افت کیفیت نسبیت به روش‌های قدیمی، تا \textbf{۳۰ برابر} افزایش سرعت برای مسئله چینش ارائه شده است.
%
%
%
%
%
%\قسمت{کار‌های پیشین}
%روش ها و الگوریتم هایی که تا اکنون توسعه داده شده است، به نام پیاده‌سازی تحلیلی\پاورقی{Analytical placement} معروف است. این روش ها خروجی‌های با کیفیتی تولید می‌کنند اما مشکل عمده این روش ها، کند بودن آنهاست. معمولا برای افزایش سرعت در این روش ها از تکنیک‌های چند‌نخی\پاورقی{Multi-thread} کردن CPU استفاده می‌شود. روش‌های مبتنی بر چند‌نخی، سرعت جایگذاری را تا ۵ برابر افزایش می‌دهند اما مشکل عمده آنها این است که کیفیت طرح خروجی، بین ۲ الی ۶ درصد کاهش می‌یابد. \مرجع{article3, article4, article5}
%
%یعنی الگوریتم‌های توسعه داده شده مبتنی بر چند نخی، صرفا از جهت افزایش سرعت بهینه کار می‌کنند و توجه اصلی بر روی افزایش سرعت است. اما توجهی کمتری نسبت به بهبود کیفیت خروجی دارند و همین امر موجب نا کار‌امد بودن چنین الگوریتم‌هایی می‌شود.
%
%دسته دیگری از الگوریتم‌ها بر بستر GPU توسعه داده شده است.\مرجع{article6} این الگوریتم بر اساس خوشه‌بندی\پاورقی{Clustering} انجام شده است که با موازی‌سازی بر بستر GPU، به‌طور میانگین، سرعت تا ۱۵ برابر نسبت به روش Analytical افزایش یافته است. درصد افت کیفیت نیز برای این الگوریتم، کمتر از ۱ درصد گزارش شده است. اما این روش نیز به دلیل هزینه بالا برای فراهم کردن GPU مورد اقبال واقع نشده است. \مرجع{article7}
%
%در این مقاله روشی که توسعه داده شده است، بر مبنای روش‌های تحلیلی نام برده شده است. با این تفاوت که این الگوریتم هم در بستر GPU و هم در بستر CPU شتابدهی شده است. که از این بابت عام منظوره بودن الگوریتم و هزینه پایین آن نسبت به سایر الگوریتم های موجود را نشان می‌دهد.
%
%این الگوریتم که به نام DREAM-Place نام‌گذاری شده است، به صورت عمومی\پاورقی{Generic} توسعه داده شده است که با سایر الگوریتم‌های جای‌گذاری تحلیلی مثل NTUplace سازگار است. \مرجع{article8}
%
%ایده‌های اصلی\پاورقی{Contributions} مقاله به صورت زیر عنوان شده است:
%\شروع{فقرات}
%\فقره ایجاد دیدگاهی کاملا جدید برای ارتباط دنیای طراحی آیسی با دنیای هوش‌مصنوعی و یادگیری عمیق به صورت کاملا متن باز برای توسعه در CPU و GPU
%\فقره محاسبه بهترین محل قرار گیری سلول ها با کمترین طول و چگالی سیم.
%\فقره بهبود سرعت چینش بدون افت کیفیت خروجی تا ۳۰ برابر. به طوری که طراحی ای با ۱ میلیون سلول در بستر CPU در یک دقیقه تمام می‌شود. که این تایمینگ به صورت خطی با افزایش تعداد سلول ها تا حداکثر ۱۰ میلیون تغییر می‌کند.
%\پایان{فقرات}
%
%
%تمام سورس‌کد نوشته شده برای این الگ.ریتم نیز در گیت‌هاب\زیرنویس{\href{https://github.com/limbo018/DREAMPlace}{\lr{github.com/limbo018/DREAMPlace}}} قابل دریافت است.
%
%
%
%\قسمت{مراحل انجام الگوریتم}
%قبل از بررسی مراحل انجام، نیاز است که برخی از اصطلاحات این حوزه را بیان کنیم.
%
%جاگذاری تحلیلی\پاورقی{Analytical placement} دارای ۳ مرحله اصلی است:
%
%\شروع{فقرات}
%\فقره چینش کلی یا GP\پاورقی{Global Placement}: قرار دادن سلول‌ها در طرح با هدف بهینه شدن GP و مینیمم Routing
%\فقره بررسی قوانین یا LG\پاورقی{Legalization}: بررسی طراحی انجام شده در مرحله GP و حذف همپوشانی ها و تراز کردن مشکلات طرح.
%\فقره چینش جزئی یا DP\پاورقی{Detailed Placement}: بررسی دقیق تر طراحی انجام شده برای رسیدن به دقت و کیفیت بالا در طراحی.
%\پایان{فقرات}
%
%\مهم{معمولا مرحله GP بیشترین زمان را در فرایند صرف می‌کند به همین دلیل، تمام الگوریتم‌های بیان شده در قبل و همچنین این الگوریتم، نتایج مورد بحث، در مورد مینیمم کردن طول مسیر ها و چگالی آنهاست.}
%
%مراحل انجام این الگوریتم را می‌توان به صورت زیر بیان کرد:
%\شروع{شکل}[ht]
%\centerimg{img1_DREAMPlace2_flow.png}{12cm}
%\شرح{ساختار الگوریتم}
%\برچسب{شکل:ساختار الگوریتم}
%\پایان{شکل}
%
%در ایتدا و در مهمترین فاز، ورودی‌ها که شامل محل قرار گیری سلول هاست، به شبکه داده می‌شود و شبکه که در ابتدا به یک سری وزن های رندوم مقدار دهی شده است، آموزش می‌بیند و خروجی آن که مقدار خطای محاسبه شده است، با استفاده از الگوریتم پس انتشار خطا\پاورقی{Back Propagation} به لایه های عقب تر انتشار داده می‌شود تا اینکه بهینه ترین خطا (مینیمم ترین حالا خطا) را پیدا کنیم. «شکل \رجوع{شکل:فرایند آموزش وزن‌های اولیه}»
%
%
%
%مراحل انجام این الگوریتم را می‌توان به صورت زیر بیان کرد:
%\شروع{شکل}[ht]
%\centerimg{img1_trainW.png}{8cm}
%\شرح{فرایند آموزش شبکه برای پیدا کردن وزن‌های اولیه}
%\برچسب{شکل:فرایند آموزش وزن‌های اولیه}
%\پایان{شکل}
%
%
%
%
%پس از پیدا کردن خطای مینیمم، وزن های آموزش دیده شده در آن خطا، به عنوان وزن های شبکه ما برای آموزش شبکه شبکه، برای حل مسئله چینش انتخاب می‌شوند و در فاز دوم آموزش، گره ها یه عنوان ورودی به شبکه از پیش آموزش دیده شده با وزن‌های مشخص داده می‌شود و با استفاده از همان الگوریتم پس‌انتشار خطا، فرایند آموزش تا زمان مینیمم شدن خطا ادامه پیدا می‌کند. «شکل \رجوع{شکل:فرایند آموزش شبکه برای پیدا کردن بهترین چینش}»
%
%
%مراحل انجام این الگوریتم را می‌توان به صورت زیر بیان کرد:
%\شروع{شکل}[ht]
%\centerimg{img2_trainNet.png}{8cm}
%\شرح{فرایند آموزش شبکه برای پیدا کردن بهترین چینش}
%\برچسب{شکل:فرایند آموزش شبکه برای پیدا کردن بهترین چینش}
%\پایان{شکل}
%
%
%برای پیدا کردن خطا هم به سادگی تابع $f(y,\hat{y})=|\hat{y}-y|)$ محاسبه شده است که در این رابطه $\hat{y}$ مقدار بدست آمده توسط شبکه است که در «شکل \رجوع{شکل:فرایند آموزش وزن‌های اولیه}» با $\phi(x_i;w)$ نشان داده شده است.
%
%در ادامه، نمودار زمانی بخش‌های مختلف الگوریتم مانند GP و LG و ... این الگوریتم در بستر CPU در دو حالت تک نخی و ۱۰ نخی برای طراحی \texttt{bigblue4}\زیرنویس{این طراحی متشکل از ۲ میلیون سلول است} در «شکل \رجوع{شکل:نمودار زمانی شبکه آموزش دیده شده در بستر CPU}» آورده شده است:
%
%\شروع{شکل}[ht]
%\centerimg{img3_output_circular.png}{10cm}
%\شرح{نمودار زمانی شبکه آموزش دیده شده در بستر CPU}
%\برچسب{شکل:نمودار زمانی شبکه آموزش دیده شده در بستر CPU}
%\پایان{شکل}
%
%
%
%
%\شروع{شکل}[ht]
%\centerimg{img4.png}{12cm}
%\شرح{خروجی واقعی شبکه برای مسئله چینش}
%\برچسب{شکل:خروجی واقعی شبکه برای مسئله چینش}
%\پایان{شکل}
%
%
%
%همانطور که انتظار می‌رفت، با افزایش تعداد نخ ها در CPU زمان انجام بخش‌های مختلف کاهش یافته است. برای نمونه زمان فاز GP در CPU ۱۰ نخی، ۱٫۱۰۶\% کاهش پیدا کرده است.
%
%
%
%آموزش فاز LG هم بر روی CPU تک، ۱۰، ۲۰ و ۴۰ نخی برای چندیت طراحی مختلف منجمله \texttt{bigblue} انجام شده است که خروجی‌های زمانی آن در «شکل \رجوع{شکل:نمودار زمانی فاز LG بر بستر CPU}» آورده شده است. همانطور که مشخص است به طور میانگین این فاز بر روی CPU تک نخی زیر ۱ دقیقه طول می‌کشد.
%
%
%\شروع{شکل}[ht]
%\centerimg{img5.png}{12cm}
%\شرح{نمودار زمانی فاز LG بر بستر CPU}
%\برچسب{شکل:نمودار زمانی فاز LG بر بستر CPU}
%\پایان{شکل}
%
%همانطور که قبلا هم بیان شد، زمان اجرای این الگوریتم به صورت خطی با افزایش تعداد سلول ها زیاد می‌شود. «شکل \رجوع{شکل:تغییرات نمودار زمانی فاز GP}» نمودار رشد زمانی، با افزایش تعداد سلول ها را برای فاز GP نشان می‌دهد.
%
%\شروع{شکل}[ht]
%\centerimg{img6.png}{10cm}
%\شرح{تغییرات نمودار زمانی فاز GP}
%\برچسب{شکل:تغییرات نمودار زمانی فاز GP}
%\پایان{شکل}
%
%
%\قسمت{مزایا و معایب}
%
%از مزایای روش پیشنهاد شده می‌توان به موارد زیر اشاره کرد:
%\شروع{فقرات}
%\فقره متن باز بودن آن
%\فقره توسعه الگوریتم به دو زبان \texttt{Python} و \texttt{C++} 
%\فقره توسعه الگوریتم بر دو بستر CPU و GPU
%\فقره هماهنگ بودن با سایر الگوریتم های تحلیلی
%\فقره سرعت بالای آن
%\فقره عدم افت کیفیت طراحی
%\فقره پشتیبانی\پاورقی{Affiliation} قوی پروژه که توسط شرکت NVIDIA انجام می‌شود.
%\فقره و ...
%\پایان{فقرات}
%
%در کنار بیان مزایا می‌بایست به معایب پروژه را هم بیان کرد. در ادامه چند مورد از معایب پروژه انجام شده را بیان می‌کنیم:
%
%\شروع{فقرات}
%\فقره عدم اشاره مستقیم مقاله به ساختار شبکه‌عصبی استفاده شده
%\فقره نیازمند بودن به سیستمی قوی برای اجرای این الگوریتم
%\فقره عدم تست کردن الگوریتم با برچسب‌\texttt{Benchmark} های جدید تر مثل 2017 ISPD و 2018 ISPD
%\فقره نیازمندی به پیشنیاز\پاورقی{Dependency} های مختلف
%\فقره عدم صحبت از توان مصرفی الگوریتم
%\فقره عدم ارائه گزارش از خطا‌های ناشی از اجرای الگوریتم
%\فقره و ...
%\پایان{فقرات}
%
%
%
%
%
%\قسمت{اجرای عملی الگوریتم}
%
%برای اجرای این الگوریتم بر روی سیستم شخصی می‌بایست مراحل زیر را طی کرد\زیرنویس{به دلیل استفاده اینجانب از سیستم‌عامل لینوکس، مراحلی که در ادامه نام برده شده است برای کاربران لینوکسی‌ست.}:
%
%\زیرقسمت{نصب Git}
%با دستور زیر می‌توان گیت را نصب نمود:
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} sudo apt install git-all}
%\end{latin}
%
%همچنین با دستور زیر چک می‌کنیم که گیت به درستی نصب شده باشد:
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} git --version}
%\end{latin}
%
%اگر در خروجی ورژن گیت برگشت داده شود، یعنی نصب به درستی انجام شده است:
%
%
%\زیرقسمت{دانلود مخزن الگوریتم}
%
%با استفاده از دستور زیر، مخزن\پاورقی{Repository} الگوریتم را دانلود می‌کنیم.
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} git clone --recursive https://github.com/limbo018/DREAMPlace.git}
%\end{latin}
%
%پس از دانلود با دستور زیر به دایرکتوری فایل دانلود شده می‌رویم:
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} cd DREAMPlace}
%\end{latin}
%
%\زیرقسمت{نصب پیش‌نیاز ها}
%با دستور زیر، پیش‌نیاز ها\پاورقی{Dependency} ها را نصب می‌کنیم.
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} pip install -r requirements.txt}
%\end{latin}
%
%\زیرقسمت{بیلد نرم‌افزار}
%این نرم‌افزار را می‌توان به دو صورت بیلد کرد. استفاده از داکر\پاورقی{Docker} و یا بیلد معمولی بر روی سیستم\زیرنویس{در این گزارش بیلد معمولی بر روی سیستم را توضیح می‌دهیم. برای بیلد بر روی داکر می‌توانید \href{https://github.com/limbo018/DREAMPlace?tab=readme-ov-file}{اینجا} را ببینید.}.
%
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} mkdir build}\\
%	\texttt{\textcolor{blue}{\$} cd build \# we call this <build directory>}\\
%	\texttt{\textcolor{blue}{\$} cmake .. -DCMAKE\_INSTALL\_PREFIX=<installation directory>}\\
%	\texttt{-DPython\_EXECUTABLE=\$(which python)}\\
%	\texttt{\textcolor{blue}{\$} make}\\
%	\texttt{\textcolor{blue}{\$} make install}\\
%\end{latin}
%
%بیلد نرم‌افزار، مدتی طول خواهد کشید. اگر بیلد به درستی انجام شود، پیغام \texttt{Successful Building} نمایش داده می‌شود.
%
%\زیرقسمت{انتخاب بنچ‌مارک}
%
%با استفاده از دستور زیر می‌توان بنچ‌مارک مورد نظر را انتخاب کرد:
%
%در این مقاله از بنچ‌مارک 2005 ISPD استفاده شده است.
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} python benchmarks/ispd2005\_2015.py}
%\end{latin}
%
%\زیرقسمت{اجرای شبیه‌سازی}
%پس از انتخاب بنچ‌مارک به صورت زیر می‌توان برنامه را اجرا کرد:
%\begin{latin}
%	\texttt{\textcolor{blue}{\$} cd <installation directory>}\\
%	\texttt{\textcolor{blue}{\$} python unittest/ops/hpwl\_unittest.py}
%\end{latin}