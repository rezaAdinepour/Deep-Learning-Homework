
\فصل{مفاهیم اولیه}\label{فصل۲:مفاهیم اولیه}
همانطور که در فصل \ref{فصل۱:مقدمه} بیان شد، برای درک بهتر مدل‌های انتشار، ابتدا می‌بایست پیش‌نیاز ها و مفاهیم اولیه این زمینه را بیان کنیم. در ادامه به معرفی و بررسی مدل‌های انتشار و ساختار های معروف و مورد استفاده در مقالات مرتبط می‌پردازیم.






\قسمت{مقدمه ای بر مدل‌ها انتشار}
مدل‌های انتشار نوعی مدل مولد هستند که یک زنجیره مارکوف را شبیه‌سازی می‌کنند تا از یک توزیع اولیه ساده به توزیع داده‌ها پیچیده انتقال یابند. این فرآیند شبیه به حرکت براونی یک ذره است که هر مرحله آن یک حرکت تصادفی کوچک است. به همین دلیل به آن‌ها "مدل‌های انتشار" گفته می‌شود.

مدل‌های انتشار در کاربردهای مختلفی مانند نویززدایی\پانویس{Denoising}، افزایش وضوح تصویر\پانویس{Super-resolution} و ترمیم تصاویر\پانویس{Inpainting} استفاده می‌شود. یکی از مزایای کلیدی مدل‌های انتشار توانایی آن‌ها در تولید نمونه‌های با کیفیت بالا است که آن‌ها را به‌ویژه برای وظایفی مانند سنتز تصویر بسیار مفید می‌سازد.

به‌طور کلی فرآیند انتشار از دو مرحله تشکیل می‌شود:
\شروع{شمارش}
\فقره انتشار پیش‌رو
\فقره انتشار پس‌رو
\پایان{شمارش}
که در ادامه به توضیح هر کدام می‌پردازیم.


\زیرقسمت{فرآیند انتشار پیشرو}
فرآیند انتشار پیشرو یک زنجیره مارکوف از مراحل انتشار است که در آن به تدریج و به صورت تصادفی نویز به داده‌های اصلی اضافه می‌کنیم.

با فرض اینکه یک نقطه داده از یک توزیع داده واقعی $ \mathbf{x_0} \sim q(\mathbf{x}) $ نمونه‌برداری شده باشد، یک فرآیند انتشار پیشرو را تعریف کنیم که در آن مقدار کمی نویز گاوسی به نمونه در $ T $ مرحله اضافه می‌کنیم، به‌طوری که یک دنباله از نمونه‌های نویزی $ \mathbf{x_1}, \ldots, \mathbf{x_T} $ تولید می‌شود. اندازه مراحل با یک برنامه تغییرپذیری $\{\beta_t \in (0, 1)\}_{t=1}^{T}$ کنترل می‌شود.

\begin{equation}
	q(\mathbf{x_t}|\mathbf{x_{t-1}}) = \mathcal{N}(\mathbf{x_t}; \sqrt{1 - \beta_t} \mathbf{x_{t-1}}, \beta_t \mathbf{I})
\end{equation}

\begin{equation}
	q(\mathbf{x_{1:T}}|\mathbf{x_0}) = \prod_{t=1}^{T} q(\mathbf{x_t}|\mathbf{x_{t-1}})
\end{equation}

با بزرگتر شدن $ t $، نمونه داده $ \mathbf{x_0} $ به تدریج ویژگی‌های قابل تشخیص خود را از دست می‌دهد  در نهایت، زمانی که $ T \to \infty $، $\mathbf{x_T}$ معادل یک توزیع گاوسی همسانگرد\پانویس{Isotropic} خواهد بود.






یکی از ویژگی‌های جالب فرآیند فوق این است که می‌توانیم $\mathbf{x_t}$ را در هر لحظه زمانی دلخواه $t$ با استفاده از ترفند بازپارامتری‌سازی\پانویس{Reparameterization} به صورت بسته نمونه‌برداری کنیم. اگر $\alpha_t = 1 - \beta_t$ و $\bar{\alpha_t} = \prod_{i=1}^{t} \alpha_i$ باشد داریم:

\begin{equation}
	\begin{aligned}
		\mathbf{x_t} &= \sqrt{\alpha_t} \mathbf{x_{t-1}} + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon_{t-1}} \\
		&= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x_{t-2}} + \sqrt{1 - \alpha_t \alpha_{t-1}} \boldsymbol{\tilde{\epsilon_{t-2}}} \\
		&= \cdots \\
		&= \sqrt{\bar{\alpha_t}} \mathbf{x_0} + \sqrt{1 - \bar{\alpha_t}} \boldsymbol{\epsilon} \\
		q(\mathbf{x_t}|\mathbf{x_0}) &= \mathcal{N}(\mathbf{x_t}; \sqrt{\bar{\alpha_t}} \mathbf{x_0}, (1 - \bar{\alpha_t}) \mathbf{I})
	\end{aligned}
\end{equation}

توجه به این نکته مهم است که وقتی دو توزیع گاوسی با واریانس‌های مختلف $\mathcal{N}(0, \sigma_1^2 \mathbf{I})$ و $\mathcal{N}(0, \sigma_2^2 \mathbf{I})$ را باهم ترکیب می‌کنیم، توزیع جدید $\mathcal{N}(0, (\sigma_1^2 + \sigma_2^2) \mathbf{I})$ است. در اینجا انحراف استاندارد ترکیبی برابر است با:
\begin{equation}
	\sqrt{(1 - \alpha_t) + \alpha_t (1 - \alpha_{t-1})} = \sqrt{1 - \alpha_t \alpha_{t-1}}
\end{equation}
معمولاً، می‌توانیم گام به‌روزرسانی بزرگتری داشته باشیم وقتی که نمونه‌ها نویز بیشتری پیدا می‌کنند، بنابراین $\beta_1 < \beta_2 < \cdots < \beta_T$ و در نتیجه $\bar{\alpha_1} > \cdots > \bar{\alpha_T}$.







\زیرقسمت{فرایند انتشار معکوس}
فرآیند انتشار معکوس سعی می‌کند فرآیند انتشار پیشرو را به صورت معکوس انجام دهد و از داده‌های نویزی، داده‌های اصلی را با کیفیت بالا تولید کند.


اگر بتوانیم فرآیند فوق را معکوس کنیم و از $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ نمونه‌برداری کنیم، قادر خواهیم بود نمونه واقعی را از یک ورودی نویز گاوسی بازسازی کنیم:
\begin{equation}
	\mathbf{x_T} \sim \mathcal{N}(0, \mathbf{I})
\end{equation}

اگر مقدار $\beta_t$ به اندازه کافی کوچک باشد، $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ نیز گاوسی خواهد بود. متاسفانه، نمی‌توانیم به راحتی $ q(\mathbf{x_{t-1}}|\mathbf{x_t}) $ را تخمین بزنیم زیرا نیاز به استفاده از کل مجموعه داده‌ها دارد و بنابراین باید یک مدل $ p_\theta $ را بیاموزیم تا این احتمالات شرطی را تقریب بزند تا بتوانیم فرآیند انتشار معکوس را اجرا کنیم.

\begin{equation}
	p_\theta(\mathbf{x_{0:T}}) = p(\mathbf{x_T}) \prod_{t=1}^{T} p_\theta(\mathbf{x_{t-1}}|\mathbf{x_t})
\end{equation}

\begin{equation}
	p_\theta(\mathbf{x_{t-1}}|\mathbf{x_t}) = \mathcal{N}(\mathbf{x_{t-1}}; \mu_\theta(\mathbf{x_t}, t), \Sigma_\theta(\mathbf{x_t}, t))
\end{equation}


\شروع{شکل}[ht]
\centerimg{img3}{10cm}
\شرح{نمونه‌ای از آموزش مدل انتشار برای داده‌های ۲ بعدی \مرجع{sohl2015deep}}
\برچسب{شکل:نمونه‌ای از آموزش مدل انتشار برای داده‌های ۲ بعدی}
\پایان{شکل}



قابل توجه است که احتمال شرطی معکوس زمانی که بر روی $\mathbf{x_0}$ شرطی شود، به‌صورت زیر قابل محاسبه است:
\begin{equation}
	q(\mathbf{x_{t-1}}|\mathbf{x_t}, \mathbf{x_0}) = \mathcal{N}(\mathbf{x_{t-1}}; \tilde{\mu}(\mathbf{x_t}, \mathbf{x_0}), \tilde{\beta_t} \mathbf{I})
\end{equation}

با استفاده از قاعده بیز\پانویس{Bayes’ rule}، می‌توان نوشت:

\begin{equation}
	\begin{aligned}
		q(\mathbf{x_{t-1}}|\mathbf{x_t}, \mathbf{x_0}) &= \frac{q(\mathbf{x_t}|\mathbf{x_{t-1}}, \mathbf{x_0}) q(\mathbf{x_{t-1}}|\mathbf{x_0})}{q(\mathbf{x_t}|\mathbf{x_0})} \\
		&\propto \exp \left( -\frac{1}{2} \left( \frac{(\mathbf{x_t} - \sqrt{\alpha_t}\mathbf{x_{t-1}})^2}{\beta_t} + \frac{(\mathbf{x_{t-1}} - \sqrt{\bar{\alpha_t}}\mathbf{x_0})^2}{1 - \bar{\alpha_{t-1}}} \right. \right. \\
		&\qquad \left. \left. - \frac{(\mathbf{x_t} - \sqrt{\bar{\alpha_t}}\mathbf{x_0})^2}{1 - \bar{\alpha_t}} \right) \right) \\
		&= \exp \left( -\frac{1}{2} \left( \frac{\mathbf{x_t}^2 - 2\sqrt{\alpha_t}\mathbf{x_t}\mathbf{x_{t-1}} + \alpha_t\mathbf{x_{t-1}}^2}{\beta_t} \right. \right. \\
		&\qquad + \frac{\mathbf{x_{t-1}}^2 - 2\sqrt{\bar{\alpha_t}}\mathbf{x_{t-1}}\mathbf{x_0} + \bar{\alpha_t}\mathbf{x_0}^2}{1 - \bar{\alpha_{t-1}}} \\
		&\qquad \left. \left. - \frac{\mathbf{x_t}^2 - 2\sqrt{\bar{\alpha_t}}\mathbf{x_t}\mathbf{x_0} + \bar{\alpha_t}\mathbf{x_0}^2}{1 - \bar{\alpha_t}} \right) \right) \\
		&= \exp \left( -\frac{1}{2} \left( \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{t-1}}} \right)\mathbf{x_{t-1}}^2 \right. \right. \\
		&\qquad - \left( \frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x_t} + \frac{2\sqrt{\bar{\alpha_t}}}{1 - \bar{\alpha_{t-1}}}\mathbf{x_0} \right)\mathbf{x_{t-1}} \\
		&\qquad \left. \left. + \frac{2\sqrt{\bar{\alpha_t}}}{1 - \bar{\alpha_t}}\mathbf{x_t} + C(\mathbf{x_t}, \mathbf{x_0}) \right) \right)
	\end{aligned}
\end{equation}





با پیروی از تابع چگالی گاوسی استاندارد\پانویس{Standard Gaussian Density Function}، میانگین و واریانس به صورت زیر بیان می‌شوند. (به یاد داریم که $\alpha_t = 1 - \beta_t$ و $\bar{\alpha_t} = \prod_{i=1}^{t} \alpha_i$):



\begin{equation}
	\begin{aligned}
		\tilde{\beta_t} &= \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{ت-1}}} \right)^{-1} = \left( \frac{\alpha_t - \bar{\alpha_t} + \beta_t}{\beta_t (1 - \bar{\alpha_{ت-1}})} \right)^{-1} = \beta_t \frac{1 - \bar{\alpha_{ت-1}}}{1 - \bar{\alpha_t}} \
	\end{aligned}
\end{equation}





\begin{equation}
	\begin{aligned}
		\tilde{\mu_t}(\mathbf{x_t}, \mathbf{x_0}) &= \left( \frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x_t} + \frac{\sqrt{\bar{\alpha_{ت-1}}}}{1 - \bar{\alpha_{ت-1}}} \mathbf{x_0} \right) \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha_{ت-1}}} \right)^{-1} \\
		&= \left( \sqrt{\alpha_t} \mathbf{x_t} + \sqrt{\bar{\alpha_t}} \mathbf{x_0} \right) \left( \frac{1 - \bar{\alpha_{ت-1}}}{1 - \bar{\alpha_t}} \right) \\
		&= \frac{\sqrt{\alpha_t (1 - \bar{\alpha_t})}}{1 - \bar{\alpha_t}} \mathbf{x_t} + \frac{\sqrt{\bar{\alpha_t} \beta_t}}{1 - \bar{\alpha_t}} \mathbf{x_0}
	\end{aligned}
\end{equation}




\قسمت{شبکه عصبی کانولوشنی \lr{U-Net}}

در معماری‌های رایج شبکه‌های کانولوشنی، کانال‌های ویژگی به منظور استخراج اطلاعات معنایی به طور متناوب از لایه‌های ادغام\پانویس{Pooling} عبور می‌کنند. لایه‌های ادغام با کاهش اطلاعات فضایی میدان دید لایه‌های بعدی را افزایش می‌دهند و از این طریق شبکه می‌تواند اطلاعات معنایی را از تصاویر استخراج کند. هرچه از ورودی یک شبکه کانولوشنی به سمت خروجی آن حرکت کنیم، اطلاعات فضایی کاسته شده و بر اطلاعات معنایی افزوده می‌شود؛ بنابراین به نظر می‌رسد بین اطلاعات فضایی و معنایی در این شبکه‌ها مصالحه\پانویس{Trade-off} ای وجود دارد. که افزایش هریک، موجب کاهش دیگری می‌شود. شبکه یو-نت به این چالش بزرگ از طریق معماری خاص خود و اتصالات پرشی رسیدگی می‌کند. این شبکه دارای یک ساختار متقارن-کدگذار-کدگشا می‌باشد «شکل \رجوع{شکل:ساختار شبکه یو-نت}» که در آن کانال‌های ویژگی در سطوح مختلفی از بخش کدگذار، از طریق اتصال پرشی، به کانال‌های ویژگی کدگشا الحاق\پانویس{‫‪Concatenate‬‬} می‌شود. این ساختار برخلاف ساختار شبکه تمام کانولوشنی \مرجع{long2015fully}، که خروجی نهایی را در یک مرحله نمونه افزایی\پانویس{Up-Sampling} ایجاد می‌کند، باعث می‌شود بازیابی اطلاعات فضایی بهتر صورت بگیرد. تفاوت دیگر این دو شبکه این است که اتصالات پرشی در شبکه یو-نت به جای استفاده از عمل جمع، از عمل الحاق استفاده می‌کند. این ساختار متقارن، به همراه تکنیک‌های افزونگی داده، باعث می‌شود شبکه یو-نت بتواند علاوه بر کسب دقت بالا، از تعداد بسیار کمی داده‌های آموزشی را یاد بگیرد.


\شروع{شکل}[ht]
\centerimg{img2}{15cm}
\شرح{ساختار شبکه \lr{U-Net} \مرجع{ronneberger2015u}}
\برچسب{شکل:ساختار شبکه یو-نت}
\پایان{شکل}


\زیرقسمت{ساختار شبکه}
شکل «\رجوع{شکل:ساختار شبکه یو-نت}» ساختار شبکه یو-نت را نشان می‌دهد. این شبکه از یک بخش کدگذار«سمت چپ» و یک بخش کدگشا «سمت راست» تشکیل شده است. هر مرحله از بخش کدگذار، شامل دو لایه کانولوشنی \(3 \times 3\) با یک تابع فعالسازی\پانویس{Activation Function} \lr{ReLU} در هرکدام و یک لایه ادغام حداکثر\پانویس{Max Pooling} با سایز \(2 \times 2\) و گام \(2\) می‌باشد. در هر مرحله از بخش کدگشا، ابتدا یک لایه نمونه‌افزایی به همراه یک لایه کانولوشنی \(2 \times 2\) «کانولوشن افزاینده\پانویس{Up-Convolution}» تعداد کانال‌های ویژگی را نصف و ابعاد آن‌ها را دو برابر می‌کند. سپس کانال‌های ویژگی با کانال‌های برش داده شده متناطر از بخش کدگذار الحاق می‌شوند. در ادامه دولایه کانولوشنی، مشابه آنچه در بخش کدگذار وجود دارد، روی کانال‌های ویژگی اعمال می‌شود. در لایه‌های کانولوشنی این شبکه عمل گسترش مرز با صفر\پانویس{Zero Padding} انجام نمی‌شود و بنابراین برای الحاق کانال‌های ویژگی، همانطور که اشاره شد، کانال‌های ویژگی بخش کدگذار بریده می‌شوند. در انتهای بخش کدگشا یک لایه کانولوشنی \(1 \times 1\) تعداد کانال‌های ویژگی «۶۴» را به تعداد کلاس‌های مسئله تبدیل می‌کند.






\قسمت{ویژن ترنسفرمر}
ویژن ترنسفورمرها (\lr{ViT}) مدلی جدید در زمینه بینایی کامپیوتری\پانویس{Computer Vision} هستند که مدل‌های ترنسفورمر را که در اصل برای وظایف پردازش زبان طبیعی طراحی شده‌اند، به وظایف طبقه‌بندی تصاویر اعمال می‌کنند. برخلاف شبکه‌های عصبی کانولوشنی سنتی که تصاویر را به صورت سلسله مراتبی پردازش می‌کنند، \lr{ViT} ها به‌صورت موازی تصاویر را پردازش می‌کنند.


شبکه‌های \lr{ViT} تصاویر را به عنوان دنباله‌ای از تکه‌ها در نظر می‌گیرند و وابستگی‌های جهانی\پانویس{Global Dependencies} بین آن‌ها را ثبت می‌کنند. این ویژگی به آن‌ها امکان مدل‌سازی تعاملات پیکسلی با برد بلند را می‌دهد. یکی از مزایای کلیدی \lr{ViT} ها قابلیت مقیاس‌پذیری آن‌ها است. آن‌ها می‌توانند بر روی مجموعه داده‌های بزرگ آموزش ببینند و تصاویر بزرگ را به عنوان ورودی قبول کنند.



\قسمت{نحوه عملکرد ویژن ترنسفرها}

\زیرقسمت{بینیان ترنسفرمر}
برای درک چگونگی عملکرد ویژن ترنسفورمرها، لازم است مفاهیم پایه‌ای معماری ترنسفرمر مانند توجه به خود\پانویس{Self-Attention} را درک کنیم. توجه به خود یک مکانیزم است که به مدل اجازه می‌دهد تا هنگام پیش‌بینی، اهمیت عناصر مختلف در یک دنباله را وزن‌دهی کند و در نتیجه به نتایج چشمگیری در وظایف مبتنی بر دنباله دست یابد.



\شروع{شکل}[ht]
\centerimg{img4}{15cm}
\شرح{ساختار شبکه \lr{ViT} \مرجع{dosovitskiy2020image}}
\برچسب{شکل:ساختار شبکه ویژن ترنسفرمر}
\پایان{شکل}


\زیرقسمت{انطباق ترنسفرمر برای تصاویر}

مفهوم توجه به خود برای پردازش تصاویر با استفاده از ویژن ترنسفورمرها تطبیق داده شده است. برخلاف داده‌های متنی، تصاویر به طور ذاتی دو بعدی هستند و شامل پیکسل‌هایی هستند که در ردیف‌ها و ستون‌ها قرار گرفته‌اند. برای مقابله با این چالش، ویژن ترنسفورمرها تصاویر را به دنباله‌هایی تبدیل می‌کنند که می‌توانند توسط ترانسفورمر پردازش شوند.

مراحل پردازش در \lr{ViT} ها را می‌توان به صورت زیر تقسیم‌بندی نمود:

\begin{itemize}
	\item \textbf{تقسیم تصویر به پچ‌ها:}
	
	 اولین مرحله در پردازش یک تصویر با ویژن ترنسفورمر، تقسیم آن به پچ‌های کوچکتر و با اندازه ثابت است. هر پچ نمایانگر یک ناحیه محلی از تصویر است.
	
	\item \textbf{تخت کردن پچ‌ها:} 
	
	درون هر پچ، مقادیر پیکسل‌ها به یک بردار واحد تخت می‌شوند. این فرآیند تخت کردن به مدل اجازه می‌دهد تا پچ‌های تصویر را به عنوان داده‌های دنباله‌ای پردازش کند.
	
	\item \textbf{تولید جاسازی‌های خطی با بعد کمتر:}
	
	 این بردارهای پچ تخت شده سپس با استفاده از تبدیل‌های خطی قابل آموزش به یک فضای با بعد کمتر نگاشت می‌شوند. این مرحله بعد داده‌ها را کاهش می‌دهد در حالی که ویژگی‌های مهم را حفظ می‌کند.
	
	\item \textbf{اضافه کردن رمزگذاری‌های موقعیتی:}
	
	 برای حفظ اطلاعات در مورد ترتیب فضایی پچ‌ها، رمزگذاری‌های موقعیتی اضافه می‌شوند. این رمزگذاری‌ها به مدل کمک می‌کنند تا موقعیت نسبی پچ‌های مختلف در تصویر را درک کند.
	
	\item \textbf{تغذیه دنباله به رمزگذار ترانسفورمر:}
	
	 ورودی به یک رمزگذار ترانسفورمر استاندارد شامل دنباله‌ای از جاسازی‌های پچ و جاسازی‌های موقعیتی است. این رمزگذار از لایه‌های متعددی تشکیل شده است که هر کدام شامل دو جزء مهم هستند: مکانیزم‌های توجه به خود چندسری (\lr{MSP}ها)، که مسئولیت محاسبه وزن‌های توجه برای اولویت دادن به عناصر دنباله ورودی در حین پیش‌بینی‌ها را دارند، و پرسپترون چندلایه (\lr{MLP}) بلوک‌ها. قبل از هر بلوک، نرمال‌سازی لایه (\lr{LN}) برای مقیاس‌بندی و مرکزیت داده‌ها درون لایه اعمال می‌شود، که پایداری و کارایی آموزش را تضمین می‌کند. در طول آموزش، یک بهینه‌ساز نیز برای تنظیم ابرپارامترهای مدل در پاسخ به از دست رفتن محاسبه شده در هر تکرار آموزشی استفاده می‌شود.
	
	\item \textbf{طبقه‌بندی:}
	
	 برای فعال کردن طبقه‌بندی تصویر، یک "توکن طبقه‌بندی" ویژه به دنباله جاسازی‌های پچ اضافه می‌شود. حالت نهایی این توکن در خروجی رمزگذار ترانسفورمر به عنوان نماینده کل تصویر عمل می‌کند.
\end{itemize}



\زیرقسمت{سوگیری استقرایی و ویژن ترنسفورمر}
ویژن ترنسفورمرها نسبت به \lr{CNN} ها سوگیری استقرایی خاص تصویر کمتری دارند. در \lr{CNN} ها، مفاهیمی مانند محلی بودن، ساختار همسایگی دو بعدی و تعادل انتقال در هر لایه از مدل تعبیه شده‌اند. اما ویژن ترنسفورمرها بر لایه‌های توجه به خود برای زمینه‌یابی جهانی تکیه می‌کنند و فقط از ساختار همسایگی دو بعدی در مراحل اولیه استخراج پچ استفاده می‌کنند. این بدان معنی است که ویژن ترنسفورمرها بیشتر به یادگیری روابط فضایی از ابتدا تکیه دارند و دیدگاه متفاوتی از درک تصویر ارائه می‌دهند.


\زیرقسمت{معماری هیبریدی}
علاوه بر استفاده از پچ‌های تصویر خام، ویژن ترنسفورمرها همچنین گزینه‌ای برای یک معماری هیبریدی فراهم می‌کنند. با این روش، دنباله‌های ورودی می‌توانند از نقشه‌های ویژگی استخراج شده توسط یک \lr{CNN} تولید شوند. این سطح از انعطاف‌پذیری اجازه می‌دهد تا نقاط قوت \lr{CNN} ها و ترنسفورمرها را در یک مدل ترکیب کرد و امکانات بیشتری برای بهینه‌سازی عملکرد را ارائه داد.




\قسمت{مدل‌های انتشار نهان}
مدل‌های انتشار نهان\پانویس{Latent Diffusion Models} نوعی مدل مولد هستند که با مدل‌سازی داده‌ها به عنوان یک فرآیند انتشار، یاد می‌گیرند داده‌ها را تولید کنند. این فرآیند با یک توزیع اولیه ساده، مانند نویز گاوسی، آغاز می‌شود و به تدریج از طریق یک سری مراحل کوچک آن را به توزیع هدف تبدیل می‌کند. هر مرحله توسط یک شبکه عصبی هدایت می‌شود که برای معکوس کردن فرآیند انتشار آموزش داده شده است. مدل‌های \lr{LDMs} در تولید نمونه‌های با کیفیت بالا در حوزه‌های مختلف، از جمله تصاویر، متن و صدا موفق بوده‌اند. \مرجع{rombach2022high}


\شروع{شکل}[ht]
\centerimg{img5}{15cm}
\شرح{ساختار مدل \lr{LDM} \مرجع{rombach2022high}}
\برچسب{شکل:ساختار مدل LDM}
\پایان{شکل}








\زیرقسمت{معایب شبکه کانولوشنی \lr{U-Net}}
شبکه‌های \lr{U-Net} در بسیاری از وظایف بینایی کامپیوتری به دلیل توانایی آن‌ها در استخراج ویژگی‌های محلی و حفظ وضوح فضایی به یک اصل اساسی تبدیل شده‌اند. با این حال، این شبکه‌ها محدودیت‌هایی دارند. برای مثال، آن‌ها اغلب در به دست آوردن وابستگی‌های بلندمدت و زمینه جهانی در داده‌های ورودی مشکل دارند. این به این دلیل است که میدان پذیرش یک لایه کانولوشنی محلی و محدود است و افزایش آن نیاز به شبکه‌های عمیق‌تر و فیلترهای بزرگ‌تر دارد که خود چالش‌های جدیدی را به همراه دارند.

علاوه بر این، عملیات کانولوشن در \lr{U-Net}ها نسبت به انتقال مکان مقاوم است، به این معنی که یک ویژگی را بدون توجه به موقعیت آن در تصویر به همان صورت پردازش می‌کند. این می‌تواند در وظایفی که موقعیت مطلق ویژگی‌ها اهمیت دارد، یک عیب حساب شود.





\زیرقسمت{حرکت به سوی ترانسفورمرها}
ترانسفورمرها که در اصل برای وظایف پردازش زبان طبیعی طراحی شده‌اند، پتانسیل بالایی در وظایف بینایی کامپیوتری نشان داده‌اند. برخلاف شبکه‌های کانولوشنی، ترانسفورمرها می‌توانند وابستگی‌های بلندمدت را بدون نیاز به شبکه‌های عمیق یا فیلترهای بزرگ مدل‌سازی کنند. این به دلیل استفاده از مکانیزم‌های توجه به خود است که به هر عنصر ورودی اجازه می‌دهد با تمام عناصر دیگر، بدون توجه به فاصله‌شان، تعامل داشته باشد. علاوه بر این، ترانسفورمرها نسبت به انتقال مکان مقاوم نیستند، به این معنی که می‌توانند موقعیت مطلق ویژگی‌ها را دریافت کنند. این از طریق استفاده از رمزگذاری‌های موقعیتی محقق می‌شود که اطلاعاتی در مورد موقعیت هر عنصر در ورودی اضافه می‌کند.




\زیرقسمت{تکامل پچ‌های نهان}
مفهوم پچ‌های نهان از نیاز به ایجاد کارایی محاسباتی ترانسفورمرها برای تصاویر با وضوح بالا نشأت گرفته است. اعمال ترانسفورمرها مستقیماً به پیکسل‌های خام تصاویر با وضوح بالا به دلیل پیچیدگی توجه به خود که به تعداد عناصر به صورت درجه دوم افزایش می‌یابد، محاسباتی پرهزینه است. برای غلبه بر این مشکل، تصویر به پچ‌های کوچکی تقسیم می‌شود و به ترانسفورمرها اعمال می‌شوند. این کار به طور قابل توجهی تعداد عناصر و در نتیجه پیچیدگی محاسباتی را کاهش می‌دهد. این روش به ترانسفورمرها اجازه می‌دهد تا هم ویژگی‌های محلی درون هر پچ و هم زمینه جهانی بین پچ‌ها را دریافت کنند.







\قسمت{ترانسفورمرهای انتشار (\lr{DiT}) در مقابل ویژن ترانسفورمرها (\lr{ViT})}
در حالی که هر دو \lr{DiT} و \lr{ViT} از ترانسفورمرها به عنوان معماری و بدنه اصلی خود استفاده می‌کنند و بر روی پچ‌های نهان عمل می‌کنند، تفاوت آن‌ها در نحوه تولید تصاویر و جزئیات معماری خاص آن‌هاست.


\زیرقسمت{ترانسفورمرهای انتشار (\lr{DiT})}

مدل \lr{DiT} از ترانسفورمرها در یک فرآیند انتشار نهان استفاده می‌کند، جایی که یک توزیع اولیه ساده (مانند نویز گاوسی) به تدریج به تصویر هدف تبدیل می‌شود. این کار با معکوس کردن فرآیند انتشار که توسط یک شبکه ترانسفورمر هدایت می‌شود، انجام می‌گیرد. یکی از جنبه‌های مهم \lr{DiT} مفهوم بازه‌های زمانی انتشار است. این بازه‌های زمانی نمایانگر مراحل فرآیند انتشار هستند و شبکه ترانسفورمر در هر مرحله به بازه زمانی شرطی می‌شود. این ویژگی به شبکه اجازه می‌دهد تا ویژگی‌های مختلف را در مراحل مختلف فرآیند انتشار تولید کند. \lr{DiT} همچنین می‌تواند به "برچسب‌های کلاس" شرطی شود و بدین ترتیب تصاویر مربوط به کلاس‌های خاصی را تولید کند.







\زیرقسمت{ویژن ترنسفرمر ها (\lr{ViT})}
ویژن ترنسفورمرها از ترنسفورمرها برای تولید مستقیم تصویر به صورت خودرگرسیو\پانویس{Autoregressive} استفاده می‌کنند، جایی که هر پچ یکی پس از دیگری تولید می‌شود و به پچ‌های قبلاً تولید شده شرطی می‌شود. یکی از اجزای کلیدی \lr{ViT} استفاده از لایه‌های نرمال‌سازی تطبیقی\پانویس{Adaptive Layer Norm} است. این لایه‌ها به طور تطبیقی ویژگی‌ها را بر اساس آمار دسته فعلی مقیاس‌بندی و تغییر می‌دهند که به پایداری آموزش و بهبود عملکرد مدل کمک می‌کند. در حالی که هر دو رویکرد نقاط قوت و ضعف خود را دارند، آن‌ها دو جهت امیدوارکننده برای استفاده از ترنسفورمرها در مدل‌سازی مولد تصاویر را نشان می‌دهند. انتخاب بین \lr{DiT} و \lr{ViT} به نیازهای خاص مورد نظر بستگی دارد.

به عنوان مثال، اگر کاربرد مورد نظر تولید تصاویر از کلاس‌های خاصی باشد، \lr{DiT} ممکن است به دلیل توانایی آن در شرطی شدن بر روی برچسب‌های کلاس انتخاب بهتری باشد. از سوی دیگر، اگر وظیفه نیاز به تولید تصاویر با وضوح بالا داشته باشد، \lr{ViT} ممکن است به دلیل استفاده از لایه‌های \lr{adaLN} که می‌توانند به پایداری آموزش مدل‌های بزرگ کمک کنند، مناسب‌تر باشد.



\قسمت{معماری عمومی ترانسفورمرهای انتشار}

\زیرقسمت{نمایش‌های فضایی}
مدل ابتدا نمایش‌های فضایی را از طریق یک لایه شبکه ورودی می‌گیرد و ورودی‌های فضایی را به دنباله‌ای از توکن‌ها تبدیل می‌کند. این فرآیند به مدل اجازه می‌دهد تا اطلاعات فضایی موجود در داده‌های تصویر را پردازش کند. این یک گام حیاتی است زیرا داده‌های ورودی را به فرمتی تبدیل می‌کند که ترانسفورمر بتواند به طور مؤثر پردازش کند.

\زیرقسمت{جاسازی‌های موقعیتی}
جاسازی‌های موقعیتی یک جزء حیاتی از معماری ترانسفورمر هستند. آن‌ها مدل را با اطلاعاتی درباره موقعیت هر توکن در دنباله فراهم می‌کنند. در \lr{DiT}ها، جاسازی‌های موقعیتی استاندارد ویژن ترانسفورمر به همه توکن‌های ورودی اعمال می‌شود. این فرآیند به مدل کمک می‌کند تا موقعیت‌های نسبی و روابط بین بخش‌های مختلف تصویر را درک کند.

\زیرقسمت{طراحی بلوک \lr{DiT}}
در یک مدل انتشار معمولی، یک شبکه عصبی کانولوشنی یاد می‌گیرد که نویز را از تصویر حذف کند. \lr{DiT}ها این \lr{U-Net} را با یک ترانسفورمر جایگزین می‌کنند. این جایگزینی نشان می‌دهد که سوگیری استقرایی \lr{U-Net} برای عملکرد مدل‌های انتشار ضروری نیست.


\شروع{شکل}[ht]
\centerimg{img6}{15cm}
\شرح{ساختار مدل \lr{DiT} \مرجع{peebles2023scalable}}
\برچسب{شکل:ساختار مدل DiT}
\پایان{شکل}