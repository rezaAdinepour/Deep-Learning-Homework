
% -------------------------------------------------------
%  English Abstract
% -------------------------------------------------------


\pagestyle{empty}

\begin{latin}

\begin{center}
\textbf{Abstract}
\end{center}
\baselineskip=.8\baselineskip

The main challenge of this research has been to examine and compare the efficiency of transformers against traditional architectures like U-Net in diffusion models. The obtained results indicate that transformers, with better scalability and higher efficiency, can be a suitable replacement for current architectures in diffusion models and help improve the quality of generated images. In this regard, Diffusion Transformers (DiTs), by increasing the depth and width of transformers and increasing the number of input tokens, have shown significant improvement in FID scores and achieved superior results in the ImageNet benchmark sets.


This research demonstrates that using transformers in diffusion models can provide innovative solutions for improving the quality and efficiency of image generation. These approaches, in addition to enhancing performance, can help reduce computational complexities and increase the speed of image generation processes. Given these results, transformers emerge as a strong alternative to traditional architectures in diffusion models and can be widely used in various applications.


\bigskip\noindent\textbf{Keywords}:
Deep Learning, Diffusion Models, Transformer, U-Net, Encoder, Decoder

\end{latin}
